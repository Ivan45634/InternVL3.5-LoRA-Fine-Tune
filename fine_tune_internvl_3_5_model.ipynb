{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning InternVL3.5-1B on MVBench\n",
    "\n",
    "This notebook demonstrates fine-tuning of the InternVL3.5-1B multimodal model on a video understanding task from the MVBench benchmark.\n",
    "\n",
    "## Before running:\n",
    "1. Update API keys and tokens in Cell 7 (WANDB_API_KEY, REPO_ID, HF token)\n",
    "2. Ensure sufficient disk space for video downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-T8dupbwlvDx",
    "outputId": "3ef6018b-fa1e-4a5f-8961-97e65ef40c12"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q pip setuptools wheel\n",
    "# !pip install --upgrade --no-deps -q numpy==2.0.0 huggingface-hub==0.35.3\n",
    "# !pip install -U -q huggingface-hub>=0.23.0 pyarrow>=21.0.0\n",
    "# !pip install -U -q transformers==4.45.0 accelerate bitsandbytes peft datasets==4.1.0\n",
    "# !pip install -q av decord\n",
    "# !pip install -q lightning\n",
    "# !pip install -q wandb\n",
    "# !pip install -q timm deepspeed flash_attn\n",
    "# !pip install -q nltk\n",
    "# !pip install -q opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers accelerate bitsandbytes peft datasets\n",
    "!pip install -q av decord\n",
    "!pip install -q lightning\n",
    "!pip install -q pyarrow==21.0.0\n",
    "!pip install -q wandb\n",
    "!pip install -q timm deepspeed flash_attn\n",
    "!pip install -U -q tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Choices and Justification\n",
    "\n",
    "### Task Selection\n",
    "**Selected Task:** Action Sequence from MVBench\n",
    "- This task involves understanding temporal sequences of actions in videos\n",
    "- It is a multiple-choice task, making it suitable for accuracy-based evaluation\n",
    "- The dataset has 188 valid samples after filtering missing videos\n",
    "\n",
    "### Fine-tuning Method: QLoRA (Quantized Low-Rank Adaptation)\n",
    "\n",
    "**Why QLoRA?**\n",
    "\n",
    "1. **Memory Efficiency**: \n",
    "   - 4-bit quantization reduces model memory footprint by ~75%\n",
    "   - Enables fine-tuning on consumer GPUs (e.g., single A100 or even RTX 4090)\n",
    "   - Only adapter parameters are trained, not the full model\n",
    "\n",
    "2. **Performance**:\n",
    "   - Research shows QLoRA achieves near-identical performance to full fine-tuning\n",
    "   - NormalFloat 4-bit (NF4) quantization preserves model quality\n",
    "   - LoRA rank of 16 provides good balance between capacity and efficiency\n",
    "\n",
    "3. **Practical Considerations**:\n",
    "   - Faster iteration and experimentation\n",
    "   - Lower computational costs\n",
    "   - Easier to deploy (smaller checkpoint files)\n",
    "\n",
    "**Configuration:**\n",
    "- Quantization: 4-bit NF4 with double quantization\n",
    "- LoRA rank: 16\n",
    "- LoRA alpha: 32\n",
    "- Target modules: All linear layers in the language model (excluding vision encoder and projector)\n",
    "- Compute dtype: bfloat16\n",
    "\n",
    "### Metric: Accuracy\n",
    "- Multiple-choice questions have 4 options (A, B, C, D)\n",
    "- Accuracy is the standard metric for such tasks\n",
    "- Easy to interpret and compare with baselines\n",
    "\n",
    "### Training Configuration\n",
    "- Optimizer: AdamW with weight decay (0.01)\n",
    "- Learning rate: 1e-4 with linear warmup (50 steps)\n",
    "- Batch size: 1 with gradient accumulation (8 steps)\n",
    "- Effective batch size: 8\n",
    "- Mixed precision: bfloat16\n",
    "- Max epochs: 2 (to prevent overfitting on small dataset)\n",
    "- Early stopping: patience of 3 epochs based on validation accuracy\n",
    "\n",
    "### Data Processing\n",
    "- Videos: 8 frames uniformly sampled from each video\n",
    "- Resolution: 448x448 (dynamic tiling)\n",
    "- Train/Test split: 80/20 (stratified)\n",
    "- Data augmentation: None (to maintain temporal consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTG2AJZT89qk"
   },
   "source": [
    "## Fine-tune InternVL 1B. on MMBench dataset\n",
    "\n",
    "In this notebook, you need to fine-tune the [InternVL](https://huggingface.co/OpenGVLab/InternVL3_5-1B) model on [MVBench](https://huggingface.co/datasets/OpenGVLab/MVBench) dataset which is comprised of various video-related tasks. Note that MMBench is quite small and is not made for tuning. So firstly you need to split it into training/testing parts.\n",
    "\n",
    "The goal for the model in this notebook is to answer given multiple choice questions based on the video. The questions can be realetd to temporal aspects of the video, pose prediction and so on.\n",
    "Sources:\n",
    "\n",
    "* InternVL [documentation](https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html)\n",
    "* InternVL [checkpoint on the hub](https://huggingface.co/OpenGVLab/InternVL2-1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szf17AKL89qm"
   },
   "source": [
    "## Define variables\n",
    "\n",
    "We'll first set some variables useful througout this notebook and doo all the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJtnWc3b89qn",
    "outputId": "ecdb83b2-e427-4546-9e69-4c733a8300b5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import bisect\n",
    "import shutil\n",
    "import traceback\n",
    "import numpy as np\n",
    "from nltk import edit_distance\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Dict, Any, List, Union, Tuple\n",
    "\n",
    "from transformers import (AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer,\n",
    "                          HfArgumentParser, Trainer, TrainingArguments,\n",
    "                          set_seed, AutoProcessor)\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from PIL import Image, ImageFile, PngImagePlugin, UnidentifiedImageError\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping, Callback\n",
    "\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input hugging face access token:  ········\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 160\n",
    "MODEL_ID = \"OpenGVLab/InternVL3_5-1B\"\n",
    "REPO_ID = \"Ivan1008/internvl3.5-finetune\"\n",
    "\n",
    "# os.environ[\"WANDB_API_KEY\"] = getpass(\"Input wandb api key: \")\n",
    "# os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "access_token = getpass(\"Input hugging face access token: \")\n",
    "login(access_token)\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8JeFyKyoT4X",
    "outputId": "1a8da900-90e2-4587-c169-9e71f9c66d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'InternVL'...\n",
      "remote: Enumerating objects: 3748, done.\u001b[K\n",
      "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 3748 (delta 32), reused 25 (delta 25), pack-reused 3688 (from 3)\u001b[K\n",
      "Receiving objects: 100% (3748/3748), 39.63 MiB | 31.05 MiB/s, done.\n",
      "Resolving deltas: 100% (2288/2288), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/OpenGVLab/InternVL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "si7CcgJAoc7S",
    "outputId": "191cf088-b4e2-48f4-fd12-93c9c8033669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\tzero_stage1_config.json\n",
      "eval\t\tzero_stage2_config.json\n",
      "evaluate.sh\tzero_stage3_config.json\n",
      "examples\tzero_stage3_config_100b.json\n",
      "internvl\tzero_stage3_config_100b_1e7_offload.json\n",
      "pyproject.toml\tzero_stage3_config_100b_1e8.json\n",
      "shell\t\tzero_stage3_config_34b.json\n",
      "tools\t\tzero_stage3_config_70b.json\n"
     ]
    }
   ],
   "source": [
    "! ls InternVL/internvl_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uXmhQGtEoT9h"
   },
   "outputs": [],
   "source": [
    "sys.path.append('./InternVL/internvl_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bhtzSpMpoT-7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/opt/conda/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petrel_client is not installed. If you read data locally instead of from ceph, ignore it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from internvl.dist_utils import init_dist\n",
    "# from internvl.model.internlm2.modeling_internlm2 import InternLM2ForCausalLM\n",
    "from internvl.model.internvl_chat import (InternVisionConfig,\n",
    "                                          InternVisionModel,\n",
    "                                          InternVLChatConfig,\n",
    "                                          InternVLChatModel)\n",
    "from internvl.patch import (concat_pad_data_collator,\n",
    "                            replace_llama_rmsnorm_with_fused_rmsnorm,\n",
    "                            replace_train_sampler)\n",
    "from internvl.train.constants import (BOX_END_TOKEN, BOX_START_TOKEN,\n",
    "                                      IMG_CONTEXT_TOKEN, IMG_END_TOKEN,\n",
    "                                      IMG_START_TOKEN, QUAD_END_TOKEN,\n",
    "                                      QUAD_START_TOKEN, REF_END_TOKEN,\n",
    "                                      IMAGENET_MEAN, IMAGENET_STD,\n",
    "                                      REF_START_TOKEN)\n",
    "from internvl.train.dataset import (ConcatDataset, read_frames_decord,\n",
    "                                    WeightedConcatDataset, build_transform,\n",
    "                                    dynamic_preprocess, preprocess,\n",
    "                                    preprocess_internlm, preprocess_mpt,\n",
    "                                    preprocess_phi3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM0HKJyKEwlp"
   },
   "source": [
    "# MVBench benchmark\n",
    "\n",
    "[MVBench on HF Datasets](https://huggingface.co/datasets/OpenGVLab/MVBench)\n",
    "\n",
    "<!-- ![MVbench1.png](https://huggingface.co/datasets/OpenGVLab/MVBench/resolve/main/assert/generation.png) -->\n",
    "\n",
    "It consists of the 20 temporal task examples as follows.\n",
    "\n",
    "![MVbench-structure.png](https://huggingface.co/datasets/OpenGVLab/MVBench/resolve/main/assert/task_example.png)\n",
    "\n",
    "\n",
    "Here we have a nice viewer for each task:\n",
    "\n",
    "[Dataset viewer](https://huggingface.co/datasets/OpenGVLab/MVBench/viewer/action_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNZQ3imilvDz"
   },
   "source": [
    "We will start by downloading and processing the dataset. Even though MVBench is a small dataset, it still requires **around 1000B to store the videos**, so make sure you have enough free space.\n",
    "\n",
    "First, we will use this mapping to get the datasets because each one is a separate subset in its own folder. Then we need a few helper functions to download videos and process them to fit the model's format (8 frames each video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gIwJovvg9YpI"
   },
   "outputs": [],
   "source": [
    "data_list = {\n",
    "    \"Action Sequence\": (\"action_sequence.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Prediction\": (\"action_prediction.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Antonym\": (\"action_antonym.json\", \"ssv2_video/\", \"video\", False),\n",
    "    \"Fine-grained Action\": (\"fine_grained_action.json\", \"Moments_in_Time_Raw/videos/\", \"video\", False),\n",
    "    \"Unexpected Action\": (\"unexpected_action.json\", \"FunQA_test/test/\", \"video\", False),\n",
    "    \"Object Existence\": (\"object_existence.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"Object Interaction\": (\"object_interaction.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Object Shuffle\": (\"object_shuffle.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Moving Direction\": (\"moving_direction.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"Action Localization\": (\"action_localization.json\", \"sta/sta_video/\", \"video\", True),  # has start & end\n",
    "    \"Scene Transition\": (\"scene_transition.json\", \"scene_qa/video/\", \"video\", False),\n",
    "    \"Action Count\": (\"action_count.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Moving Count\": (\"moving_count.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"Moving Attribute\": (\"moving_attribute.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"State Change\": (\"state_change.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Fine-grained Pose\": (\"fine_grained_pose.json\", \"nturgbd/\", \"video\", False),\n",
    "    \"Character Order\": (\"character_order.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Egocentric Navigation\": (\"egocentric_navigation.json\", \"vlnqa/\", \"video\", False),\n",
    "    \"Episodic Reasoning\": (\"episodic_reasoning.json\", \"tvqa/frames_fps3_hq/\", \"frame\", True),  # has start & end, read frame\n",
    "    \"Counterfactual Inference\": (\"counterfactual_inference.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "}\n",
    "\n",
    "data_dir = \"dataset\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yVdfSXMGlvDz"
   },
   "outputs": [],
   "source": [
    "def read_video_pyav(video_path, start, end, n_frames=8):\n",
    "    \"\"\"\n",
    "    Reads a video for given start-end timestamps interval\n",
    "    and uniformly samples 8 frames of it\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    video = container.streams.get(0)[0]\n",
    "\n",
    "    av_timestamps = [\n",
    "        int(packet.pts * video.time_base) for packet in container.demux(video) if packet.pts is not None\n",
    "    ]\n",
    "\n",
    "    av_timestamps.sort()\n",
    "    start_id = bisect.bisect_left(av_timestamps, start)\n",
    "    end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "    # in case it is a very short video, lets take a longer duration and sample\n",
    "    if end_id  - start_id < 10:\n",
    "        end_id += 10\n",
    "        start_id -= 10\n",
    "\n",
    "    end_id = min(len(av_timestamps) - 1, end_id)\n",
    "    start_id = max(1, start_id)\n",
    "\n",
    "    # We sample n_frames frames for tuning following the original paper\n",
    "    # But we can increase the number of frames for longer videos and check out if it helps performance\n",
    "    # Change the below \"n_frames\" to any number of frames you want, and note that more frames -> more computational resources needed\n",
    "    indices = np.linspace(start_id, end_id, n_frames).astype(int)\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_id:\n",
    "            break\n",
    "        if i >= start_id and i in indices:\n",
    "            frames.append(frame)\n",
    "    assert len(frames) == n_frames, f\"Got {len(frames)} frames but should be {n_frames}. Check the indices: {indices};, start_id: {start_id}, end_id: {end_id}. Len of video is {len(av_timestamps)} frames.\"\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X9-8O207lvD0"
   },
   "outputs": [],
   "source": [
    "def collate_read_video(example, path):\n",
    "    # Some datasets have a start-end interval, so we try to get it if exists.\n",
    "    # Otherwise just set a very large end timestamp\n",
    "    clip = read_video_pyav(f'{path}/{example[\"video\"]}', example.get(\"start\", 1), example.get(\"end\", 1e+10))\n",
    "    example[\"clip\"] = clip\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9jUicyg46_7K"
   },
   "outputs": [],
   "source": [
    "# Download the videos from datasets repo and unzip.\n",
    "# Make sure you have enough free space before downloading and unzipping\n",
    "\n",
    "# videos = snapshot_download(repo_id=\"OpenGVLab/MVBench\", allow_patterns=\"*\", repo_type=\"dataset\")\n",
    "# for zip_file in os.listdir(f\"{videos}/video\"):\n",
    "#     if zip_file.endswith(\".zip\"):\n",
    "#         shutil.unpack_archive(f\"{videos}/video/{zip_file}\", f\"{videos}/videos_unzipped/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrZnpKaflNQG",
    "outputId": "3d52c667-dbbd-49bc-a4e6-9fa11860b8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Action Sequence\n",
      "Annotation file: action_sequence.json\n",
      "Videos are stored in: star/Charades_v1_480/\n",
      "Videos are represented as: video\n",
      "Videos are have start/end timestamps: True\n",
      "star.zip\n"
     ]
    }
   ],
   "source": [
    "# Or download only selected task with appropriate files\n",
    "# https://huggingface.co/docs/huggingface_hub/v0.24.7/en/package_reference/file_download#huggingface_hub.hf_hub_download\n",
    "\n",
    "TASK_NAME = \"Action Sequence\"\n",
    "annotation_fn, video_dir, video_type, has_clip = data_list.get(TASK_NAME)\n",
    "print(f\"Task: {TASK_NAME}\")\n",
    "print(f\"Annotation file: {annotation_fn}\")\n",
    "print(f\"Videos are stored in: {video_dir}\")\n",
    "print(f\"Videos are represented as: {video_type}\")\n",
    "print(f\"Videos are have start/end timestamps: {has_clip}\")\n",
    "\n",
    "annotation_fn_local = hf_hub_download(repo_id=\"OpenGVLab/MVBench\",\n",
    "                                    filename='json/' + annotation_fn,\n",
    "                                    repo_type=\"dataset\",\n",
    "                                    local_dir=data_dir)\n",
    "videos_zip = hf_hub_download(repo_id=\"OpenGVLab/MVBench\",\n",
    "                            filename='video/' + video_dir.split(\"/\")[0] + \".zip\",\n",
    "                            repo_type=\"dataset\",\n",
    "                            local_dir=data_dir)\n",
    "\n",
    "# Unzip\n",
    "for zip_file in os.listdir(f\"{data_dir}/video\"):\n",
    "    if zip_file.endswith(\".zip\"):\n",
    "        print(zip_file)\n",
    "        shutil.unpack_archive(f\"{data_dir}/video/{zip_file}\", f\"{data_dir}/video/videos_unzipped/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThTV0DbDA9RP"
   },
   "source": [
    "Make a following data structure:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "    /json\n",
    "        task_name.json\n",
    "    /video\n",
    "        /task_name_prefix (optional)\n",
    "            /task_name\n",
    "                video_0.mp4\n",
    "                video_1.mp4\n",
    "                video_2.mp4\n",
    "                ...\n",
    "                video_100.mp4\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMNQND-ilvD0",
    "outputId": "ac1c78fc-6fcf-4e4c-b82c-76f53dcfc1eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video', 'question', 'candidates', 'answer', 'start', 'end'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=annotation_fn_local, split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSWorVK8B9ap",
    "outputId": "c0c865e8-9399-4d27-b562-8b9efbf08990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video `EDXBD.mp4` does not exists!\n",
      "Video `K47J5.mp4` does not exists!\n",
      "Video `9MNZ5.mp4` does not exists!\n",
      "Video `QXT9W.mp4` does not exists!\n",
      "Video `ABHC6.mp4` does not exists!\n",
      "Video `ALXUC.mp4` does not exists!\n",
      "Video `BAUQE.mp4` does not exists!\n",
      "Video `PHH6B.mp4` does not exists!\n",
      "Video `MNC10.mp4` does not exists!\n",
      "Video `W7CR5.mp4` does not exists!\n",
      "Video `Q8UJ8.mp4` does not exists!\n",
      "Video `X9WTR.mp4` does not exists!\n"
     ]
    }
   ],
   "source": [
    "# Some tasks in MVBench are missing video files - keep it in mind!\n",
    "has_missing = False\n",
    "for sample in ds:\n",
    "    if not os.path.exists(f\"{data_dir}/video/videos_unzipped/{video_dir}/{sample['video']}\"):\n",
    "        print(f\"Video `{sample['video']}` does not exists!\")\n",
    "        has_missing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1stzqVuCtRF",
    "outputId": "c11388de-691f-4816-98e7-07b8a2ecdfa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length = 200\n",
      "Dataset length = 188\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length = {len(ds)}\")\n",
    "if has_missing:\n",
    "    ds = ds.filter(lambda x: os.path.exists(f\"{data_dir}/video/videos_unzipped/{video_dir}/{x['video']}\"))\n",
    "\n",
    "print(f\"Dataset length = {len(ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c44886c523194321a45ac116a99d7e09",
      "4c53a11f8b4d48bab7bb07005e09dab2",
      "1e40a2be14574c9cab05c6a2066506c9",
      "6997d1b4ec2e40268bbfe56ff5ad1ddc",
      "0c80b263628d4c37b129278c8cb80fc2",
      "606c508016d34f1bb86d21b2c4e3cfc3",
      "8c9c50dc1265410e82248451e1fdd156",
      "e9e922c6bf14487599e94963b8afd747",
      "e7dd46cf21684684b12361c82c5e01ca",
      "7c13ed62e28e44468633fdfa010a07cb",
      "24b4a2b91c7e484bacb5a543a60dff44"
     ]
    },
    "id": "eySn7X6zlbn3",
    "outputId": "0c291bd5-d7c5-4337-ef61-99d9399d3cad"
   },
   "outputs": [],
   "source": [
    "# Load videos and split them into frames\n",
    "# ds = ds.map(collate_read_video,\n",
    "#             batched=False,\n",
    "#             fn_kwargs={\"path\": f\"{data_dir}/video/videos_unzipped/{video_dir}\"})\n",
    "\n",
    "# Make conversation\n",
    "\n",
    "def make_conversation(sample):\n",
    "    id2choice = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "    question, candidates = sample[\"question\"], sample[\"candidates\"]\n",
    "    answer_i = candidates.index(sample[\"answer\"])\n",
    "    answer_choice = id2choice[answer_i]\n",
    "\n",
    "    mult_choice = \"\\n\"\n",
    "    for i, choice in enumerate(candidates):\n",
    "        mult_choice += f\"{id2choice[i]}. {choice};\\n\"\n",
    "\n",
    "    conversations = [\n",
    "         {'from': 'human', 'value': question + mult_choice + '<video>'},\n",
    "         {'from': 'gpt', 'value': answer_choice + \" \" + sample[\"answer\"]}\n",
    "    ]\n",
    "    sample['conversations'] = conversations\n",
    "    return sample\n",
    "\n",
    "ds = ds.map(make_conversation,\n",
    "            batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwVmDv2dsMqA",
    "outputId": "98874a15-435d-40fb-f929-765fc303223f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': 'ZS9XR.mp4',\n",
       " 'question': 'What happened after the person took the food?',\n",
       " 'candidates': ['Ate the medicine.',\n",
       "  'Tidied up the blanket.',\n",
       "  'Put down the cup/glass/bottle.',\n",
       "  'Took the box.'],\n",
       " 'answer': 'Ate the medicine.',\n",
       " 'start': 1.5,\n",
       " 'end': 17.1,\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': 'What happened after the person took the food?\\nA. Ate the medicine.;\\nB. Tidied up the blanket.;\\nC. Put down the cup/glass/bottle.;\\nD. Took the box.;\\n<video>'},\n",
       "  {'from': 'gpt', 'value': 'A Ate the medicine.'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Apply Processor Patch for Video Support\n",
    "\n",
    "**Note**: There is a known issue with InternVL processor for video frame handling. A patch file (`internvl-processor.diff`) is provided in the workspace. You may need to apply it to fix video frame indexing if you encounter errors during processing.\n",
    "\n",
    "To apply the patch:\n",
    "```bash\n",
    "# Navigate to your transformers installation\n",
    "cd $(python -c \"import transformers; import os; print(os.path.dirname(transformers.__file__))\")\n",
    "\n",
    "# Apply the patch\n",
    "patch -p2 < /path/to/internvl-processor.diff\n",
    "```\n",
    "\n",
    "This patch fixes the binding of video frames to video placeholders in the prompt, ensuring correct frame-to-placeholder mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6JsAJjJx1C4"
   },
   "source": [
    "\n",
    "\n",
    "```json\n",
    "{'id': 578004, 'video': '013901_013950/1025449886.mp4',\n",
    "'conversations':\n",
    "[{'from': 'human', 'value': 'Render a clear and concise summary of the video below.\\n<video>'},\n",
    "{'from': 'gpt', 'value': 'Aerial; drone flight around dangerous eroded steep stony slopes of cabo da roca; granite boulders, sea cliffs along the high coast; long seashore with lighthouse, overlooking the promontory, portugal'}]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRTrOYp6lvD0",
    "outputId": "e0f038f5-bef8-4695-95a3-4efa5e636e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2TokenizerFast(name_or_path='OpenGVLab/InternVL3_5-1B', vocab_size=151643, model_max_length=14588, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151669: AddedToken(\"<img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151670: AddedToken(\"</img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151671: AddedToken(\"<IMG_CONTEXT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151672: AddedToken(\"<quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151673: AddedToken(\"</quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151674: AddedToken(\"<ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151675: AddedToken(\"</ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151676: AddedToken(\"<box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151677: AddedToken(\"</box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model's processor\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "processor.padding_side = \"right\"\n",
    "\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtxYp7h2lvD0"
   },
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "In the next step, you'll need **to define a custom dataset** class and the necessary functions to prepare our data for fine-tuning model. The VideoQADataset class extends the [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class to facilitate loading and processing \"MMBench\". This class will handle the conversion of dataset samples into the format required for training and evaluation by preparing a prompt and making array from videos.\n",
    "\n",
    "Next, you need **to define collate functions** to handle the batching of data during training and evaluation. These functions ensure that the input data is properly formatted and padded.\n",
    "\n",
    "Here use the processor to turn the (video, target token sequence) into the format that the model expects (which is pixel_values, input_ids etc.). Use a dynamic padding of the batches: each batch contains ground truth sequences of varying lengths.\n",
    "\n",
    "Also you can limit the length of the text tokens (input_ids) to a max length due to memory constraints, feel free to expand if your target token sequences are longer (I'd recommend plotting the average token length of your dataset to determine the optimal value).\n",
    "\n",
    "The formatting of the input_ids is super important: you need to respect a so-called [chat template](https://huggingface.co/docs/transformers/main/en/chat_templating).\n",
    "\n",
    "Labels are created for the model by simply copying the inputs to the LLM (input_ids), but with padding tokens replaced by the ignore index of the loss function. This ensures that the model doesn't need to learn to predict padding tokens (used to batch examples together).\n",
    "\n",
    "Why are the labels a copy of the model inputs, you may ask? The model will internally shift the labels one position to the right so that the model will learn to predict the next token. This can be seen here.\n",
    "\n",
    "The collate function for evaluation is different, since there you only need to feed the prompt to the model, as we'll use the `generate()` method to autoregressively generate a completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VideoQADataset(Dataset):\n",
    "#     \"\"\"Dataset for Video QA fine-tuning.\"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         template_name,\n",
    "#         raw_data,\n",
    "#         video_data_dir,\n",
    "#         tokenizer,\n",
    "#         ds_name,\n",
    "#         num_image_token,\n",
    "#         image_size=224,\n",
    "#         is_train=True,\n",
    "#         pad2square=False,\n",
    "#         dynamic_image_size=False,\n",
    "#         use_thumbnail=False,\n",
    "#         min_dynamic_patch=1,\n",
    "#         max_dynamic_patch=6,\n",
    "#         min_num_frame=4,  # for video data\n",
    "#         max_num_frame=12,  # for video data\n",
    "#         sampling_method='rand',  # for video data\n",
    "#         repeat_time=1,\n",
    "#         normalize_type='imagenet',\n",
    "#         random_seed=0,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.ds_name = ds_name\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.group_by_length = None\n",
    "#         self.tcs_loader = None\n",
    "#         self.template_name = template_name\n",
    "#         self.num_image_token = num_image_token\n",
    "#         print(f'[Dataset] num_image_token: {num_image_token}')\n",
    "#         print(f'[Dataset] dynamic_image_size: {dynamic_image_size}')\n",
    "#         print(f'[Dataset] use_thumbnail: {use_thumbnail}')\n",
    "#         print(f'[Dataset] min_dynamic_patch: {min_dynamic_patch}, max_dynamic_patch: {max_dynamic_patch}')\n",
    "\n",
    "#         self.image_size = image_size\n",
    "#         self.is_train = is_train\n",
    "#         self.pad2square = pad2square\n",
    "#         self.max_num_frame = max_num_frame\n",
    "#         self.num_frames = max_num_frame\n",
    "#         self.min_num_frame = min_num_frame\n",
    "#         self.sampling_method = sampling_method\n",
    "\n",
    "#         print('Formatting inputs...Skip in lazy mode')\n",
    "\n",
    "#         self.raw_data = raw_data\n",
    "#         self.rng = np.random.default_rng(seed=random_seed)\n",
    "#         self.rng.shuffle(self.raw_data)\n",
    "\n",
    "#         gc.collect()\n",
    "#         self.root = video_data_dir\n",
    "#         self.cached_data_dict = {}\n",
    "#         self.dynamic_image_size = dynamic_image_size\n",
    "#         self.use_thumbnail = use_thumbnail\n",
    "#         self.min_dynamic_patch = min_dynamic_patch\n",
    "#         self.max_dynamic_patch = max_dynamic_patch\n",
    "#         self.normalize_type = normalize_type\n",
    "\n",
    "#         gc.collect()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.raw_data)\n",
    "\n",
    "#     def get_preprocess_function(self):\n",
    "#         # Select the appropriate preprocessing function based on the template name\n",
    "#         return preprocess_internlm\n",
    "\n",
    "#     def load_image(self, image_path):\n",
    "#         # Load the image using tcs_loader if available, otherwise use PIL\n",
    "#         if self.tcs_loader is not None and 's3://' in image_path:\n",
    "#             return self.tcs_loader(image_path)\n",
    "#         return Image.open(image_path).convert('RGB')\n",
    "\n",
    "#     def get_image_path(self, image_path):\n",
    "#         if image_path.startswith('s3://'):  # for ceph\n",
    "#             image_path = self.root + image_path\n",
    "#         else:  # for local image\n",
    "#             image_path = os.path.join(self.root, image_path)\n",
    "#         return image_path\n",
    "\n",
    "#     def get_transform(self):\n",
    "#         # Build transformation function\n",
    "#         transform = build_transform(is_train=self.is_train, input_size=self.image_size,\n",
    "#                                     pad2square=self.pad2square, normalize_type=self.normalize_type)\n",
    "#         return transform\n",
    "\n",
    "\n",
    "#     def video_get_item(self, data_item):\n",
    "#         # Build transformation function\n",
    "#         transform = self.get_transform()\n",
    "\n",
    "#         # Ensure the first conversation contains a video placeholder\n",
    "#         if '<video>' not in data_item['conversations'][0]['value']:\n",
    "#             data_item['conversations'][0]['value'] = '<video>\\n' + data_item['conversations'][0]['value']\n",
    "\n",
    "#         # Get the video file path\n",
    "#         video_file = data_item['video']\n",
    "#         video_path = os.path.join(self.root, video_file)\n",
    "\n",
    "#         # ИСПРАВЛЕНИЕ: использовать self.max_num_frame\n",
    "#         num_frames = self.max_num_frame\n",
    "#         tcs_loader = getattr(self, 'tcs_loader', None)\n",
    "        \n",
    "#         # Load the video frames\n",
    "#         image_list = read_frames_decord(video_path,\n",
    "#                                         num_frames=num_frames,\n",
    "#                                         client=tcs_loader,\n",
    "#                                         clip=(data_item.get('start', None),\n",
    "#                                               data_item.get('end', None)))\n",
    "\n",
    "#         # Generate special tokens for each video frame\n",
    "#         special_tokens = '\\n'.join(['Frame{}: <image>'.format(i + 1) for i in range(len(image_list))])\n",
    "#         data_item['conversations'][0]['value'] = data_item['conversations'][0]['value'].replace(\n",
    "#             '<video>', special_tokens)\n",
    "\n",
    "#         # Transform each frame image and stack them into a tensor\n",
    "#         pixel_values = [transform(image) for image in image_list]\n",
    "#         pixel_values = torch.stack(pixel_values)\n",
    "#         num_patches = pixel_values.size(0)\n",
    "\n",
    "#         # Select the appropriate preprocessing function based on the template name\n",
    "#         preprocess_function = self.get_preprocess_function()\n",
    "\n",
    "#         # ИСПРАВЛЕНИЕ: добавить group_by_length как атрибут с fallback\n",
    "#         group_by_length = getattr(self, 'group_by_length', None)\n",
    "        \n",
    "#         # Preprocess the conversations and generate the return dictionary\n",
    "#         num_image_tokens = [self.num_image_token] * num_patches\n",
    "#         ret = preprocess_function(self.template_name, [deepcopy(data_item['conversations'])],\n",
    "#                                   self.tokenizer, num_image_tokens, group_by_length=group_by_length,\n",
    "#                                   ds_name=self.ds_name, num_image=num_patches)\n",
    "\n",
    "#         # Create the final return dictionary\n",
    "#         ret = dict(\n",
    "#             input_ids=ret['input_ids'][0],\n",
    "#             labels=ret['labels'][0],\n",
    "#             attention_mask=ret['attention_mask'][0],\n",
    "#             pixel_values=pixel_values,\n",
    "#             image_flags=torch.tensor([1] * num_patches, dtype=torch.long)\n",
    "#         )\n",
    "#         return ret\n",
    "\n",
    "#     def pure_text_get_item(self, data_item):\n",
    "#         # Build transformation function\n",
    "#         transform = self.get_transform()\n",
    "\n",
    "#         # Create a blank white image\n",
    "#         image = Image.new('RGB', (224, 224), (255, 255, 255))\n",
    "\n",
    "#         # Dynamically preprocess the image to generate patches\n",
    "#         images = dynamic_preprocess(image, min_num=self.min_dynamic_patch, max_num=1,\n",
    "#                                     image_size=self.image_size, use_thumbnail=self.use_thumbnail)\n",
    "\n",
    "#         # Apply the transformation to each image patch and stack them into a tensor\n",
    "#         pixel_values = [transform(image) for image in images]\n",
    "#         pixel_values = torch.stack(pixel_values)\n",
    "#         num_patches = pixel_values.size(0)\n",
    "\n",
    "#         # Ensure there is only one patch\n",
    "#         assert num_patches == 1, f'The number of patches should be 1, but got {num_patches}.'\n",
    "\n",
    "#         # Select the appropriate preprocessing function based on the template name\n",
    "#         preprocess_function = self.get_preprocess_function()\n",
    "\n",
    "#         # Preprocess the conversations and generate the return dictionary\n",
    "#         ret = preprocess_function(self.template_name, [deepcopy(data_item['conversations'])],\n",
    "#                                   self.tokenizer, [self.num_image_token * num_patches], text_only=True,\n",
    "#                                   group_by_length=self.group_by_length, ds_name=self.ds_name)\n",
    "\n",
    "#         # Create the final return dictionary\n",
    "#         ret = dict(\n",
    "#             input_ids=ret['input_ids'][0],\n",
    "#             labels=ret['labels'][0],\n",
    "#             attention_mask=ret['attention_mask'][0],\n",
    "#             pixel_values=pixel_values,\n",
    "#             image_flags=torch.tensor([0] * num_patches, dtype=torch.long)\n",
    "#         )\n",
    "#         return ret\n",
    "\n",
    "#     def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "#         i = i % len(self.raw_data)\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 data_item = json.loads(self.raw_data[i])\n",
    "#                 if 'image' in data_item and len(data_item['image']) != 0:\n",
    "#                     if type(data_item['image']) == list:\n",
    "#                         ret = self.multi_modal_multi_image_get_item(data_item)\n",
    "#                     else:\n",
    "#                         ret = self.multi_modal_get_item(data_item)\n",
    "#                 elif 'video' in data_item and data_item['video'] is not None and data_item['video'] != '':\n",
    "#                     ret = self.video_get_item(data_item)\n",
    "#                 else:\n",
    "#                     ret = self.pure_text_get_item(data_item)\n",
    "#                 break\n",
    "#             except Exception as e:\n",
    "#                 print(e, self.ds_name, flush=True)\n",
    "#                 if not isinstance(e, UnidentifiedImageError):\n",
    "#                     traceback.print_exc()\n",
    "#                 data_item = json.loads(self.raw_data[i])\n",
    "#                 if 'image' in data_item:\n",
    "#                     if type(data_item['image']) == list:\n",
    "#                         images = [self.root + item for item in data_item['image']]\n",
    "#                         print(f'Failed to load image: {images}, the dataset is: {self.ds_name}')\n",
    "#                     else:\n",
    "#                         if data_item['image'].startswith('s3://'):\n",
    "#                             data_path = self.root + data_item['image']\n",
    "#                         else:\n",
    "#                             data_path = os.path.join(self.root, data_item['image'])\n",
    "#                         print(f'Failed to load image: {data_path}, the dataset is: {self.ds_name}')\n",
    "#                 elif 'video' in data_item:\n",
    "#                     data_path = os.path.join(self.root, data_item['video'])\n",
    "#                     print(f'Failed to load video: {data_path}, the dataset is: {self.ds_name}')\n",
    "#                 i = random.randint(0, len(self.raw_data) - 1)\n",
    "#         return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VideoQADatasetWithAnswers(VideoQADataset):\n",
    "#     \"\"\"\n",
    "#     Wrapper around VideoQADataset that also stores original example data\n",
    "#     for extracting ground truth answers during evaluation.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, original_data, *args, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.original_data = original_data\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         item = super().__getitem__(idx)\n",
    "        \n",
    "#         original_example = self.original_data[idx]\n",
    "#         item['original_example'] = original_example\n",
    "        \n",
    "#         return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dataset and collate functions\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def read_video_pyav(video_path, n_frames=8):\n",
    "    \"\"\"Read video frames using PyAV\"\"\"\n",
    "    import av\n",
    "    container = av.open(video_path)\n",
    "    video_stream = container.streams.get(0)[0]\n",
    "    \n",
    "    total_frames = video_stream.frames\n",
    "    indices = np.linspace(0, total_frames - 1, n_frames).astype(int)\n",
    "    \n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video_stream)):\n",
    "        if i in indices:\n",
    "            frames.append(frame.to_ndarray(format=\"rgb24\"))\n",
    "        if len(frames) == n_frames:\n",
    "            break\n",
    "    \n",
    "    return np.stack(frames)\n",
    "\n",
    "def build_transform(image_size=224):\n",
    "    \"\"\"Build image transforms\"\"\"\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "def format_text_for_internvl(conversations, is_train=True):\n",
    "    \"\"\"Format text for InternVL chat template\"\"\"\n",
    "    if is_train:\n",
    "        human_msg = conversations[0]['value']\n",
    "        assistant_msg = conversations[1]['value']\n",
    "        # Replace <video> with frame tokens\n",
    "        frame_tokens = '\\n'.join([f'Frame{i+1}: <image>' for i in range(8)])\n",
    "        human_msg = human_msg.replace('<video>', frame_tokens)\n",
    "        text = f\"<s>[INST] {human_msg} [/INST] {assistant_msg}</s>\"\n",
    "    else:\n",
    "        human_msg = conversations[0]['value']\n",
    "        human_msg = human_msg.replace('<video>', '')\n",
    "        text = f\"<s>[INST] {human_msg} [/INST]\"\n",
    "    return text\n",
    "\n",
    "class SimpleVideoDataset(Dataset):\n",
    "    \"\"\"Simple video dataset for MVBench\"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, video_dir, tokenizer, image_size=224, is_train=True):\n",
    "        self.data_list = data_list\n",
    "        self.video_dir = video_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        self.transform = build_transform(image_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_list[idx]\n",
    "        \n",
    "        # Load video frames\n",
    "        video_path = f\"{self.video_dir}/{item['video']}\"\n",
    "        video_frames = read_video_pyav(video_path)\n",
    "        \n",
    "        # Transform frames\n",
    "        pixel_values = [self.transform(Image.fromarray(frame)) for frame in video_frames]\n",
    "        pixel_values = torch.stack(pixel_values)  # (N, C, H, W)\n",
    "        \n",
    "        # Format text\n",
    "        formatted_text = format_text_for_internvl(item['conversations'], self.is_train)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = self.tokenizer(\n",
    "            formatted_text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Create labels (copy of input_ids for training)\n",
    "        labels = input_ids.clone() if self.is_train else torch.full_like(input_ids, -100)\n",
    "        \n",
    "        # Create image flags (1 for video frames)\n",
    "        image_flags = torch.ones(len(pixel_values), dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'pixel_values': pixel_values,\n",
    "            'image_flags': image_flags,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "def simple_train_collate_fn(batch):\n",
    "    \"\"\"Simple training collate function\"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    image_flags = [item['image_flags'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad text sequences\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    pad_id = 0  # tokenizer.pad_token_id\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        pad_len = max_len - len(input_ids[i])\n",
    "        padded_input_ids.append(torch.cat([input_ids[i], torch.full((pad_len,), pad_id, dtype=torch.long)]))\n",
    "        padded_attention_mask.append(torch.cat([attention_mask[i], torch.zeros(pad_len, dtype=torch.long)]))\n",
    "        padded_labels.append(torch.cat([labels[i], torch.full((pad_len,), -100, dtype=torch.long)]))\n",
    "    \n",
    "    # Stack text data\n",
    "    input_ids_batch = torch.stack(padded_input_ids)\n",
    "    attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    labels_batch = torch.stack(padded_labels)\n",
    "    \n",
    "    # Flatten video frames\n",
    "    flattened_pixel_values = []\n",
    "    flattened_image_flags = []\n",
    "    \n",
    "    for pv, flags in zip(pixel_values, image_flags):\n",
    "        for frame_idx in range(pv.size(0)):\n",
    "            flattened_pixel_values.append(pv[frame_idx])\n",
    "            flattened_image_flags.append(flags[frame_idx] if frame_idx < len(flags) else torch.tensor(1))\n",
    "    \n",
    "    pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "    image_flags_batch = torch.stack(flattened_image_flags)\n",
    "    \n",
    "    return input_ids_batch, attention_mask_batch, pixel_values_batch, labels_batch, image_flags_batch\n",
    "\n",
    "def simple_eval_collate_fn(batch):\n",
    "    \"\"\"Simple evaluation collate function\"\"\"\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    image_flags = [item['image_flags'] for item in batch]\n",
    "    \n",
    "    # Pad text sequences\n",
    "    max_len = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    \n",
    "    pad_id = 0  # tokenizer.pad_token_id\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        pad_len = max_len - len(input_ids[i])\n",
    "        padded_input_ids.append(torch.cat([input_ids[i], torch.full((pad_len,), pad_id, dtype=torch.long)]))\n",
    "        padded_attention_mask.append(torch.cat([attention_mask[i], torch.zeros(pad_len, dtype=torch.long)]))\n",
    "    \n",
    "    # Stack text data\n",
    "    input_ids_batch = torch.stack(padded_input_ids)\n",
    "    attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    \n",
    "    # Flatten video frames\n",
    "    flattened_pixel_values = []\n",
    "    flattened_image_flags = []\n",
    "    \n",
    "    for pv, flags in zip(pixel_values, image_flags):\n",
    "        for frame_idx in range(pv.size(0)):\n",
    "            flattened_pixel_values.append(pv[frame_idx])\n",
    "            flattened_image_flags.append(flags[frame_idx] if frame_idx < len(flags) else torch.tensor(1))\n",
    "    \n",
    "    pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "    image_flags_batch = torch.stack(flattened_image_flags)\n",
    "    \n",
    "    return input_ids_batch, attention_mask_batch, pixel_values_batch, None, image_flags_batch\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimpleVideoDataset(\n",
    "    data_list=dataset[\"train\"],\n",
    "    video_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    image_size=224,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "eval_dataset = SimpleVideoDataset(\n",
    "    data_list=dataset[\"test\"],\n",
    "    video_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    image_size=224,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Eval dataset: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sH2oWArlvD1"
   },
   "source": [
    "# Dataset and collate functions based on template\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import UnidentifiedImageError\n",
    "import traceback\n",
    "from copy import deepcopy\n",
    "\n",
    "def read_video_pyav(video_path, start, end, n_frames=8):\n",
    "    \"\"\"Read video frames using PyAV\"\"\"\n",
    "    import av\n",
    "    import bisect\n",
    "    \n",
    "    container = av.open(video_path)\n",
    "    video = container.streams.get(0)[0]\n",
    "\n",
    "    av_timestamps = [\n",
    "        int(packet.pts * video.time_base) for packet in container.demux(video) if packet.pts is not None\n",
    "    ]\n",
    "\n",
    "    av_timestamps.sort()\n",
    "    start_id = bisect.bisect_left(av_timestamps, start)\n",
    "    end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "    if end_id  - start_id < 10:\n",
    "        end_id += 10\n",
    "        start_id -= 10\n",
    "\n",
    "    end_id = min(len(av_timestamps) - 1, end_id)\n",
    "    start_id = max(1, start_id)\n",
    "\n",
    "    indices = np.linspace(start_id, end_id, n_frames).astype(int)\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_id:\n",
    "            break\n",
    "        if i >= start_id and i in indices:\n",
    "            frames.append(frame)\n",
    "    assert len(frames) == n_frames, f\"Got {len(frames)} frames but should be {n_frames}\"\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def build_transform(is_train=True, input_size=224, pad2square=False, normalize_type='imagenet'):\n",
    "    \"\"\"Build image transforms\"\"\"\n",
    "    if normalize_type == 'imagenet':\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    else:\n",
    "        normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((input_size, input_size), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "def preprocess_internvl2_5(template_name, sources, tokenizer, num_image_token_list, text_only=False, \n",
    "                         group_by_length=False, ds_name=None, num_image=1):\n",
    "    \"\"\"Simple preprocessing function for InternVL2.5\"\"\"\n",
    "    # For simplicity, we'll use manual text formatting\n",
    "    conversations = sources[0]\n",
    "    \n",
    "    if len(conversations) >= 2:\n",
    "        human_msg = conversations[0]['value']\n",
    "        assistant_msg = conversations[1]['value']\n",
    "        \n",
    "        # Format for InternVL\n",
    "        frame_tokens = '\\n'.join([f'Frame{i+1}: <image>' for i in range(num_image)])\n",
    "        human_msg = human_msg.replace('<video>', frame_tokens)\n",
    "        \n",
    "        text = f\"<s>[INST] {human_msg} [/INST] {assistant_msg}</s>\"\n",
    "    else:\n",
    "        human_msg = conversations[0]['value']\n",
    "        human_msg = human_msg.replace('<video>', '')\n",
    "        text = f\"<s>[INST] {human_msg} [/INST]\"\n",
    "    \n",
    "    # Tokenize\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    \n",
    "    # Create labels\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'labels': labels,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "class VideoQADataset(Dataset):\n",
    "    \"\"\"Dataset for Video QA fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        template_name,\n",
    "        raw_data,\n",
    "        video_data_dir,\n",
    "        tokenizer,\n",
    "        ds_name,\n",
    "        num_image_token,\n",
    "        image_size=224,\n",
    "        is_train=True,\n",
    "        pad2square=False,\n",
    "        dynamic_image_size=False,\n",
    "        use_thumbnail=False,\n",
    "        min_dynamic_patch=1,\n",
    "        max_dynamic_patch=6,\n",
    "        min_num_frame=8,  # for video data\n",
    "        max_num_frame=8,  # for video data\n",
    "        sampling_method='rand',  # for video data\n",
    "        repeat_time=1,\n",
    "        normalize_type='imagenet',\n",
    "        random_seed=0,\n",
    "    ):\n",
    "        super(VideoQADataset, self).__init__()\n",
    "        self.ds_name = ds_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.template_name = template_name\n",
    "        self.num_image_token = num_image_token\n",
    "        print(f'[Dataset] num_image_token: {num_image_token}')\n",
    "        print(f'[Dataset] dynamic_image_size: {dynamic_image_size}')\n",
    "        print(f'[Dataset] use_thumbnail: {use_thumbnail}')\n",
    "        print(f'[Dataset] min_dynamic_patch: {min_dynamic_patch}, max_dynamic_patch: {max_dynamic_patch}')\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        self.pad2square = pad2square\n",
    "        self.max_num_frame = max_num_frame\n",
    "        self.min_num_frame = min_num_frame\n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "        print('Formatting inputs...Skip in lazy mode')\n",
    "\n",
    "        self.raw_data = raw_data.shuffle(seed=random_seed) if hasattr(raw_data, 'shuffle') else raw_data\n",
    "\n",
    "        gc.collect()\n",
    "        self.root = video_data_dir\n",
    "        self.cached_data_dict = {}\n",
    "        self.dynamic_image_size = dynamic_image_size\n",
    "        self.use_thumbnail = use_thumbnail\n",
    "        self.min_dynamic_patch = min_dynamic_patch\n",
    "        self.max_dynamic_patch = max_dynamic_patch\n",
    "        self.normalize_type = normalize_type\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def get_preprocess_function(self):\n",
    "        # Select the appropriate preprocessing function based on the template name\n",
    "        return preprocess_internvl2_5\n",
    "        \n",
    "    def get_transform(self):\n",
    "        # Build transformation function\n",
    "        transform = build_transform(is_train=self.is_train, input_size=self.image_size,\n",
    "                                    pad2square=self.pad2square, normalize_type=self.normalize_type)\n",
    "        return transform\n",
    "\n",
    "    def video_get_item(self, data_item):\n",
    "        # Build transformation function\n",
    "        transform = self.get_transform()\n",
    "\n",
    "        # Ensure the first conversation contains a video placeholder\n",
    "        if '<video>' not in data_item['conversations'][0]['value']:\n",
    "            data_item['conversations'][0]['value'] = '<video>\\n' + data_item['conversations'][0]['value']\n",
    "\n",
    "        # Get the video file path\n",
    "        video_file = data_item['video']\n",
    "        video_path = os.path.join(self.root, video_file)\n",
    "        \n",
    "        # Use read_video_pyav instead of read_frames_decord\n",
    "        image_list = read_video_pyav(video_path,\n",
    "                                     start=data_item.get('start', 0),\n",
    "                                     end=data_item.get('end', 1000000),\n",
    "                                     n_frames=self.max_num_frame)\n",
    "        \n",
    "        # Convert numpy arrays to PIL Images\n",
    "        image_list = [Image.fromarray(frame) for frame in image_list]\n",
    "        \n",
    "        # Generate special tokens for each video frame\n",
    "        special_tokens = '\\n'.join(['Frame{}: <image>'.format(i + 1) for i in range(len(image_list))])\n",
    "        data_item['conversations'][0]['value'] = data_item['conversations'][0]['value'].replace(\n",
    "            '<video>', special_tokens)\n",
    "\n",
    "        # Transform each frame image and stack them into a tensor\n",
    "        pixel_values = [transform(image) for image in image_list]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches = pixel_values.size(0)\n",
    "\n",
    "        # Select the appropriate preprocessing function based on the template name\n",
    "        preprocess_function = self.get_preprocess_function()\n",
    "\n",
    "        # Preprocess the conversations and generate the return dictionary\n",
    "        num_image_tokens = [self.num_image_token] * num_patches\n",
    "\n",
    "        if self.is_train:\n",
    "            # For training, use full conversation (human + assistant)\n",
    "            sources = [deepcopy(data_item['conversations'])]\n",
    "        else:\n",
    "            # For evaluation, use only human part (prompt)\n",
    "            sources = [deepcopy(data_item['conversations'])]\n",
    "        \n",
    "        ret = preprocess_function(template_name=self.template_name, \n",
    "                                  sources=sources,\n",
    "                                  tokenizer=self.tokenizer,\n",
    "                                  num_image_token_list=num_image_tokens,\n",
    "                                  text_only=False,\n",
    "                                  group_by_length=False,\n",
    "                                  ds_name=self.ds_name, \n",
    "                                  num_image=num_patches\n",
    "        )\n",
    "\n",
    "        conv = data_item['conversations']\n",
    "        \n",
    "        # Extract the correct answer choice from the GPT response\n",
    "        answer_choice = ''\n",
    "        if len(conv) > 1 and 'value' in conv[1]:\n",
    "            gpt_response = conv[1]['value']\n",
    "            # Extract just the choice letter (A, B, C, D) - usually first character\n",
    "            answer_choice = gpt_response.strip()[0]  # Get first character (A, B, C, or D)\n",
    "            \n",
    "\n",
    "        # Create the final return dictionary\n",
    "        ret = dict(\n",
    "            input_ids=ret['input_ids'][0],\n",
    "            labels=ret['labels'][0],\n",
    "            attention_mask=ret['attention_mask'][0],\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=torch.tensor([1] * num_patches, dtype=torch.long),\n",
    "            answer_choice=answer_choice\n",
    "        )\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def pure_text_get_item(self, data_item):\n",
    "        \"\"\"Handle text-only examples\"\"\"\n",
    "        # Simple text processing\n",
    "        sources = [deepcopy(data_item['conversations'])]\n",
    "        \n",
    "        ret = preprocess_internvl2_5(template_name=self.template_name, \n",
    "                                    sources=sources,\n",
    "                                    tokenizer=self.tokenizer,\n",
    "                                    num_image_token_list=[0],\n",
    "                                    text_only=True,\n",
    "                                    group_by_length=False,\n",
    "                                    ds_name=self.ds_name, \n",
    "                                    num_image=0)\n",
    "        \n",
    "        conv = data_item['conversations']\n",
    "        answer_choice = ''\n",
    "        if len(conv) > 1 and 'value' in conv[1]:\n",
    "            answer_choice = conv[1]['value'].strip()[0]\n",
    "\n",
    "        return dict(\n",
    "            input_ids=ret['input_ids'][0],\n",
    "            labels=ret['labels'][0],\n",
    "            attention_mask=ret['attention_mask'][0],\n",
    "            pixel_values=torch.zeros(1, 3, self.image_size, self.image_size),\n",
    "            image_flags=torch.tensor([0], dtype=torch.long),\n",
    "            answer_choice=answer_choice\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        i = i % len(self.raw_data)\n",
    "        while True:\n",
    "            try:\n",
    "                data_item = self.raw_data[i]\n",
    "                if 'image' in data_item and len(data_item['image']) != 0:\n",
    "                    # For simplicity, handle images as text in this version\n",
    "                    ret = self.pure_text_get_item(data_item)\n",
    "                elif 'video' in data_item and data_item['video'] is not None and data_item['video'] != '':\n",
    "                    ret = self.video_get_item(data_item)\n",
    "                else:\n",
    "                    ret = self.pure_text_get_item(data_item)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e, self.ds_name, flush=True)\n",
    "                if not isinstance(e, UnidentifiedImageError):\n",
    "                    traceback.print_exc()\n",
    "                data_item = self.raw_data[i]\n",
    "                if 'image' in data_item:\n",
    "                    if type(data_item['image']) == list:\n",
    "                        images = [self.root + item for item in data_item['image']]\n",
    "                        print(f'Failed to load image: {images}, the dataset is: {self.ds_name}')\n",
    "                    else:\n",
    "                        if data_item['image'].startswith('s3://'):\n",
    "                            data_path = self.root + data_item['image']\n",
    "                        else:\n",
    "                            data_path = os.path.join(self.root, data_item['image'])\n",
    "                        print(f'Failed to load image: {data_path}, the dataset is: {self.ds_name}')\n",
    "                elif 'video' in data_item:\n",
    "                    data_path = os.path.join(self.root, data_item['video'])\n",
    "                    print(f'Failed to load video: {data_path}, the dataset is: {self.ds_name}')\n",
    "                i = random.randint(0, len(self.raw_data) - 1)\n",
    "        return ret\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VideoQADataset(\n",
    "    template_name='internlm2-chat',\n",
    "    raw_data=dataset[\"train\"],\n",
    "    video_data_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name=\"mvbench_train\",\n",
    "    num_image_token=256,\n",
    "    image_size=224,\n",
    "    is_train=True,\n",
    "    min_num_frame=8,\n",
    "    max_num_frame=8\n",
    ")\n",
    "\n",
    "eval_dataset = VideoQADataset(\n",
    "    template_name='internlm2-chat',\n",
    "    raw_data=dataset[\"test\"],\n",
    "    video_data_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name=\"mvbench_eval\",\n",
    "    num_image_token=256,\n",
    "    image_size=224,\n",
    "    is_train=False,\n",
    "    min_num_frame=8,\n",
    "    max_num_frame=8\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Eval dataset: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DenEK2IqlvD1"
   },
   "outputs": [],
   "source": [
    "# Collate functions based on template\n",
    "def train_collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function for training that batches examples together.\n",
    "    Handles padding for text and stacks video frames.\n",
    "    \"\"\"\n",
    "    # Extract all components from examples\n",
    "    input_ids = [example['input_ids'] for example in examples]\n",
    "    attention_mask = [example['attention_mask'] for example in examples]\n",
    "    pixel_values = [example['pixel_values'] for example in examples]\n",
    "    image_flags = [example['image_flags'] for example in examples]\n",
    "    labels = [example['labels'] for example in examples]\n",
    "    answer_choices = [example['answer_choice'] for example in examples]\n",
    "\n",
    "    # Pad input_ids, attention_mask, and labels to the same length\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        # Pad input_ids\n",
    "        pad_length = max_length - len(input_ids[i])\n",
    "        padded_input_ids.append(\n",
    "            torch.cat([\n",
    "                input_ids[i],\n",
    "                torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Pad attention_mask\n",
    "        padded_attention_mask.append(\n",
    "            torch.cat([\n",
    "                attention_mask[i],\n",
    "                torch.zeros(pad_length, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Pad labels (using -100 for ignore index)\n",
    "        padded_labels.append(\n",
    "            torch.cat([\n",
    "                labels[i],\n",
    "                torch.full((pad_length,), -100, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    # Stack all tensors\n",
    "    input_ids_batch = torch.stack(padded_input_ids)\n",
    "    attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    labels_batch = torch.stack(padded_labels)\n",
    "    \n",
    "    # Handle pixel_values - FLATTEN the video frames\n",
    "    # Instead of padding frames, we'll process each frame as a separate image\n",
    "    flattened_pixel_values = []\n",
    "    flattened_image_flags = []\n",
    "    \n",
    "    for i, (pv, flags) in enumerate(zip(pixel_values, image_flags)):\n",
    "        # pv shape: (num_frames, channels, height, width)\n",
    "        # We need to flatten this to (num_frames * batch_size, channels, height, width)\n",
    "        num_frames = pv.size(0)\n",
    "        for frame_idx in range(num_frames):\n",
    "            flattened_pixel_values.append(pv[frame_idx])\n",
    "            # Handle image_flags properly\n",
    "            if frame_idx < len(flags):\n",
    "                flattened_image_flags.append(flags[frame_idx])\n",
    "            else:\n",
    "                flattened_image_flags.append(torch.tensor(1))\n",
    "    \n",
    "    # Stack flattened pixel values\n",
    "    if flattened_pixel_values:\n",
    "        pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "        image_flags_batch = torch.stack(flattened_image_flags)\n",
    "    else:\n",
    "        pixel_values_batch = torch.tensor([])\n",
    "        image_flags_batch = torch.tensor([])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_batch,\n",
    "        'attention_mask': attention_mask_batch,\n",
    "        'pixel_values': pixel_values_batch,\n",
    "        'labels': labels_batch,\n",
    "        'image_flags': image_flags_batch,\n",
    "        'answer_choices': answer_choices\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function for evaluation that batches examples together.\n",
    "    Similar to training but doesn't need labels for generation.\n",
    "    \"\"\"\n",
    "    # Extract all components from examples\n",
    "    input_ids = [example['input_ids'] for example in examples]\n",
    "    attention_mask = [example['attention_mask'] for example in examples]\n",
    "    pixel_values = [example['pixel_values'] for example in examples]\n",
    "    answer_choices = [example['answer_choice'] for example in examples]\n",
    "\n",
    "    # Pad input_ids and attention_mask to the same length\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        # Pad input_ids\n",
    "        pad_length = max_length - len(input_ids[i])\n",
    "        padded_input_ids.append(\n",
    "            torch.cat([\n",
    "                input_ids[i],\n",
    "                torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Pad attention_mask\n",
    "        padded_attention_mask.append(\n",
    "            torch.cat([\n",
    "                attention_mask[i],\n",
    "                torch.zeros(pad_length, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    # Stack all tensors\n",
    "    input_ids_batch = torch.stack(padded_input_ids)\n",
    "    attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    \n",
    "    # Handle pixel_values - FLATTEN the video frames\n",
    "    flattened_pixel_values = []\n",
    "    \n",
    "    for i, pv in enumerate(pixel_values):\n",
    "        # pv shape: (num_frames, channels, height, width)\n",
    "        num_frames = pv.size(0)\n",
    "        for frame_idx in range(num_frames):\n",
    "            flattened_pixel_values.append(pv[frame_idx])\n",
    "    \n",
    "    # Stack flattened pixel values\n",
    "    if flattened_pixel_values:\n",
    "        pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "    else:\n",
    "        pixel_values_batch = torch.tensor([])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_batch,\n",
    "        'attention_mask': attention_mask_batch,\n",
    "        'pixel_values': pixel_values_batch,\n",
    "        'answer_choices': answer_choices\n",
    "    }\n",
    "\n",
    "# Simple collate functions for backward compatibility\n",
    "def simple_train_collate_fn(batch):\n",
    "    \"\"\"Wrapper for train_collate_fn to maintain compatibility\"\"\"\n",
    "    result = train_collate_fn(batch)\n",
    "    return result['input_ids'], result['attention_mask'], result['pixel_values'], result['labels'], result['image_flags']\n",
    "\n",
    "def simple_eval_collate_fn(batch):\n",
    "    \"\"\"Wrapper for eval_collate_fn to maintain compatibility\"\"\"\n",
    "    result = eval_collate_fn(batch)\n",
    "    return result['input_ids'], result['attention_mask'], result['pixel_values'], None, None\n",
    "\n",
    "print(\"Collate functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "X_DDgpOulvD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shape: (8, 270, 480, 3)\n",
      "Question: What happened after the person held the clothes?\n",
      "Answer: Threw the pillow.\n",
      "Candidates: ['Took the broom.', 'Threw the pillow.', 'Sat on the sofa/couch.', 'Put down the pillow.']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "example = dataset['train'][0]\n",
    "# Read the video for visualization\n",
    "video_path = f\"{data_dir}/video/videos_unzipped/{video_dir}/{example['video']}\"\n",
    "clip = read_video_pyav(video_path, example.get('start', 1), example.get('end', 1e+10), n_frames=8)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip)\n",
    "\n",
    "print(f\"Video shape: {video.shape}\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Answer: {example['answer']}\")\n",
    "print(f\"Candidates: {example['candidates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aLtCWizwlvD1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQABgLdtZGF0AAACrgYF//+q\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2NCByMzEwOCAzMWUxOWY5IC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMyAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTE1\n",
       "IGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50\n",
       "ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi\n",
       "X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w\n",
       "PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0zIHNjZW5lY3V0PTQwIGludHJhX3Jl\n",
       "ZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAu\n",
       "NjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAh\n",
       "DGWIhAAR//73iB8yy2n5OtdyEeetLq0fUO5GcV6kvf4gAAADAAADAAqMEdMpfgXXeEq6ggAAAwD5\n",
       "ACfg2AhoaMLfwZo7gFrfwuLkqr+Q7/J4sDl4AAwQjRX6kS3BUDdBobiRzUsy4CF6NRPyUCGbrlVn\n",
       "qxmsC+t2mOGDUQnfhe3+YBHhN4hesNjaD0lcQ1ZUe/iP0uKQ5WUuuZLVv4IRpKFjPZWQSKiHQvck\n",
       "coCrl/msg0W0ebbxySrAsgXVx3zP2LsEcgCCW5v9Nzj9hOwU8OQ5GQxO99b1RqwcNEDYqApJHEiv\n",
       "HVFAlIsiyVO2fAZHJv/kN0e7rXuDuef0ugC0o4CMJFBt1fv+htOtM10ajgqnMpbu8QMIRD7LjjEL\n",
       "AM7frKPRQySe8kwRoid1TJjqPGecMoWeKlTfgpmJscHdP22c5ImwmjpXF75znF9rTIB1oRW7jksY\n",
       "ZhMDFFvq4d9AQ7z2lW00b7hN3t8DTRtOkzCQ6H9YSvZtn3LKgTZDVg+CG3qAnyBa+CpiXFVN47dM\n",
       "1pjm3vg3L5XevguRBsYhXq8uCPMdP0KIzOVDUOb96G7puf/4JwwhO23NCaoTWmM4oZ0SdCdPR6Ik\n",
       "Nw6JBsJeSg6rmrUagcwP3qv/NguM3J2preuZ5ZJ3FyHK2yOP1RBGkTG7BGc8ffwwxAU2SuGhRSrP\n",
       "kFKPP0v1w2Xsbg+r/vGBzA2iyiCDa/Z1BbOjBXe0KUT0k5/xZxnTrlo9hTFG3RtsgvH35X+p3Zud\n",
       "v8zSub/qCuHS5xh0e1rrvC4CX+kerNuBAgzdQhR+W7CjBZNjNZENvo0zK3F9yVLitwxAs9+Rr4cn\n",
       "zR+rjrL12x7Z6jpLzgKL+WRQUi+U3iG+YuDfdQRRd8STItGJTj13/Pa7Ux2n39plWQ7ggZ6VTdoQ\n",
       "vHs9Q55Vjb1TNoJ2+4Ic7/U9HgjcDWcO+xRwEPflEOh0sC/ofgq9jppgB0TE2Tn6/Tj1rx2mAAyn\n",
       "XPpt3PD+b4eSAn5PFciwwrf0aIyyaMLeKk59u1mvL4gpSnXW2HQmyjuBF3+jVJe5uP3uSe/9eA4x\n",
       "1JWb1meiFyjT/UUEAf4r6VHw8U/arvLdgLRAUUr/C5eEzv4hj7MDSGdzCUXnREeqeKks0YtOeTdh\n",
       "qSVP7C4q7Nn/5yWjMqVuqxOUuS9ERf/knIdyUkGOZ63pso/07wErM9Oyop4By9N4rYDmB6gvbjP8\n",
       "AQ9izgKjs88FA8DJUN8XcoaDvm4TIuY8A87GRKx7fR1CBN2d0Z8StuFutDfnMTWtRkT4UgxOM5SA\n",
       "fDQ9PNF0hEkxxkL51xFW02Zb4nY/ur3DnOQNk4u6JynOC3btkYqogbBt/ze9DGO0RkbhWqGfzft6\n",
       "/DTkQE1yIjif9uyUZmbUXBSKSajR+rkiU4lpSdUZ5mXVNiZ2QHGwnNLGBL51La6hT0uqlnnmOedr\n",
       "SWg49xRhfg6cPDBRATIhMvD9Tq4O5WHMBP+/Pgg61NMiYmjbBP7vthHCBUB+j+8O+KQeD8bPBjHW\n",
       "vNQ8t6vUxgChOaXhzmJo1MluLTCirPHc/moeeKSixI3vMrxHO3Wo0+mQMrUo19tDqVn77zQWk3ri\n",
       "cILhqK2peXSYUcHI1mxEAAjh6i/BDDkchJHnbFD/KhXhM8KvaMV+AoawChQEQuPiZvSl0ADZVkmX\n",
       "RKbK/BMmNiX1PPS+x5i09oT0nRPmNcNkKVIXUSGdQ/heeXxPRhgVCr/fBs82s3PkFe29qHAe6ONc\n",
       "F5bTVAMLQ93DOab/BY5psie8bxQAFAxIaPJk431KxQoDDK3pra7UM6q006ro/5oxswft2SM6yt3+\n",
       "Z1qRVVXLvZWCjJnCRsqyitYwyvaNISb6U03XS3OFEgfPZGCe8Q37H9+Qbx+rObRGZUO0vARwD9Xf\n",
       "/MhwojlSxIDNBRp4i71Jq9xO9Hv34oB4bZSWnNNOo6SjjQfzc+IBMTKwPgQ749eCF0vP74/eykVm\n",
       "iCJFw3BEsoAzkfhJjtq0Xpoyo6y8bsqjsZ0whxC5YwmeJptJc8AVauZicK36VQeCq9cDejc3Nqts\n",
       "CHC9QXCDKr0PNZE/ht56/krwUJYEv64hPJghrgc6XoxJWJQJjS3SB+EQAHcUJxxdTwbmn6vtZ9Ie\n",
       "0ePz6TNov/WZdYqHcao7gRyY2vSeAHhyaJHFVqCb/GFscam65fdj+/cEAEPhT/nszAfwx6HWNkri\n",
       "sR2kwz2oGhGqOnxNehhXZHddYvt9YJy+1uOUm8dQ4OrWANF7liwRDbVL093GVxJQZsKdKlAP7X3F\n",
       "5pcc5vSrUxivQMfqmRCbS/G8EJOURCVu//jf/D7iMPBWeqD23mM0tjyhiG4v7viFdHy+OtPm0nIk\n",
       "PftWKdbS1S83WDilDsRX77RdPWvMv1zr8gE6bCwekvXneAZSYe3VMIpXFeOmVZ/lLP75C8+ASmJ3\n",
       "K6iXVKlvt1T3+La3NCzPy0kkJIXPB6RhCG1RFB0unoigkLzROsOeztE2/0edzwJuZDkHfxI45Gub\n",
       "tTT3351EwpxOe0bf1kFzSuf8Alh/jLYKcen904uMeeaisoPqwLExzoUNkAbT1N+0Z3HSnw1ZtlMM\n",
       "P8Irrm0hLNvbdsG6wRE1GbigqNVs1g+/6+HW/V1Kk3YidkCc0D+CrmTuGN/V9/Hfeudr9MGTEEBj\n",
       "VQxdrjjtliNeb1qA5ZPwn1moxFXJ/Svn2sV9nQXFyVR7yp1mSiBqNvSG0do01Z6XXNcAMuCNyK5r\n",
       "8qnEFDcMutzVJiENHGzKLrU/Dhkht+ofIMnFi2pj9CSnG7SNe4dYujpgt53602G/ONAbKG89xULe\n",
       "2zi6pKDSoVzmp9ZPmmU+fJ2x1IC0aBRLwCCgCRDO5d7xhxmbPmN67dkab5GYj4ZH/z9GZF9u6xOV\n",
       "wZS0DmFKX6WwlWapWbV+nPoV8tl2gLEm7ZEpOYDFr1mAoYxHbV0GNrRtK+AcVveXEi8wEqLgq/Kz\n",
       "rB+l+Gmsfp8yPvBjmWqysLykaQ5lo2HbOfutDLpiRgX7l2sTSpk1JR3zImJ/svjz62hcVQrhFdHq\n",
       "N/YeGFbuGLdpuKPfu6ynFfpDlxQmidkMm8fiS3sf3MaXiBSS6nGn4gZxJWA8F+JKFIblUcMcfvak\n",
       "47D0pCTpEsWhLJZtY7s0INoKQ+AX4nuP9kxMpXaMthBrh6TtwOVzkna2NEZiQ4MmXfARRP7MJeNE\n",
       "vtDO3wwHlOeHh0o/FjOv83BS2piV4grAZmeACG0vmaDK9W1R8Inuh0wacd0ha/cmJ4cBxEaPCoJv\n",
       "o0R+jX+VL/9J2ireY4q/szrLPFV/Z15YDiE1I5W1bQs9qKUATN/AYT3kqFBVtU8AQc5DgEZR0NG5\n",
       "pQ92ydO207SvOkTrV7GufhHXfg88Tfuy0Fuhd+/pX25c8kugOK2u+4A2XSAYootEDd/7q0y2Lz9/\n",
       "v4k7EGA1NiEv4rQp5qgoAH5utFt93sZyg0ya8KkDMIemUqT9cQK02tQ8AvvhHqX6+KUB+Kt414W2\n",
       "bTQRGKI8a+1xZtdW2OA5vrxw5OoifwkhAaJ5uj7GouDdJwkic+uKLgu4lPdW6GQ5fK5qplsEdYnv\n",
       "yM7LRmXR/WIVfc0kp6EcAUdqySPaD/p7eUOeVpO8JdNtQFyQCibtoxXtUihe+g+MS6VmnY8tXoGA\n",
       "diE2os2D4QSrH3aOUmlMj91n5rqbgjoQca72VVzjrKSDKwqVW+m3tvkHbipuGkIWgsG5vLpFcVXY\n",
       "tzM8HnoMHuUYaoqPwJoBuURZ+ImHER8eWyot0owlQ12n2nLetJvpmhje1FGGQ1wPA6jYxSO75esc\n",
       "Rl3PvjI1VHxdHfaSCJNzuYcPsbtcGCXMyHBWO5mDqhNT4EITTAvz9TAKY54jaLep5R6HWqHaypjn\n",
       "JyySWqJs+7gyPSU9GNqWVXBDOQQ/t4JbL/MjxP3Ws55frkUtgr82QKUg6LbHsbuArdP1lS7xCC6H\n",
       "IiK9N+fdY/jzeAMmAJrv4EQHF935G6m+gCDihgW+38GudinNlJfTHhAN5FwFr6z1mJYVJne5FNk0\n",
       "xYOMUtX7pP87S9k0jzLAKszeiFoQDGid6OiI3Qrg9AEYdeFSYiv1vK3yWcH325ofQRWpRZmaiNYc\n",
       "WF6D7Ii6rs1Z+C91vdWpzQZzKVDrA0qDTkiVAXCFaAixS6opybTA3TNbPzcjnwj6xBP/10FO/VtB\n",
       "XjM1YJ3gnQdok/7HvQhYVVFEhgzXsWYJGA34Wt9NT1+2N21na59mzk+2W8r1xG/OL9OHnvrQ5Ams\n",
       "KyR7vnd1SpY+5ckda3r/WBdVq68RCZlPkc3NLrtfJR0fjowZ1PI5GVOAVXBoeBrJPdsmsZtl+M3Z\n",
       "3qticD5r6F4rs+G6qKoRktIA0ToAvWu3XMoM08R+odQ1N5e7oVEi4hIzdfA8SG7UegwPjM1kG2ok\n",
       "iFEdc1KYzKlZcfCyNfRGXA3hRrNupALhuFnpRIm8u4fpC0f02//ufrtaVhliSnWa35UnkakRIV1l\n",
       "rqxeRO8koyYQCVQW309u2ubluCjfhfgT4RprqJMQFl9rxtEsaZlUnB43tukIqo5vw6GKDvvtZT8t\n",
       "B/Q2Au3SXB7tTohYSOb9PQp8t2tNF0hrUS3yKya2+3Uf4XeDQ3p7B9qVJD3pmRqGGVbbACUyBGTw\n",
       "E4IziEsUR1/g7z28YUerUoMhs19YzHcfAfSzzs6BuiMawCGvUEofxwY6JihP+fyhPykAO1hX+VU5\n",
       "x3fhQ5nnwuygRlL2isbwqEQPjd+O/9l1Sgbqa5vQk71qtaIAdgq8dzVeO0qRHLvdZ7WhjTJOgQqN\n",
       "/o89nqEUFHWzwPCeIA3Tsw3X5pyqYsVqqse287LRp/QCa05C9AG1ixruUOm+vy97B79s6IXSUL9k\n",
       "Kz5h525YaZWFJbouaIGg5gV3ksbDUgHXParj22gov50HO2+MuScT2Fj3uoFWZTLc0taE0EleZxEk\n",
       "9rIiE40mY8ST333aG3G9Z2akXJDXnfEEo3ph7a2FzBx0jcdj9Ox1dxm3o5juoheJKLIr4x7GIRNA\n",
       "s3TBU+v1GBAclItYEM+tjpxWjldnpe/XQDME8rmxTZ2gqFLOnqu4s0xUVKkRPapzb64Kr/pwn7b8\n",
       "TDANgH+AAFbr4jtKgGhLw3hPNVj2qhlvLHIXkiRCpGSjEwQ7LeNdo7sTdYdudylcOof+FS22tlJl\n",
       "f8bqs/vhyxnARwbvLfOfs7YrX7pGs8CLWGfEI2b6BDJsRIrk8p0Mmtq8sSsXjyFkHc19WU23Bzbz\n",
       "36Isy62NDylt8K/luEkC3656nkZBjioEMeLIQGjt0Mj9Vr1NMz8/Q2xzbDsMbrCbqmsejpt63SIu\n",
       "G506cWqEe5RReB/hjL0qXCt59LtVbynNCcmlIdGrRJ3hEE1QBUZexsBZArt/HDg281Aj2sFeeIi5\n",
       "YiT7Yhyrthl1wD4M5MTdvkSkXxHgSggcpZYAwdUZS7ZBnFt0grrzEMS729vKl1OTwH9a6LF+xSr5\n",
       "JtVf/f/YAIqto3PF8nCLxBnsHTlPHKQx+dFvTnmboro9SdVL3UlFyu7rVCtt8xEX/eLVYvc9XygK\n",
       "ie69ftYAdabsazj+KSWy+EN7qCw38cooLQH4tFTItLsgnaB0ZpfCe0LpJ9y15/rwe7Ml7EZE4o/V\n",
       "V+QYJHcgQPyL55tagDfFl+hG7C5J7C0So2X+fLORxck8Ad429j9IIPHmuNh7xoqXpWt2D7RBBxeC\n",
       "UCJDJvDpJLfk4Nsfd0E83Dtt2YjZeupkRaMnLd8NOcAeuDCFXNeB1CljSDGeAaNSnRdmScK4J00v\n",
       "u2fR/BWlLEhzCO7HZJrPDQFvlzh7c97UEVPRRMCLU2sQ66Ll0L3hp+hsZ80N9rto7jmkFBQqNkA5\n",
       "6/XMya9G9Wx7m66X0pydpPBXMILskjojqwYit4CbUgGRxksTUGWo4QffBWjTEXl9LLc/88fcldpx\n",
       "cXvuT/M8xc4Y4PgDh7PcI7LMK2Nq7ouQjhX0SoRIYCBdlWuLRJVSvDiaFzIniZoEyG46dkoA60HD\n",
       "lXv2xz27+Hpnc2d/q6e/+G4nGxPna5OM+uadsxozxP8OqKyqfDbFXn2JNnX+4lGVeVlfQ3msjklB\n",
       "sXGfZ7RIEgJauD5zu8MjfoPzjPS5gZ7ws/Ap/anygVyi0dDu6nPKbc2mCiLtGvcesLAhdQsGJ4ZF\n",
       "kVUtGD9PSvSC9jpvNqZxoXisvkW8bgc1Yw+Yvhy8XDT2ezLhLqkKTyU3DpgGVn32O8gHXt7g3+Vf\n",
       "42ce9CIEklppOMEgAPveLrtHR7dJ2xDGh6mcUz6gZTs8e0pXGFU2E4gyPC2/C9XA3AXRf5n0pOcL\n",
       "Vhf4Bi/EI9DdPZY3O5+g6FjqPWy8hfBG4zDJLYqZ1vqM28DGRd8CV2PVc8yN7Uihz8K8FO8e2e+y\n",
       "cRoLtwDKIE3mlD6x1Y4xFKe9wyM+fLJLrPd9b9XmOdXw2L8GRVjRC7RuFCkkGyXUHe3YxsDhp9Hg\n",
       "BPZhlBdF/0OwaLbQENgV1QzyY6PwNZoAdgkcEVVpK7UFnFt6pxKmCxWKShxFqIXATjxA/+Bn+QRB\n",
       "9MgA2XP/8lKQNWo2fWEl/774yKzzrRUmYMsWazZiE3xURbsvmmvK08AAp+FtJfBkAbRqF9LotoHa\n",
       "zLfkBeJE18TrJWAVVGG89Ty8jmeVXU267lXo+Ub2uRFbSgPmqgJ1qDiaZbZRaSNeO9CJiMO9iVKx\n",
       "55XNkWG8hNwRuQD/PU/WorM+0zOQCezQZToCVYHxGOBU6Vl2P5XM6xb/Kqv/1VRkgocVYocScHwM\n",
       "p0qxD/tsM8kjat/bS2TWwKM/J7/QNirOqoQXS6+/448Z6n1FLYHUoeog62+BdrjPvxIddOkhNmgz\n",
       "gfjK/Li9iNhxO+fjvzAOeP3sdTS/844xO6i/jqUnFu19TwgIWL84zzGu6NIM6Pp1Yx5BR2HCXJTq\n",
       "tTQGoQMol6XOsUjzDobg/ftu3nxriR4ju7vct7QPQ1zbDVFVEICsp87gMuG5Tu91tVvKomLqEGFd\n",
       "nA57R4ORdh6z2x02zKB/qgVN+S0YBuVZVOC69EiKsbBh6bQocgoWmx+339edzBpIQmF+u/Gsoprt\n",
       "IyDz0DOU9n5DxoFGDZfvtAbO2aPRYOmSMLgiuptFL017Oq6nF1U51Tp4HcUDKLGoIhn0hBwJRV+d\n",
       "6JWL6WrolkwA5nhI1SdjqameHX+fbNDgbANEKhq7XiR8EigXHrcH0VH9WDYBf9lNt/7KZkuJlnlv\n",
       "gwW4MMMKUilv/9UBSsxJuCtt/zdqdAjLZT9TBqrgXM8kEIwN/PW1eEIXl5F84Ouv+nkeWmUZMhj5\n",
       "EiBJpbyJZBp1ib85/hNB0UMhm+S52Paaw0LoEe/zZK5WsOQYvkP+9slbkI0R5ZZA+/DEm3QK5N7/\n",
       "gyFQ5YAnMLD3llBYx0CtAQ0C0scP0K3g3r9RkIsk7XdXtD103uD83ciNaz3pPs2Gqi8I2CErVb9P\n",
       "xjxyk739hm7+zHO7ixPmhQrDZ/RQmw/exclQZemE+sBGopplkjpNV6KJXvIxKT0YeVK4e7x9pVii\n",
       "eyMtpA5tv2X++EnfRpbRv1kCPUEJ7gWtyJeADw48Iu0co5ql4HcxtsbkFIhT4hPairg9mFPvFws8\n",
       "AsA7/UIJSY7FZCfHSyM1pwawrK/3ABRAG7O8++Pbuo7QWOPIEQlLj9jhkWf6XZSlq5zdiDkCCWJt\n",
       "IV2/01N8Crq3WvCIfg4m3M4PoUiA5z/UF4Qw7Di5Xal3ZUbosz+NRuXcV8z3XSC32Jb2A6+0TZhH\n",
       "e5yzi2du+hf64vRWCRMpqRvL6j3jP6JQS8cinv7AJ/0XSEw5XsyQCYDG3H/1rDqxRLxHw7UqqQOa\n",
       "OZ40hkAqsSVkkfc5Dyj2666HxBCqbcEISWVAKJv2D0WKuI96klFX55gUU+5PH2zO7IhTVnzvmqby\n",
       "ggaJ+Is5A7JCYj/hKM9VAHc6JsWW1p5LHVTdBOEqfcsPhL95QND33+/sqDyYMhRaDuWXY3nkIm+q\n",
       "JRUle6STN1nyXnnDU8t2d+myFhriBIj/dMyNz4fM7P2w28NGRYyjllpS0LZM9ny+/2vB0o5BbYyg\n",
       "AAZXpjc8pXa7vymcGZ93poV53yvnvV7ysp36bWa3+cWFMnibPNZ+8LewrtVWCbXLIIJZ8W2KbP6U\n",
       "G6CSYH4fH8Yp4l6UebFAM9gjMHPO/jOLbXwRmCXlDSvyVJ5FNyFwz2GAcyRnW972/1sz81oYJ0L0\n",
       "vOkJvk+8p8c/C4rM8yCD0inZLWYL41RvbWmc6zf7wV82xf1Fnn+d8/2JY0NhtIr5tHH4Qx8ZRtKr\n",
       "E6WTKuhpND21270k4lUPmnlwNnKHNV4iEKIkG1tyJQytpihYNOJvnr/u6cMp0PeGqazwuNrFI7o4\n",
       "rb9vKjUV8y4Q0xAInxRSlAQwpEn66J7jLkCcnpiKvYL/waBFW6zfLQZs57Dqqwi9NBRjRYcXW1/m\n",
       "Hd4BHSnT6Mi/NiZ5VnkmJgYiX5jfxDgyIRGDKgs8D/m7EgKOqhkXakQotf985afIrudR6WB/l58u\n",
       "EAvW0TnlG/NyOnyI+88bbjak3x719uDxQY9btWOpuQgyIRF63CzwNu0CXpsSjvt8k8EwVoVMGYkR\n",
       "yMBXJbmCZqZv4kHix/2NkpbswWnUST3X8L9Zpt2vtKuLnnOLultQhVkaH0JgVrnq0e198T5Cg2sV\n",
       "zNe/G5P+iNMnEN7ILiZgF/vgGDEy/Ke6EuBMYNYE0JGIQwLp4zrr+8zGYPLBRcJk4+Nlp2u2AmNi\n",
       "aybgIaAXu0VSpW4nfQGeAo1IKV/jjvsEeoXCDany+iVqtAa1rpp4tJoyK4Z3DS/65EwDT/6RX2gJ\n",
       "SIdU1s/MNIAQxiNFQHmxBYlA+xqSkAAAAwC3FTruR7gFkka3Dd4ZjnfYUcd+i7f6XpzPn9+zk3pg\n",
       "LbiJf7OLPUXTaELU4PfrgSZ/iQEmn1rov7214JM2se8kEK1NH7vXeKrg/sCpdMqLS+6MaOQ3Hi2/\n",
       "3L5VX0i5V/pe59LJXE1x9UIC3p8Ep7bHDKbzoW8y5pHRtWBhy+BVmb3m/SxHbAly45EdPGsygGmp\n",
       "gFS4SbjtqNXZnmUm+ewQ3lmO+hk5cg2bpiSQkkgahAn4+tg7OhwXdiD26VDWDcfkHAWlMEs8hmn0\n",
       "cRa8uDT19X/VJ5wSroUny41ACZHEhQnmcENq1m/9A1fakCJru1/+B0PrryzuWWb/9uZN+mK2B+KE\n",
       "+mTj6tMkmRR1j+VBhEcGeNNK9QTrWq//pOfC4XYhr+iF9+W+/TMY/tTf7bKE6Jkvec9/774hDWY5\n",
       "563jJbmgdwEtkEVBl2wyLy68OzWmNboeN2YloS8yRozue9j35Do+CWg0I1KPSa561k5Bqxm/SJrB\n",
       "d4Iz/SRVviF+1jrGeu8r8Un4qE0lbiVQV2MDRL0YN10VsgyiBkcq0yP+O3sfqAiYIyRw6AAsaPhn\n",
       "cPBie54d7QV7AffAQYoRVcR9S+VUg2KofbDjtyx181qP9ZkEfXi6D2ye0MndLSq8l0BOU+yLH57c\n",
       "gC60y331HOEJqfhz1kUfnfTW+algQYlPYjOpDKGGVW9qdOsyS8VeBJVmAqvcQ18TrQwF/nLslgkL\n",
       "Sw+jzxnpTABmGylVEMIbdcNB7YeqT7dQMUkNH6ZlbGHVprbQLjEDAXPCQW/RrtW1xntHOoTtdMg7\n",
       "YnDLuaN1kzZpSXaTjj0hAFVWnjjstxEpE67Hzi4VfzRRnPiofrPZActXvf041mJaAZtGCpRgC8Hw\n",
       "OtjQz63YDZmJSaGt9Md1N7a3D9/mHymSs9VP2SJ8eJiqGPnzpMgPN16hiM2ULckOEjM/K4voXvRT\n",
       "KTdWHlv7WqPHPVo/NwMycLHGsdB19dEjp/mwdjlsDbSeECjeLQfhoxffowry3twrL/R8KjAVxlZ7\n",
       "5uMSpB6jx+dUE2E0nnRZAEOSN9RhC6Tp6YitQznH936WP4+wkOXwC5Jb+DADVhZHBDME97K9q0qa\n",
       "2WQYL9nvkS/LmP71WVqg7G5GD4/rqZArdXGFPPbwlkVxVXCO0iGGo8NxtuX86eONIzDrmqUOpqoF\n",
       "LHXbBrhm992fAHE9RQaQSqnNJzqwgrvQOGHFTqMmLAbYzy5qtfVvtAVPaIsSCBKwB7dT683yRkUU\n",
       "jZ1WUHQQ5lu4bAJ/xvTfxtZXqo0zFv1+nBv/w7S2PLDZPoAMgLwBEOlXHe3dZxi1xHVH6lEAatAD\n",
       "UatS6Eli6L80xl/tCNnU2nF8oo4loHOgXN1pRVKwEjo0poviDNMW74LfbNHm/Yr5vGh7EeeON+Uv\n",
       "aocexQlkioWdYtqhM9z4iCbinGHB2BRrmBNIbq40Y0F4HF/09zXG0LMvA3P/dEdqRWTJyCOy0RDU\n",
       "7UsxW3DmTYXsq6v7T5gUxyPb2/3Tp8wbRb4TzZisxnNIzPgFtLLld/wYMsak5225sy0qikmmBSmu\n",
       "M7vsGU7/Pnc/vbUig5l8anVAisGZAdLRXdcbxjVdRbZkHyTk2TQOh/WY7AsjKhnL9WzF5J7OlKg3\n",
       "e3+fXmx055m/ZUHobslp4IgCKb3EjlsHLwhMv3N2g0VPQBQ53vP5GtJnxinET0ebrRrPmSJnSgIz\n",
       "6O2P2x8wKcSjqFbwKka8mFRxoCtyEZ/86VrQLlB7OQLwhDPnesjygHjyrvHAn0QtoHHzwjKate31\n",
       "sFdaxaKJuGOOekK7ZcX73Dojlkp28fhIIuvvjDRmIPbtwcSSXDVYyevdu63DxGUXFvm6cdvbWqCX\n",
       "smflkvfc+c2d8gYl1Y7uTsBDum0xvuN4/TUIq/dzbJPxLFgkQf9f8bj6tGrM924xCbu6Amggq/gY\n",
       "QMej2xlQmU1yS6syEFDk3o4D0Fh2V6nzi/nYkroIlibaCC0TE1J2fPTb2RczFd15ZG0lWvTTDZON\n",
       "uYKJyAbjWqzQTbNp7ZVTiFCdVf0cdXx7yPKTuxzRjZGFfRLR5HYhhI4MK4+ZMKOxuqLocWXemdyy\n",
       "KXS2lq83VehgbkJBus6WVBPOjGpwKyomyg2l3TgQttUDvGYvRYbu155B6+vdVkgzr/F1dQt/pGEA\n",
       "3Z5klIQ20tOnsbNzbRhTlEisxTTBBmm1LMQJkSYfI/Mb35/YgoMmaJLuR2qfxvtIGNtRXF5x2wZ3\n",
       "jfq1K5WrAGAcMxox9oY3xABjvoRBefSNB2IIUtpEXui8iMIfLnuasBiJjdzo5A5upvra0hl++LD7\n",
       "FIvXuvpIIsRIXn2rc8HoHlS1rpyRCfVG7nV8H37djURTFg9DC1Z7IkiLH+YL0VeXrs/43BuJ48ZI\n",
       "0MAAAAwuag3s93TNQAAAAwAAAwAAAwASEQAAPcdBmiFsQS/+tSqAADqHtkabGaZMzhpcQAlZI3Z+\n",
       "/VBBnbrO23uJt6zCSk+LteJcmXpp9eU7XtEo57nw3qdn4gO6OV0JDQwyI/7zNBoEMTN/G3eRK+tI\n",
       "ML8bQEPhyavniGKj3cE9wk/wNDlVaV76RXvTswbDv5UTapet+bqe6UfTTvQOMAhAVnGVXsAkm1M1\n",
       "C+G4gNb9WdPGTck0pfmBtOS1/iF77iKlynFS4VRAxDbrTnb272w1eT+zB0Hbi6wVSRb1ShVwfw1/\n",
       "J4a8g7hlTmLiJxU4Q1h3R1RLZSf2nNTJkcEM+3K/T36V/1ygUFpsyaCR2Kn66XKziqNIFq6ZQF0y\n",
       "ruKOZIzBkCNX8Xei4jhZBNYsK+eCNYuCeW1XOfRumxpRSzWpyHSPZIN3s8j/SLExC3zbVN2N4svh\n",
       "H4N358BGbezKYBlv6A1pckOMhT0piocgxcDFFEfoaX4Zonu5NCZUn4gA3DcPcGg0v6moakWqsInP\n",
       "+ftrWacD8oUYaXKG+rJzpJoGPDcjgBHfqROsSXJY2GQbewcs87+8rw+B8FvoxNn1pAUEz/Q5S4uv\n",
       "ozeYWW1ndRhWE/5e3YpmzjWTw6t6p1o9izl/paW46f6gsnR4pOY0TwDBsOJNpfAgbySKUnc3vUqO\n",
       "NCtgq5bfg8bLHM7VoW8g//evzY6TxtftH9uIsNWZqA4DCIc37p8YqbxlJuhhxxWAmfslzLjqLHsL\n",
       "hzORRLFlGhLfgO3Iy+DI3U7J2aZ02NLlnCwHwRU27w/p1TjpmNDud65ofDWN+m6Fp8G6lczh5/73\n",
       "+rd7HsckNyIKYY+TjzKsZcvUG71jUUQcPIj2ZD742v9eZPIZNJhndDCf9liXijOfv0hN4rjX3PVh\n",
       "9GoVVUj1hj2XEcy+zZsM56oFgmorV00B1OGDY4aSVu7h7peEwnvp6nn0w5ISnSQPPuQmMjMIJYZT\n",
       "opWpU+XTr40wTYV9g2WBPcyKIdL+TCxKHzIjaTPLvGATwRmpH30QI+MfjjheZyIAySxL8bXalVpx\n",
       "Gc2UsuqCdmfTDY+fULThDBUigJeu4VkWVTwE1FMuZq4dEXvvathaFXY6JOLDt10QVGzGdnxjmcIB\n",
       "DJICpd9KEshzZO9kJA/0osPxMpNECHu6Q/AVsCUBG6vZPl7u3XLhZ3zJXciyUWxn1djdQHfzMLgo\n",
       "aZpgAjuQgFtkRomocu36KsEx24GhFYzIZ5tSMpKRzcqVKHgHd2IM1Pol2vn725lG17HgxbdvRGDt\n",
       "jZobFLT1MK8WuIaJ4rU/bq3U1aSP8nH6vFilSCHRkb1bxcvN/Vxtx3MnRpQWa1g5IJhwEZAJeG0H\n",
       "swsaXldg0vX/3qpBrug/L38/pfIUduPYnpC4/IYVeHOgt3ps3kHpfM5H7WXv8+iiSYQHaZrUjfwW\n",
       "N7R8B5qqkEAtvommIKdUO9GNsHcI7sMfr+icSfbmevSgJJvs6fF8KlUTP0XpNboa+82O6qAzLLi3\n",
       "cUpV88bpDSZuuLFya9tnNfGUL4dsFblYC925sRTCa6ZTPo1ChlAo0Ag4zTebBNYf6jnXzh4SGePQ\n",
       "ZiiVfGm26b0x8rLDZZ9R/33I/LWB+THDenxXkolLxrgqyhs+2gV2BLDIyy3YgePnTEv9mrVg3JfG\n",
       "YNZBm5GuFZbGMDQjYlZ9iI63A1F07mTDDsdLA0+k8QuSO2d85TeiU6eIDrfuZCo+G1Ug7wJEdN+V\n",
       "sTx10AMstr57+VGUkVEkj1mHAMLUTI9PPK0Bwzymk/YVEyJcn45Qtv+8VkrCxFqR9TmBwwFEBjf2\n",
       "YJeZc1oCqZqmOyIH0lxbUjNOitxV8GvsbEfYHPqZ7b2aghvTQniYyRPRrBeSy3xeRMpOiDWq0pNs\n",
       "sIn5kmb0PkcGLmoOmNmgQXSoZi9DF0JKHe03Tg/evecphP5xfp8gbVYIaEPHcqiXy0A/dnlJwtyd\n",
       "gmP+p5l8+zLu2mMUVFgkjr8OHXVXiVRlWFNi8jTK5NkgKaHEqDSokqFXu8+DvxPny0CBxm50Ucsh\n",
       "EwgFIDG/IqNC9AOUlyTQKYU4Yqq5xss4dqviouWrPFY7PGXdkSSwety2a/4Uw8CVmURDRGxxGFr1\n",
       "FlOcTwXichUkwlYW4ommxsTl/5ubzj2BoGn07ok5msaCVx+jWah7yhpPhhWA5K7tnklInorPYuHZ\n",
       "wX4A4VkrQaS2DEZ+5hrWzwbA3fnRAouFeapjm6SsgodQNrGnuvd0PqR5tCeLOCWGm9NPPa2UVGG5\n",
       "pFwHh3FKq/WfvkhyUN1BRtluWurFQTQ3XuIdKl8cmPEBiVXJcKnEnBSCCX8tR7k80bh6PTVGozWx\n",
       "M4R8KJC+9PXD/WY6RhZzdQwl6xHVU2tze9I8o/CS3E/CKE8iSLAJmGgm9/BuPvKv/3YXlVCcS7lZ\n",
       "XVEzYusdxDNTnaJVZzhhinoEf25rhsWRpnO3S61x9CoCo/14J5BxXkwzTQgEUeyp0m+85EKdSnXD\n",
       "TES0f4HNziVM7J7qpYyr6tcOzdx9BJdSesMvh5JQb+uFqr7zPGNl0fjZBfaHnNSHvoa3cZDQX823\n",
       "8qSYTTG5ruHexVtM/mUcNDFouJnok+L9PYVdl6rcxCHw/kbM0eAQKmo7+ABb+zsChLgjHOHy34jw\n",
       "PCarhk3xCwjSfWmZBtDSpd1YqPcEedcRpj4iEI6TbDDoimFdlizKdTx3kgrA6vx15DYOky8BahNw\n",
       "/xgF7BLg4wH0cPxqhD9kUkIvudnqrO4IinVTYc/a7DRR2wip/6nREYsYp4dIaNa3gEkTm30v7x5G\n",
       "OX6Me8b9HFrwVuYf3r9SUKv7zpdkGwqiOEydpj9zj8h8+22h51Z+9kepclqwJGfHEaARg8itTaTM\n",
       "qVDFHUILBO95s7SBfic69Ad75WA5IGNo8Pm0G+YAAoRVUUPrFERC5FqDSyWX64N0LrDVXDsfpF7l\n",
       "MmwMGSonNvHT5cs5wL1aFaanx8v53dWJnSmKy/EiK+kOQ7J/p5PWQyuwqZIPgl41uFD91kKpfISz\n",
       "/04ZZRk2o2stMWqhLQ/Y4EXhs1Gh3qzSTseEv6Vx0cYpC3spsFwg4vOgHdauNrad0ZOh/jaEqhMh\n",
       "nxliMItvOj5ThTJY/KlVNZWL8iIz4wZpshJwgy5zH5Q/BGjN82YO+UBXhekqN6wozb93n9KRHvmu\n",
       "rGWk6uk/gzAxYErGcqWzFpOh5KfIZdJ3SHK2JtOU4CTcwnNU/eloJK3goh1iGgJi605ZSmaL+p5/\n",
       "s3G9XqWL2Sl3DYvCgbXtrRzEPPc8aC36FClX0J+fU3cI8h1jma6cw85pmrc4E4BKeb60MbE6CR0M\n",
       "oop4rWT8CRLAT7kjzE1ulfhXuIq5HdDLjvnO19rf/Anyz0ehE9k2PErk0BTS/8U+v0e0DLDM+KL/\n",
       "kV3s0AyBDY/IH4URMtIbDhhW5U9U1tHm0QlTO2lUjElEYkMSdmvKe3rmBfQBwcG0vHfgcMsX0r3M\n",
       "blYxPcddWFSfAM+nxC7VoEkKTNu46rgtXb46DT/wu2at4ky9X41mNym7AD2ej2hDL+7vQifITRwI\n",
       "2L9QUl5SraD+rPX9VTaUTUcw6eyWWPs0ByiBiHaVt6/SVmHUB484jzaw6g5Y0DT4ZbFDHljSzHLR\n",
       "4LG6LQpbzDqZP6DaPK17lnYe9sGgcz4JhsxS8ly3LOtb+tT+BRqFy99/Zv7EfMK/KbsasFisObV0\n",
       "mpgyZzF1hDhbngURrLL+aH6mUCwZuBIfK2xkPV2GG8uFzGD+/KRRT1KxSiBLmnn4EhOpQE6wNAv9\n",
       "ohNIGd+uFfMDcOqnk2gsUSJy7wmWTW3LM+6I0Zh94hf7qUTo6NyVZLN/bpvsyY1/s5xkRuTZFJsE\n",
       "kvgxWHvfPXXJOfWRKTwgrypED0TCfh4xstTbBjy9Bw9i4jKO3iIlAI/nOo60OSRxEed2C+pLB9lN\n",
       "7DZj9hBuQqx5gWI8unUsYqaZGCRuIjAn1CXFdLCG6Ep5iPbQoziAnVNWWRPEc2iMzSRrVQrQ/VJp\n",
       "vJD5kNO+y15JYiT3NVIUBr9Qo3LU6lBaIgW83u0h6PyL8zAuPmhNHi/s4boEgYPtYEwRRdIreqlG\n",
       "ZpFcrWapyAMIKZcgkLMYx86aZuSmWDJ2yaYgH4PtlNUAl8VpRJ5N9p4CjrxZ8DNcPQZsszx9nScr\n",
       "1igAgtMMjd8rfCyTVxSiec3yG4bf9P9qaKPPs2gjfwg1HvnHV2Gs/MH+jpSTHOfJsD7cIwYmnrr9\n",
       "2y1cueDIEBOkXw0w9yDlGjt0V0vC/FlNP8m+8r0DFd3FXusPLeVdt6fiDoXYEcnceT55in3RVvbv\n",
       "z1bLyiRvzzrhzpUBt0FgMy6i1hOA1rojMQHfrAFcSnlGD/zWvjhlrTQKl5r9MbcYMjC/afSdf39e\n",
       "piZQLrKs2qtBQkKP6tg2Zt7DGoQwom6AmXm6B13gKwWKOXMOoDuMkK6EyX5yi59dhLvig5NiraH1\n",
       "6rP83E+PMO0UbY9MdU3vYASaI0Mm73W9sjKeUyxOdx2aGsR7BvOUAZKdfSkOy/t+5fvqxFGZE5+/\n",
       "8tc9hD60kcN3LlRAvjz/jA596rjBzr/W1dpdjKm0QINJeqEn7FvRWODYYSLbTMwTTZfRG4zGpFLp\n",
       "oyNGnhf7JJEEn9PjT3m2Bnpc+8Vq3Y/TCGaEBdlcLKbxkomatvhgxLBfE6ymKD50IG+EveWzDdMP\n",
       "w3231/t9W1+jLN5KsIUL2FGWiDw2ssWn8gKNTjYpCS9q7Cvv3chkViR3iBI06TKvRLvZUBVWilI5\n",
       "E7A/RbIw9ZrVwrjbEJ4U3iqPwZz/u38EukAWATA1iZB1LZEO2UhInkpdLEIuQF0IvyEltukzsNw1\n",
       "VIKivM6YM3KUfAr/2eB0kty2VPE/pCIHsiNtiugG2C7Bhj67Ex9vdcCGKeOR1F6NPW8nMLYXQRAX\n",
       "+JrHGUdfcjtQbJ8a8JTuXvM05y13Og+fUK+Mq3yAcMd602osrqBIPYQA/v8A49Fso/Pzo58DxyRB\n",
       "dARi/5VkKuqMabZVIg2au5Uu1FDJuV9ube2O37V8AFi+fswTePH/5gejrvCyQfiyU111pkVYFQFZ\n",
       "HjWhSMcKSbfill/oxuu0FACykdPdAI9s7sEEoPEdpoqc7hPKNr5z+D98T91yTG5xu3XdzFPyWqrQ\n",
       "BlSF3eKSagpBGOTCpH/LPeJqfaVYr/bFqTPMsUcsMEsNYlXaBq8OUv4u4eK4fhl5chmFFK0T+/+Y\n",
       "BCTUhb/gFPhryVqqI9JsWtB40qIauOjhc/9/nwxDecujrZKS97X1dR58TlJjxXJGo3ZMzZcEkgQn\n",
       "wwvNZgkQqacdnAoDyfo2G2634jwXRKFNaG/K06fwsFOV6o7dvssfwyiwGCP54xQ1fS2b2JP7c1Ha\n",
       "w5q1qgW0n/+GgOQcKAj7yY8L4yjyMKdKrbCX6R7E0CCrUFV+rbXPEe/YXcHqJ4ezeBk0GiV5Yuzn\n",
       "MmfhSBb5ZmvsNDGu9ed5EWmKNiRGJ7JN+d/XM5uPzl9rpM+tKIid/AVn2CxS6Ah2qmjiHkyUun2j\n",
       "6dRUVMmEFtDyeQjthUv9eGrYbvD8MfhC0j+Wx2AdzU0d2mccN1Gg6nZ0mw5zU9/HoO2WUzKHbU23\n",
       "WhfF48e4fyjrukf+bJJ8qwNbTAH7YOWdx/V3OZBRTIDOWBLtdokt08u3NHRiE5Wp/VPW1fKuOuCK\n",
       "JHQusKfjbKhkjzlhDdPH67N0DnG2PlAPN899C2iTi9o3Pv29Ueg0wupcqZv8jq4uMU6L2DiuIzbG\n",
       "0l4WoIBLsw8lVyQna6qaeNkTBv8+8/1ogcp7X0Maun1uwQ8uTcXjwTEkge05NfuxIe+zZ42b9BNv\n",
       "Af3nJiPlu2ur3WlyE1sdUVkpAdTuibkvXc5i4kBRKQMSW/9RYxpwmoSNNwyA2sjABy0QfxTdfGtJ\n",
       "apE8YhN0mAOH6Ais95Hb+K4dxOlMFLjisJM5H/x4d2IiCaCS6J9H0Bb8AwPM6PIvg33MTWe5jt+H\n",
       "ndyicWQW0ac/WeobR7KnzJITB8x971D9BFknpBtgX8Me0puYj5QYnKTZRfZJkhyINhPeXngBbaUt\n",
       "Bmc3do8Lsg8SCk6cDCrxCHqGz+FhkXFYTxU/onRJaAg9oEINdBapONPVhOU2RCUl3UhPofu628ZV\n",
       "sbxhqy5/AwafWQVRPF6lV1kGWSB4sK9pCqKtQz/5lpUQfJdxxh2g1vMzw1usMfrG2wQV1Toxp30V\n",
       "qDCstFdT+0Xigxkiv8pcWlXkIj8WIAowiT+KJd7xDngR1S6L72Y9CG5Zlv0PbJYESQ2tbkkcNS35\n",
       "T/DMeXwEKGJpUdktr7FFmI27X6XYEFto5LvrNC8vcd41oB9HyomcBTOjTB0mWpAjl4bFYzFQbaZw\n",
       "U9LammAErGlrfsOM2IS+gxjZVd2PPpFVX7LkqMQTZ1aYF1JlQgkk7kgKnQkBSRNsrwf/62Wp7Ds/\n",
       "6dAoPzOZe0wjU1hJu+dx2daVNOHuLL4ayyjAUfXlELO4DstTxtKXy2i2UZc5tMwQcpkEsfsZlk/x\n",
       "5EHU9KJcVkOOzqtCkHH1IOOV/crulelfnoNB1/ykwDkjTCa6GBQk01FmMhHJYYEIgLhf7ZWR3RH5\n",
       "dKjnJgpIqcHeoQfJbmHJaVVjJUGtRckCQzPkNhpDtT2g3O1ZLruyqjxcLzKexznEd9OG0ZZZTu9g\n",
       "n3498ow5U5/XWoyf9OMFacBUwBswxeZSXd91v8/heBbkABwR1DFFNazQeE8i+TmFyYriuuamnQqP\n",
       "2LQmOawphs4Xds75xGJd2QrG55QkYNfRi7oJcqv0y11sxoJmd239RX5lsNbtprJfnCjn3rduXtvw\n",
       "JJPJaqnMeXCbjyY6F+tWpffnvk58yYVkczUUCzusdrt9xZ0y5C60XTKTP26t3WvIEuC3MTiojyGc\n",
       "kocPntfA2m1v/y/SPu71+r7B0s5T+Lq1ZJpxoEHzfDomgg8coJ9qxipNC82xzbtJK29KyFhJWJe3\n",
       "OCfNryvD17SJ/LqhjAQ6AgCaWL95cBXaAfYvStXvxWTI27EvBabiM/NB4Q+SCY31+18cu+G+qw17\n",
       "VurqLDcOeytdlrTw7iFIqkD4E34TkYttr3kKj5ZIAXFkfrAUiEFBT5P76Pr1747aPfJqG6nQss8L\n",
       "8RWr6KIqX3hitMoRLZOVbB6JZrvbYwTPKhvGmJZRdQiz3jnt+dxjU8Ro3Ep8ZpEMUf5/gVeTAeqO\n",
       "JP5rHQSH9FDyiL8jFehsEZSgmTOJczXzS/4vArJzCmRzJVVOHrYxWeYQAWPtoebAUZVX2xwBCWcC\n",
       "gWWwUTinwfY8kIhgQ/sazmBWoYYTubi1RYf53IxF7Isk5f5y36IwCMIPOLOWLxAaYA3AS2OkuaJb\n",
       "qj9I3eR7tCA/qOoyV2EWUt3yHEKHWJBV3yVmaR/NLjKruMsFFoKfK9+qrXBXI6+7nr/vG1kq5NvY\n",
       "fkFds1F8krOy5sIECDw/+pAVHBFoi8rWierZOFXhsrY/duCdIOmj3fNQJlPE3vyNyy6zE0UYvX67\n",
       "dq15YRhtdoqGDBprPVZ9GnL2pimwgF3/UIzBjikr8NNFp8Ury4fedHm99+G4U5EtmIU+aGLk4o8w\n",
       "Y2F0Hv7XfOeQxSmU6hSYVaw0fmNIEKZwyvpbklkGKTM+BfUrwY8CAltE1Qec0neCKep4/+27TK51\n",
       "kShhnJ3qfWIhbD/A1sqETBVvbMkSKGtNvec3nH091uZpAU97hm/EIKpJ/Bfm1wrPCPqdayuw8dJp\n",
       "WQas2Y62OpwLzJ++rgmu9dGiI6XWRgqw428tFCSA8isqNxUBp3fw8gjqWIc/2tbGBznUROiIbuvC\n",
       "e98RDL07VoQhfwTHtql3SaORToF6rdPdpCZYL7D4FthFfr318yCn8C3RofUXBhcRvXJLXAh+H66B\n",
       "VpgFdtpsEPexGQlR9V+WMChuhiLs9yhH2p54zg2QnOSrO65vDKF6a+ssW/6sU/1ZiLD3bik79lk8\n",
       "R5m2+FpVJIFMUPAIWj/fSC2bZ033ua8Ra25Whjr7vzGhlQmfGv7ZZMx0VM7yzUAI6tgeTCaICrN7\n",
       "d2EvXKLjNxZSghu1lca5MerzVo1jSC0CbrPnRa96vOnMyWkzvVL1W24BUagYfQuqj+S6GTRjgywd\n",
       "SZDUNf+wSnfU3v0ZAqPJ2cPkEl+q0SUd/gmAEv6QmRFWgnR3KBsAz8Dqg05YSNc34Euwvo5fhbea\n",
       "Qy8DokS3WC8K5V1sAno0KztHNS81J1wqrH8rrArmp+PT4bSwfl/00t0zdQbgbQHTm8qeqgsa/T9v\n",
       "Kv85EdIjGkXoRkXku0L/2ax61JWN9WgC4pEyH1V8pxNAuaEnFz8uppPb20321sLGMIYb3LwzyB4M\n",
       "5ZIzz5qx4x+FqWT7eykRgbxon354VAN+CZ8iI8JNEaY6f/2bSANcluVlYaa3S+DwjDpo8cVvwYqL\n",
       "uVZBBRKbnxmP/djB/ooYUpHd4K5rPTWjy0IJc4tSBfFRTCSLiFTfY670eA39gXj8pYHh73aQPuDd\n",
       "8nM7PcZ0qlMw7sfvpEW69QKN3mH+l23bHkYCc9OQ/L5sIU64/41BkioOOOK/dET4DjL6TkHjqkwS\n",
       "zEBkDJPbOBfnGi5M+TGjOE7XTifOePuBwzDcgMIlsuZTDmDlW7ByqF3KKsHkcjRORkXSWcgS6c3J\n",
       "3Eke8Y5wf8GG8xvIolgabu+9d8wz7XPZSWIN8iSfQIhJLokkXkmgiogaqX3rnvoC2PFfbBX3v0Eb\n",
       "RBCyV/SU7uLbB8gvxuL9J0c8/SUQxCsY/grP9ENsdeaI60Ux9EeLGNHd17osIbEs4ft+sagj2LWC\n",
       "83bTY6VTWdfmd+CzUr81C18cV0HtSDwfU9PA1edr5kaJQKbz1mRayGRIKJmKVJPE3WOqKrRwEEnI\n",
       "CRsHFglgR6psF2JRuLkF3z7HXx6Ojc7M2+qYoueGSpLy7CkWfBirRYVKdXmNssQrqfTMv9/jq8PJ\n",
       "spBU0KD+RKydw5WoGLjTeROs2+T87DzAch53TSw/4m/4HC/KxoRaqKYTH0giStmmge4D+WguLhQ/\n",
       "8AgoUBhwwrMgL9Pia8RIWfhAYsPCQITYwqYTFbUlCupnuaTbkTSUrpEuW5xGJO649dwnDse5qW1m\n",
       "rpFBtf46pMNA+vpisYsMDNBOmz0Rz7ciTM3fYMYUIwfhR09oXXjZn+1yLQwvgTXK0NDbhcIZvRon\n",
       "07ImyJDBwd5oXNvSXzTnK3dXoepCgPwH1dZPJ/sMFYNIwBGLz38TbZZyrSSle010v5ieWRhDw4vH\n",
       "YgZqioVHAV0LV+u/GUGn8cjDkO/OAdqS1csJnkshEOTnlaU0sM29c38qE3uKFXVwEWKt5NqSH09V\n",
       "VfPXKMyCKpScukb/9Rr6SHKGc7Uc6B3fVcO7OqhRb8GINQ1jcq3F2s5+3CAZeMAE89CF02cXJqxu\n",
       "r1DCY24ec+mpTFQbyGwzgdbV+1kOBBP38aS/+3cpBQpZ8u3PpHWdXckHYDkszvHKjeJw8Fm11IR4\n",
       "7HPvGC3PN/R+KvuttcfCBwykq2ony4EgUbhnp8AbgZWyR7pQEWkj7opQ6u5V0vU+wCQS0tvM/59n\n",
       "1ObIFQAx9bwRXdkaL+JsLabGCfybamGXFQ//xm0psUmBonll3slqXgYiOKotAKfeqJJVgYFqRPnL\n",
       "2CcFIiw30mbvJJRaK6JtySgLLfrijiNXkGFnNvjQeYn5OC5OmwL/F3G7vkz+NguOC7s2GHDo6Z82\n",
       "Xqd408J+8cDccOC61Mt0GW1BDQemp/Pk+q21BAwToF2uILW4DaUuKXEInFoO2ufADR3f9Zqgt0nN\n",
       "+Pe59Hge0+nVaF+NXw8RrJUH0dz7aEN4xb0S9XILJ0oSLTuJb4GGYduHyQf/ZnlBF2JZLfEK20VO\n",
       "qiFJTM3APZInlPK352kjBpXXygG3VPqkz9UxjlSEOOI5ZakmEn0Rl3sGhyGECFBYoWYmCDNsR5oN\n",
       "BCSmDOTWVtgKI5FyJw9/PxgXNgKJmlkBvXF4s6G0H921q/GgF4uDVN22pbCCUkGP6OIPAFXJvBmo\n",
       "SPfcktB+/5VgKNIP072zzToFwIRbvzvruL5s7hnPAzDifYj2crY9+9oxhBccH8uegQ1fLbBXnFYW\n",
       "V4Xb/G9Vb6ggOQ8frDdTCEGyUUk3y0oU6G0FKGyIYGn9MKSxi71ecSXUresuYzN5Uq6H2lbFF3Is\n",
       "Bk4bd6nROEoJQYHJVdPZ6XXWZnlSiojIV/13fIZmxZXMDqW1eZKZ/KwS80HGxxrX1yQ7J0rGjGXK\n",
       "RH2+HSv5mKq+ME3ZDm5HQNPc2C9Y2VBCoxCA8x8K1QHuEFHXSMGmtp9n4BuvQmrESEgzTQQNUN4f\n",
       "8Eb/IhQIvuXiWoLiobWm7oO5P3wuCtorPm2ahhBkA2R2es7mEl6sdDdAudZOKYxq6cQcEaetdatZ\n",
       "3OtG6iKyWOpkQo+GbmV+n6SHuBkzeqAWIofbvWevVBh8zKkbw9cRmTXNl/J/phR1X5QdE9xpUAxm\n",
       "brOD+ji6SYDoqxA7ac5/ZZguzK0/VdN7LMdbaV9/yph4AoxBjOHS8jeuUxVHeciXhx2piWlRQEdS\n",
       "bIIqn6BEXiDRpFZkdxlzc0XvDzx+ZKvZnVlIUUfBPlkv5mOQ18M4qjrkWZ4m4lV+IOK5Es6qYNNq\n",
       "2xUCCYo+ZOUAeHjkB+zZTsat7xN1I66P4rMulAEAlmEtUeSZdYpVnV7j5wIIBGZGiXDwxWvsWIZE\n",
       "dNDlanjAPnwoaG4bvY9hL/MvMGrcX2m9P8AhfflamIQpGQCb2EhLWPiJCsdmrKL+ntkg+ueTNHic\n",
       "URmhUrUMgPxlf5gaaoX6IU5nHpeAHOQ8MeRVboI74S14Z761R6g/5MdmFQrQroLChDwMoyhrNP8q\n",
       "JDhmgCPaw8zRCmwBdDCtgmWGqRPfZBE472v0n9UFzuzPvjnUxJG6CeAJiWcx370NamJaW74pX/hk\n",
       "8JeVhbMn9GOJQE8w730Z9MrfVSLt6Yraayw0o81Lo+IrYYCliEQnmn2AcRXV4rGCFfWlQSmrj+ov\n",
       "xu51Su3ohrwlYPnv63FDQrmwAo002NskNi6c6PLwb1vwtOB23muPKY4tPKKMvIYuzCsB93VhjsAE\n",
       "VyKS3a6/v90hbOta7aMteBChgcsUNdpTKOpzb9xebZHyJz/eJUmOltQuwTp88UoPuJafZxNgIlPo\n",
       "TWVIJHtcTm/P0Spt3GvheHmUytKfJ8bUFECN2ZEWvsNButCkapinHS3417nXqcuxXta/vfuQ4fEG\n",
       "0SQQhqr6iPS63gWJAAvGo4lsQnnEwMXIo9YC1mfjMQ4l4PBh9Xo13Q0q3iMZqPpuStSnGmaI6c/2\n",
       "kPegxdVMuAjxgq1+dNVoRvys5fr878R39Ac5xXE5b0K4gwAjsy73zm0GXw9MWTwrCU1Hl47mT99i\n",
       "4rNoL3I/ceKIOjKVqH4seeVjOOjZAd0Ohi+/IkLR1I7J+7zBju0EdKtMannzheFULzsNtNCoXoLv\n",
       "pHPXQGtjhm7QSkKZGWyYIhSxmOy1+JdvlMpgaoOgm1bOmwGNOQCF8B+7Z6afESYwN3U+bmV7Kxya\n",
       "KMmFuOLkGXrgz5ZwrCmfstOb5CIxtxRiyLef0dzF6dhXT6PkjQ1ir3U8Tklr+JJZ3Cd76DJ1FmJt\n",
       "wVTakyoQdgMeZCx0JwsSGC40VtWkun31v+3MITpqXMWtZO0sAMy3MpnF/LCddQKncjuuw55mFSxK\n",
       "3l/GmVBq1QSSehq+SMpFUpd9vSw7Fxv2+N4O7sut/tM8I853UnrtBTZRvOIykVM4d1wDSmWTY617\n",
       "Hmvw3yPlU/JdgGzb1hpWdtfHus9pHUExbq3QWtQ2eJ6CBo9+uiNRfLaQ8J4ETQDFEfcmxYFLByfs\n",
       "DA9doRywFWcL0OyUD8sQNxuQnXsjEdZ5/fWSdiXM20NOXXU4IKN4WXSsbtk2fY2tOQiBXN+b7PIo\n",
       "TaEDt9LvDc28fHCoCMk2HvEHhLXIUx87vEKJw177VnKLRdvgGISx4WCRDS3S/OQt0aek4pPZncTW\n",
       "Q+n5vK4+8i+UVcqmHSIL5nbj7AOKDpeUHMPsCWdrGkwKgS37G9UKhF8ErrBzNJq5E0GXKFPy3F89\n",
       "erer4JySCBzGuKD2OBK3bYlATkw7d3KELecJGuEF+aneOy8gVlLmN9DwlF0e/oGQi4jY/MGczznf\n",
       "URKVaIlxhjCO/O1H6MvvJU1Azbl+S91d/tJbRI+7DduvwbRwn57aM5IzBrqPdGyIID0pLbAzHdH7\n",
       "BAU7S42UlPJmAJtS1HC1U8ZJdC7nb7/fhjTH/E42IbclE8gZZ3L4172QTMKsF0HGAP5y7ukEWcvZ\n",
       "F4N1AFyTMhRAcHn77VZ4+5j5yRh/JQzHiPwvRtcEcdeN7p4/KofiIFJRhbpKJwqBL88vrkJUJ1mI\n",
       "lW390J18NGc2n0xDYfLQBDVuoF+X0fKuDHLKgAgqVyYd8UwR/DhXQPmQLfvfcRW+0uKcGHHmiGG3\n",
       "XItQ/U+4InBJZrf6QS5jr+cSzItklbsjZVhNnwbCucRFQc2LHHzRh2ikFGpsFWDdpHHV4nDPJBDs\n",
       "H01iXAnkksMrQoeD+fcavMmK3VOLKrfd9+w6GlgERVNQ7AZNsRg/D8uMvjb1+Gj9j1E62kqbL4TD\n",
       "94CTNHoO1sXeXHE689OXD5gZ0DYESDna8amghay2WeoAc3FVTXUUp6fjCM0oQE0zNQ6szU1+SGGi\n",
       "18MfkqWBCJsz62SdCWU12k11pHR2cO43oopnfepW59gLmm/7CNc97ZBLClFtOGVE+Vx0zh26GDmZ\n",
       "2iTqvNwwkqnhIcn1CtLuFfzEcfgi1ApzkcHt0fcWRzBa4npG4VQa5igvHeiIjJgB9yw7Ieb/3/8a\n",
       "p65+jp8NbYbMCiB8vzvzGJjmUNfaSxis+zja10cmTyg/vrUBC+ggpxIAAuiaYyMiIBJJiPkx9YPH\n",
       "Ob/qz2qt6ep7lUeyOOetCM8lDw+eNV/WBwxhSWWGjz/J5er0ir2IbfLMSDP2DUL73b/84ioas9Xm\n",
       "bbMUMOJpcCLCNgVxRea0XUQgICqOIw3SDMCvgh9zl4iccUWXQCBYr6EaVu2EiemKDIEQGGeqS8OZ\n",
       "Man6Wjank+sLlukUqQvOxsP8blxkpOBI7pi8oxnzw61LGCde3aBU/QSTlsPtfAiw2UUJzNC2YZk9\n",
       "cYwHMUWSaC/GULW3UHsOuztWQWICWmAPKtLHkXc6gJkSA1i9qyY4ANQ4CakFcHOnynMjwbGHlZpi\n",
       "Zi1XJerx1u6n7NHf30D+kLhogAW6FFYfu/8kAv6PBwzQeb2TBLwGjUYJYtSHg290cOl77DvoOKYP\n",
       "LWwV32bC5+LHzgenyOp7S7cmOR4uN+M1Xa09m62+pVxTv5ZwTKGiGZCQk4Gi6TBSWGOMEVSZsGK4\n",
       "6QlN0wf9kHRU+afoP2Leuy0CN57fMWpSFNDT10MytgDODCfS6GlXqnShhOmo/8xcJ67Djw+xs0Z+\n",
       "zijNTt+SjxU6zFMDkHqn4hgPSe4kTR617h4scjL/eztlypsg3ayW+7cBvgz0Ac+hXuTQehX35Ncd\n",
       "ekkfqbeP/IEQTigBNYwc+YYVVBQvOfYcs8U8ilSbMuBYe7Xh/V9QcEz0TAqwAFMaI7gR9xSWiQz/\n",
       "8pbHg9DVLfR4O1kqYpZ7mU9ctkfeOTw04p9LKjlS624k7XjRM5brsmLIZslT1OEcuKb5Xg+wSZRZ\n",
       "f4ZjPhJyHsOXk+jkSJsavYQqPcF0JYYVi12bEev0XebDWHbuvHUy5+eZVrCyFHroWFIMRo8FCfjQ\n",
       "kha7SSzr/CUZHTZ+oCIhHG0G3VAy7BCM+Fcj5s9ykO9YRMyztYRXvT4g+wNpVe+LRRwV3lwPm969\n",
       "06rtYWIJX1cuCJBrfx3aeFlt2cpu6bQNaK4tuRZn/AAAffpXAcudGzHkCQ2hAzCC5hcIc2CYY5RM\n",
       "AoG8a8pQxTmJ7B/IWTF54SFxhFXYlPXdQ3+T/R8OWRVB7OyKx2J3apDyMj7k243DLEv7b4kzpU+D\n",
       "zIL9BTZsUrYPw9TRfH1phDk9kvKmdhg2fLP2b27X92cmO9y7sTy/K0Hh/rrJ7zzgjikOIiU6sYSo\n",
       "wvZja3MRq266HOWqLUQXIei9cH+Ct/JGTrFHmeYBuJn3Bd6LRcSrLekrVfax+nhnFSK5+TuowyV5\n",
       "OW2LRA7MP7h27z3Ey16A651AgsEg967CveWwLMMYHxzgG3nL3jND13TgGz7o19CM6dXl/p53H5Eq\n",
       "UezYeZZCrtwkSKlIBr/r4A0Ja2KhTmJbkGy2FtQSlKaKfxfd7zTwwASP6zN2bGfFSlqzD6xMnaSY\n",
       "iCOjBZDQny40OnVFMz6fE/3gy2effCS49pbGDuhAnwZRAm5L8q1i2S+tsICXDksQMOZCsipTEWCQ\n",
       "Df8pZ/aoeJ5CaZKArC1p4pka+jKUHaXjmdol8UBEA4pTJrleV2+p9rWzndr1NpdoxNvi6Qbod0OL\n",
       "l5p0W4PhyVW8FA+RBS/P/zOwCUq/MZpxLvWa1AOJh0nv1NI+h8BE5622JkHyVulsT+o8GNSTlNr6\n",
       "cMczMbneDT6gIRbnEOpYpSLWLwIYau5khu80B4GqVog2GQGRJGHm7024dbQz0yUwVf9Pn+mROqbN\n",
       "dKyoYsGtd1FQ5oHhluPtIEsoszKoZosFH9qVrzahmzG/v4rorNSOBazYhxdMZqvHz2KSLJwUWWKB\n",
       "phDw40e/KQOZupm+9FHgwkp6w2Z2oxgjCkow7lOBn9ACm24MsynERzFoLQjFkxXtAn62LLHhAFLV\n",
       "CEzo6YkM1XsTpZQp6NMW/zB5tYbeHrmfbCdGgIgxq12PXJdvo0vOZSbXXZxIQ8Gnan/PEzKCQ8ZL\n",
       "h7KAIkUpcgcxwsOn7Xa9oTHCAdeW1DcHkHk/ofjY/solQEG8kxO5ii5ejYXUwK6VUdNxIEtX3nOY\n",
       "6n2XkFdDi5udtJgSArz3E78rCB/zpX+E+Ms6cBp3S7CUXn/qbSra636EeQu6VL8ReSyH5ODKva3m\n",
       "fBlMn5lYeyYF8hEulwd3pMZRLnxX47PA1KNQTSyAxzd/nFD/olLMGUPr9CoHad1c7V88sCMtXSjx\n",
       "e70WNJXWoW/gdWpwgUbYdgk/rKYdvTlNmX9yX8izhzfcSFLQdrjv2bSLobnXyQz8xqFLw67glTWx\n",
       "KaDYSwKKcwAz6ZHptxnI8dc/Erfy0o/eQ6kfGr9/wAaTt0u3FMPBz3k1nyvTO+VIQWbE7vzZISey\n",
       "QnsL9LG63JiiMmQTuKYpJPhviw+kGvV4Ar8efgXFAuElOErXXg2pjNSo/xZWrySQv50HlAlm1SQW\n",
       "HT7TnW4scKWd0Uum6Bl3qDiP0slUgB7yJst66sdpEpWdguJZPO2Hdl8mKBF2+eJciGBquJtWLi1A\n",
       "LrBfZz142KarB1H3IVlwDWyi8+D3nyPJvJM5BewxZS/88/CkHkKA3Hn/F2a3uRO3UU5TA0fP3j3E\n",
       "lhUoAryknlaysx7a/NVNBOZKs0HIwkxOPzxiZ4TN+Fh6RxQidDXYHIGjZT9SbNT+yBbcJgyWor79\n",
       "v2LGPCb/ne9EbT5Zy7GziNYPJAHemvu6cf+J7U/NZeWPEK9hpoI9//IH3EttKF8HQZQd0yX+9blk\n",
       "zgZVqJJ3p3afVQZxILILceEpoh16VGcnjRSptzcKDV8ILmnR4UZut3/9jhVsBXAas3PM4exz+jWd\n",
       "DxUHqRUlkrQzIXtjfH7Am16arDnbzWIHOOzbJZv9NmmrU2pSQijP0VZKH/dzoywksTL0YbALP7f9\n",
       "YlYbkHkgFlr095Zt6yrLOCQ0aYjZXhmtW42pr5izwC236PgJAJxzEHc60MtpOQGSYJoeV9QOIsAT\n",
       "yajnqOUOYkKgt8CkEVe2zX9aoUToFv89WCJTyu6BIYxCI4S1aN66niecbIwwRd8Covcr4r4R8qAz\n",
       "Y4zOwoxqRNeoS3vTYwie2BQDxYneR/GP+DYTM4pa0Ybydo3ZgXosuYqJa5KTUnNQlOFeLlcEBpZR\n",
       "5g9duoyi1RiDHCi0NsmrkspvLnRCa21OuErF+ndqv6bF1Kgq74ujH9hlA+Zg4KFwCc/LaLGwCYLj\n",
       "jqEc58nefkMItB3V+Qf/X/M+BlrMLNAZLV2+X5TkJ2hcv9xl0HC713fInQeBA4X2MHZIWIkwDgmu\n",
       "AINmun6WQjNxSe364ZBKYxhUYf4FOK1wrhamPpHd8jsv2YK0MP5IdpOeUUP5cIbfrYAvdqhVadbb\n",
       "PWXpp317UQUKDtzFwOREnayV26Rk7A5AEDL1PAnyHY0bHbCxb/Bouk1xReALy/b3OPe+C4kyHMNs\n",
       "FWFlO1b++5hM1yiseU3yfSDB/qNLKeELqOkEEYYZ/USgX1j9YIzTOgDwA7Tu7ebqztKXd4YiuM/I\n",
       "LvW4rPaWxdrLg1BLIJGt7W2lKR5HMHWTPwMANljbRfkGSCLpNzoOojvNOudpVicVxiMuCgFrZMgA\n",
       "HrL7K0isq4dzV13yDCCLJn1ZHNk77iKxukMbk2PoJePFqssOwuPr0gwkFCebuEwcz+J+dGtzthaY\n",
       "n2iUCZP+SpfovkFt09bDFcP/UZXO+eaDcbz+WmLjzPpjfpyyOTtPyO7h+pyVwuqKu4uvRZvxdZwT\n",
       "uI7wC34qvWnChParGmLTeNMIeGfazIZiyJK9fb2YNua6tl5FXZI4IpECFX4qB1uRRPMkSOZjQh7j\n",
       "Ont0r0OAxhmY37pNbfhhira/yx0S2PjUqYttTw6CExwPhnbbUZHY/3SNznr4b0xfL2+DtBLwLlf2\n",
       "Ph/crDDhJYOZSm3lDbK7xAgmJc5ddmKvMFxcLB3OaRDxPMYsLRBkl+n5YplXghp67UNH34eMRBby\n",
       "SYVAcwgLpUHXwyTRmIn4Kax0A0TBdh64iCmCtWDQH96IrLV5BWIn21/0okZAXI/VncY+wjyagbdz\n",
       "tHbPOrsZOjVcg/7XewPd9LsD266zBEPhlftDnr5+4y+ElMZ4PGyMxt7l0m8Y99B1AaheowvJncV3\n",
       "kE+3O0Osz5he+X/wk1vfvdslLAYkV10crWoHdcS8xIHZzGfRM+QJu945rYfG2Yymh6bOYT2PRPrv\n",
       "UK60q0ToI+ihWaykCwKW6b5C/XO3ASmBxK9x7HxGapqBhBLOSToJwU1nPyRmHbM4SaxsO8k/tW5/\n",
       "FjkqZOPYKdsF/Gw8TRuJYkk7xDEBtPqtSLNS40TcKjp1mQxYolViZbuSCwABtZ++o0lrA5vBb9RD\n",
       "9ZcIQhRzblYpbtFTfIM/KJzzFFgztunxc6kmdAOCcxnPABm4PwQ88Fzyzn9m8iIvYr9uuqHLjMOk\n",
       "Mkbk9QUF8S4mycn9mamDJBcf0R5Uy117He0qYkWUiksy9FhuS8FxoF3xh1xbIE7NJjZngy/tXIWl\n",
       "SLC7bqNzVPmtpi73jcgoT2//1XSQIFfYDv10Ax+GcKYF4JswjMjZu2S71FLqhtoLuTIyjx1s7fJw\n",
       "q1gbGh+P0ZFQ5uSWKsbRjuh1AUEHF0UwKJweLXRSQ4P3lsSYt1rNCfqNXpzwLjW0oaxbMlqz9P8O\n",
       "1HMZGXgH12rBK6kVlulHShmqDRzGsJAHMe8SptVN/CvFgNUIDa2RPExrnx20eWJx79SzRHZTz5DF\n",
       "wRuuMhq3lIOfB1Pr2GteEQEGMoca/9xsL/tNpfiz76jM+gydLSgHZ3sMlF4EhVMtKIyJVsyXfO/v\n",
       "ErMLnqB+xMckOljJkK3LUcy0NCyE1YUFfl7r7ZsCFDDx3GYn7Y1E8VoUWOCtZqhJCvUnkK5NEa5J\n",
       "9+FtMmFptNL4mXS/8+S/FX+nS2o0Ef0fLqc3eisl6PFVlzLuju/4ZvokCZ4WsBJ87phBJC1PFZtp\n",
       "Mgfdi0VQw00SgUTIvug1Adz3ZUDVrvs0F/axS9l4v03SbaHoY6JshE/EJzYO8H2yQcnATETtMIBa\n",
       "WK93d+1kCTx9QdYo4ajfhUQ7paWmVco8wn9K/037h0Ckl/Kb9CagPWgLnK9wDB/wGQwFzBRotnHd\n",
       "wEyFmFd8IOM2cTzEafvv6TgaK1BRzOMeS1ndR/RsCFEWG7DZu021tos5H5plq4tE3aboyNPlh6++\n",
       "b2cgeYjQNIDbRp3FtIxukop3DQ0T+CDHyP8fJFDeH8aP5B0Qq8AKO5uVSB8uK5PcN0ahN2+73862\n",
       "/ZY8jdZUTX9H6ehozKyiY9TEJYB0i6gXhqsUFkk/GCGZCYoIKQs/c61Eo92mjZRKO7bxOkxVc2Kn\n",
       "wQESVFwl4uLp611mrkEanY0VtsRkLeCaGBEJXNpNXUO+jsaqIV3A5HRYMluy50dcTRA2GQchthGJ\n",
       "fj5Xz6WyIoqV3TqmekHNOh//5nYXHabCasbrPIhRh8ID9/RjvGenyXeIxYfTRSKxrG8F9tyvgUVj\n",
       "NzTbSsKQSJiuOG8grp5JMzyeBkocAGwwzX53aaKH7bh0nYfzf7YPkAWnjohrylnVv7IOqy1b9IHh\n",
       "W4do1XVt+pu01JWMXFH+oc/3XOW0PLmQbhHlTQqpgYjIgb7AIiskzUQZNm5DHCdSCJJpaUYOKdaW\n",
       "5DOq4ymAZF0WgZWCd+E28xBZgQceGzEJHRVDhmEg0HYCcf5fZPryO0cqbfyx83Ld4V3wqpH67I+n\n",
       "aZCKx8yxbvqbIhr4cFu2L7V8TAA3sLaufCvx1+Vj+xsknBbJysQdSI1rkDyZK9urphjTKvsuoHKm\n",
       "1KHtAoDS4GAezl+fhNvfOeSCym9lneLVuLGGdwATNLU4obIEJCP0R4UWYSdd+UoW3n/AH5vwlX/w\n",
       "d264Nlu5LwnHAJ7/KARj3vTyMwWB9SFSWuf1knTLQfcfTBcp+IrY8Hse/8S7kFcveKZ/nwuahFyR\n",
       "dRRWaKHNY6p2NnL8z3LFPCZLPmXXjdRfqOv4qAldYWo2qz5rBM8mmaN/Qt4Gnuq/K/2EdKY6pNtr\n",
       "nqYQkj3AmY/5ZrsS/O7E62ePkMk5vBTuQeqb6KcKDhXo4tY+WAkqV0ZF61yIieR53durArzdPQbZ\n",
       "o9qM9R8GkdsSqcTALLzACMJDAp52Rbl9gboxGa4A8aRove6FyfWks8+BYHsd5pU1Kas/S922l6If\n",
       "BOmb1MqJFPlg6Gw7z/gJMzXB3Li9x6Sact5QCxrgH9T24mCTZY0Rn57TtybevM0UGWbCEdTzeoD+\n",
       "RI/kpdexRN7yUydKAtShsZvCRKSNQr4WXmCkB9ZvagT2pgo7xSCfSL/4ooY+s/sEQXq4MRmUYu0l\n",
       "/p+bucqsR+MRTtfmIGLDWdj0AT8HEAF0rKJXgzFZfuV4pFsty6hvomBH2rtWhwK3RqnK1HlA/23j\n",
       "dw20JlnUA/k3fcfVNRr2Pa8xrXXIM9c7F8igk80hK0BkONNfrjItRriEmYev7ZvmxmkdXEcQR4V7\n",
       "+HKrCXdn48M6Eq0FIvUV5QOhv/AE92jF8UWGAkC9ivxNt26B++JJHOIWId38NI/GXGytnwJBZFU0\n",
       "+a7vgoV1eYu4nfUmtY0b3ICbCKnDxJ3YGgql/RGrYy03uq0FDLVb7hyxWELCeP5X9Sy4Pv+y7BGY\n",
       "uOpweOOVtYqjCTD30qDG9gVTkkPMF03dwmKc4YQeDQATiKVZE1D/V2j//A6gpKqAnl9ePlbKUIXd\n",
       "npFAj/8E9tazL6kPgyKAbBJmpvwdfBFzHWk9+78dK/PKly3Ofk59HVWNEuFRd7betJxUmcupk+mZ\n",
       "gFJz5w0KvCD243GFqL/3iWQ2RhpesxZPVsI5sd5OBVtX5/M6EcY9c+y4PkC2giMui/3PIVYo69kj\n",
       "TN23AIpdHh8mxuhZpeRQn0qmIWBJIgKVtLoHpF0uW3Ga1bUcyHwwiIMpI0Lmuf3K7VjAKzwe+9M2\n",
       "2t+saiyJmXjaDezpy7DylYRLfVx4nyBR0u+tAVkAHMxCxLyiamt1DkFoL5bC6Ix+Gmj/RRvhEyhT\n",
       "RPia6KcnsNQzECOvhMorHSFXQm2SchUKyildcvkyYuhMIv6wb72NUVQ3AXhZIIYw5IPsbiaB+9+Z\n",
       "TRB1zbaC2mILbBv14alVVzCAA3hfm1mAnmCTIkA4qzesFM/NxDquqhlhS3hagNYHDZ0CZD8Zz7f7\n",
       "9fKr6Dbs0hGY+x5XW6gNlpjXf0Uf0AI4VzxQZQJZ9/vGIWPti2Q2eTKv5UlP6mS/8FFe3smKyaeb\n",
       "tARbQ65tEydp4Zt28agmdf2urX/8ADJl543OqZeDQ+K6obQjHMJc8t+PysNUFizBCe00hT9nxn7q\n",
       "ngqaBXSzEsREgiaLW9h7CLJ8FwhI0E9xpke1fueW25VJWfnDGYbrDFSbRwPGRa54B4ScRQ8FVitK\n",
       "NHNXQtOda8P6IPYI3nl8NAm+2oIMhAihroYaC/XVesxvKN6ed2iD3sDhz/W+DZaCcVDWf+vNIg2y\n",
       "rSC19CY4boTbhcW4e/o0JXnCMAmL8mX4Lt/R9QnbGwfMbipLh/swcIg2zh6GN0CMibGBBxqLP5iv\n",
       "XUZl/KQBApJV5WUgjm3DiD18oPG1+CBYlVSTO4t0QRspU4hMsxdlZGw2C4yapMSXGPLMzvBcizHP\n",
       "383YUU3ebh4ZW8514qvAaE9/8vASCPZ12W+RD3pGIgzBqpVV0l/13M5yyumwQW2vaAgKW58P8mQ5\n",
       "7LT6nenQj1hIX8ImA977j+wfqgfKjgj9nswkh/K25Oc5+8sgdHlZookkd+07SdGYBUN7h4lpa2DP\n",
       "dGzBOVs1bRsYzfGv+osQCNv9dCU1Zo7+Pt88BmGK530Pv9b3Q212H+PeuJGGUL3Ab4P6MGS2LiuA\n",
       "boaqw32gt8vRysIX2VJs5N3esC1pVby44Qj5FizY+6xPTZ3UtWfvOsbu3iFZ3juS0MiCskSXe4Kk\n",
       "5LPvTG26M0TcnNpfrn7/szS8n7tHFBEUeIHVyR8XHYpJW+pJPdtJbl6PbhBsm1oRTSD7qsVMewUA\n",
       "2xqJNQQuwjd6kZHtPSi+tls7YbsMnCCwme2MKceX1jXBQrdRfvn0Eku2D0cz0kE4rPGCJii/ICSZ\n",
       "epInbRY4YUQiNsfiv2hKH6/Jzqnam0Ro/2i56P6u/SNJ6HFSShk4bY49Sig/iexhIjbTABWuCVZY\n",
       "KFOlrmds4kVRsACHH3MciBq55XjHXeFsMbxL+tp4LvTTk8JrbpTedYbvi9Ssf2U6zQY7GTKJECAV\n",
       "xJvEKZ43x34xs/92iwvpWhkK0AHL6p8UO34M2rxVdXWLDeD8ieEZXQGOWc1o7YTU1XmA/MAf4AAA\n",
       "N7xBmkM8IZMphBL//rUqgAA6R9Wc7BOSAk37UrwwppNl2G1yeCn7x24yPeQZA0JMMtilfT0x2zrb\n",
       "pWk3afyXFBbObAsdnRZn4BE5zx9ccrMUoJKM9tkqr7v2xFWVoszOEjHa2fp68IjARmQh98DAA+Cp\n",
       "R3j+h2jrD9352CWFworqcztLjv3ryv9XfJEWKNSVjyvFIjzEfEIyamLfGW8vykNTbUATdiNc17im\n",
       "pTaXnwtAvhw1UYklYceveOMUBQtqx87XVvkdIr6WGVRFdthO9ygB4vAh+WvDoTpAL2xkCjChV+po\n",
       "6R3Pf5YYkLIuPxi3xYSXKDEMlQWJ6UVsbtt3XAXVaHh4x8scetowY/0t5B0YrgUydoA3eZunHr92\n",
       "fJVGjEH+HruJ8vFPWueOnHEWeWRWxt/rhRAjlXXHC5puRfAQRvS/jZgPEjuYdCGuRXgiH+r36dzP\n",
       "o2epgyjsnxYp2lOtEa5Og93bRDOX+JSwFZ7nwcIdPh4luC9pQnmq11RzB6B5UNkE7JcigHTo8JDm\n",
       "/rk6qOn7F+NMiucT361x855os6IPyFHliHJ3oD7COmnh0jq8p/pwhelU8mWvN6BZbcldDXIozkOv\n",
       "LR8Z+ysSkoMJT+tyH9XpPwgcm/dCzcNOCXAAA79RqbsoN7u19M3Cj8gZYA2HS4NcFcYZEakTNA+C\n",
       "AvlYXvSesQ2qbbYSii1AqQuT8qlqi/SZYXrPiK+zyAlVxDCUIjnk4b7/Ua/PtvUWiu13JVIouTAq\n",
       "7DY+W+KiKNszPhSJQRezY6cA6/jf0XPqq/J/Ivjj+pgO/YmwYKZ3OHg+MXe6cvp9dPGrYle1bRL/\n",
       "9t0btZVrONbNLWOsa+qQwEzDoNBxYydaWN09RYg6aDubYp5Qqgc71X5cwEgx+iudEn9Mu0YCxUCa\n",
       "3Ot4k1ktK6XTuHpYQVyNNFlq6OaW3cUfL6b9cX1N3etYp1pABa4j+70Bk1kgOOJ4AEe0dZSkI/+c\n",
       "JsKxZ5LeqUrfEGxTZizBEtFFayM3yCjTQJB4pBsJl48nBJVC0dN15EJC61zBoPVXIFzlGPkRzJzU\n",
       "f0sBucW30OV7aYAsiFy+fRygnyKIqBPEs2wgWuE+I6T/uNaBgnev02mkD9v3gpVZWaI3YplVTEHY\n",
       "3KnkaQoabawfJAiBb+A1AEK/6qC2UKNja1zrfy/Jk6C+q0Wgw8WaGauxf7q7Y5/24ubwBnzIijmd\n",
       "2/AAiCTXUDvIZSdcdZhgdo+D6qlnXdqVFyUhvJfrv1iEbgW+VBRZjbL/7Sc4WRbV+IzL7l+ULChR\n",
       "qfrdRTFSYYgsTEOvMZNag8SDloeG0yWfQ9uAezENdSncvc04B6ETql4XLVsOxSlEPOQJcGfazWvq\n",
       "gu5INEANhccs45XdMl4bolcihF1oB018UDgBAjZUYN3ZQrOtAw5shDO8QqnxYh86YEntkLqfTRjb\n",
       "gg7ufyoYDf5WdDPtfq3QOvi+pL559GQUqydWLtFh5EpVRQ10EzUuVBeuSFkETtMu6rZ4FHlvnQT2\n",
       "CtHOJ3Xb44JswY7MH+XGvTbH4ua/gJ58VlbB8w6YVuc2J406drE/8p7ti1Zxccx1N3TL/4JDLi9O\n",
       "h/kBdKY57vbv5IuNgao393T7dRfgrnkcj9neYEweKNvdL2buzx7M6d/0RBBlsd1kr1WHHsuD9+ST\n",
       "uFy8Hwy57DSh3EqgW/4skC0AF1Ov1Iq+yAjj9VF3cCSWtVpdT59NIctctXG+ticSlAYth+/kOQrP\n",
       "d7I9VbBbqlzE6ORy1T7PODOrDUok1A93Tn5oxelKOSyeH5QbkUw3gU1SSiAjoNJ5Y6Z6E3PDuRK0\n",
       "YBa5EX7MCJ9EZe+bqYskZreQykHpX8gIUJ95ApXGdJBqMBYIJkzEPRbLxC6wEASPNk9v2sdMY3O4\n",
       "iCoYtEBKf0OsBPAXBJvnE7ABNuZb1PYDtMIC+/IzGOz0Dha6Po321eAzPOJXioNo2JAdHDrdqqUO\n",
       "1PsVhzBHweoYFhl5IsbNk6p4yTdiS6DjbzjFsyJew4hsxjq21utB0xI4mgAOndUqz7H08w/fUOe+\n",
       "EOleRDJfbXCQVKNmMRHYKpHb47nE6kg52wHDyG0+BKQ96aiO/OSIdjELiADMkHMLiHwox5P70tpk\n",
       "jrHJARxtJKYEdzLaGXRQ0majNocQgp8DOJtSu0/zRTnlobltHZY2oQmdpo4v8sz+w9jqd2R7Atag\n",
       "sf9yqSukhBgKNn7m58TY6HXsKGhwpsyDl/yd4IX//TK3Z5p/ObnQiQr5FnUtBpPLuUXeWYGu3ZhU\n",
       "OXmYkyW+FSfpek5hSv0OEcAqfSVCmgcfRJJSMae0kOskyQNh7nv6rEasomXzxw6aXK6NfQG0Fu8N\n",
       "Dh0RC9oLA+O4/qydM4uyXW/eLrYWpAe6UHOd8NAdMuObyF67glW6nvtsZPBvIyZGU82R5q9OiEjV\n",
       "DHkiFbKczdOVYt1VfAl+4YapkrOlyclHdoJAqe2JQ8xRXOk+g4Fa7mgl91bNWN95YDlfQN6T6rDa\n",
       "fVAb298t8HjauVTYiZYnvoP73Mt+Ob+kGNZjKfhfNbZvhpguryDu+3+OZ38d6SxYMFbzLtjTIT8J\n",
       "A1aKsvkgwMRBr68DvSFelq7HDfBlyA/ghx6TF8yW0LBLE08L5XPEeVBbujTOE55e1cy3bm85USnr\n",
       "OFJzewEtaxaRHqMdb34nwSTJey/RFp3Q4P+2jvp22V0vVt/f/7u/A1xzOvURdxYU/sxtfuqTIbfL\n",
       "IZrNZDGQihNsD3cRO5GeJAFwZoog7kDggYzQ5/7RP4VwzFCA4JIX8Vc6MSB/KQ9JtU0yb4LLFrtS\n",
       "thNBrrLt/asor3Offht7rEerPzQzKb3L106YsCcpOQ2irrP8kg+LsvQ6x2/xE3tY5VK86CsFd2Iv\n",
       "Hp2TcbxoERZjzx6ScMeHhsa70Jkj94oaR/sGQAGDNisWimfVJVsHk/61lLai3OORGuxkMe4h4h+0\n",
       "4tOMSMlkuZ3UDbipQRPaxJ/JAzv2uumOGK4argc5AhdpK82EaDruM3We4pX0GeXjWynOzu14+Plj\n",
       "Y2Z+EaS8qq1HqBlcIP9ubETf9MGKg7WXB7KTwuYCP5DCx+qXznklIZ9yGZOE60J+Ns9iHAfi2bb/\n",
       "gIKvBtZL8Zethu3Gxzyoes4yB3ulYnSmbsF644FjT3uAvcW0pbS4byVwzq2gBhq4xvHFxu1DkuJL\n",
       "5rDZ0aP8l8lJ0eNgDqdPKNwuQxDA1ne603h3oiYfQ2lUYzAT23tKgkiaqt/kjUerZwNlLQVTg9GJ\n",
       "EJVS684h5ssRsfkUWUHkRQTkk1wEwNIRwuKYeqeoUcMZFyU4Jwsf7KsF3XHYRtvMsWaUW9gdr3VV\n",
       "DqSKh5cgerRLITdIS81Eo97vsrL/8j2kHhqB05I8OqF0eE7FN7M/7AL8iRK/QYYYADtkPht7HOJD\n",
       "DJ01xfQi29Hanbsr7Q98XQxuVpJf75aq1cM9Z3ecr1oV84OFdsgJiP2T2VUATDDiOipcwXN8nRfg\n",
       "TY4o8ZO9E3P2OF2SUk6fjrA4tz6OzlepkNgCY9CUezE1+YtlxHfsXG/rsehL8ZxytseX1UXmWLIz\n",
       "N21wp6OPajM5oF9CK9f00mHBvS/vSvwY05p+nIgwnhwm1Ry+B0G/OO3/NpaGSmWad8D5ckem0cbk\n",
       "IJ4LjyiGXicxJNfsS7DS9Cry7vS3gatn3fnLArUozBrqnAVso+jYoIH2Ai8ZRFsGmkGmUScHgKop\n",
       "oD28JRwtRQTpssZHFnBZM7bKucNitHEt69CKuX3i/yfZ9WpHnY0KQcb7PvU32wrp9avBiBRoVuiR\n",
       "30/BtatUz7HzfSr0qns5/byVJ8cDjz2nqQ3BDNEQsk+F/vDlntpLOZk6RVIpwRf1o2ky1j1d8yhA\n",
       "0RqDRfEk2CyuerR+/VGG7JFF1xTjvXpSmHnpU1vOVQeMwsfygXByOaDz2GVNf6NrB38WFG9Hhf/3\n",
       "nFfXXz2SOmO7PnJYDQsr+yKUeyew9/zp8vMNqgO6I4Lrcgf3g4mxGoPZBz5jS4AaZUKfe0y+U+Ge\n",
       "WhFw5XnM/8CpGyb1J0fkZsEUClA69itHlXhFl8JEaxlPiSvdtaJurANylss7rny3FqvBLG26NmzW\n",
       "XPcdWULzoajdOvp3wCtd+iR4qpHm87GolzTksWYx4CZHTiYIdYOanv01cfUNLTWbaTtktYF1yoCE\n",
       "BDPhtQCZSB9SwnwrNl1LnO5WJ2Ivb6e2tTzw/9Htwq2NjXFfcSY+B9wQk6kT+thyefIxMfcPUgR4\n",
       "iXvHIHvKMYaHxybQRJ75mCnM8kpu+8uwi4DjrbuWQ57rqfpE8v4KOxqC5Feeybwx1Sb/r90ZzepN\n",
       "9aW+5PpyfBY1zGt67IAps0ZDVaivPfkeAYu4324voAcJUjBDLLVeRDsUOZW6GeP+b3t/TbVRTvnb\n",
       "291hvUlQdzwr0QlpDZipXBnE0Z/mx0SvmV4XYXckFuH7vCpLtfG5/AqKjvRd5gZ7IGmQlZkUxHJC\n",
       "PYOU8Skhv53D/162NBY6qN3SiMUViGco64mYIQc2pwhpqynrqbG+8wFZqnwLqs8U5ctI+YRZ8M2D\n",
       "BT9MjPrKvUmY5mdyE99MrT3wZSnhiuWvwTTowTm2mGMqrDmLROX1Wm18HGvgir3l8SCBRKH+sV9S\n",
       "3iTqeVsm+J8WJ51+CdDQAswUgVR2ceR+HV/5HEhwBKt6Qyz9OhHTCBhvx2bsiwoX9c4+NQdW9Erx\n",
       "PX3Py1L76qFwYHZvFoUZFgaGWVzQl/H6UJsNtDYpSUUrz83Kxw0urWUn4Y9Kyx2IKT6D4Ovvrrwb\n",
       "6a6mGL9u6IHxN/O8E46103+mlQ7NxvIH1KAxlrvKBQ8nSQKXcx3vWMAnlZwt5IbWfAkrdgtOyUb8\n",
       "XriMAy0fXJi4jVHYtotA+QADCsoni+YfI8Vzs+dypYh8RhhZXlaX4xcgkvomL8oyBvp3jzNo9D8J\n",
       "25Q5e17RKXGRWVXC2/FWv3RJDBolBiCGrZEvTxsvTGcP44VGPlZx+WUuiv6YgTsm4G0/lmyLR3rL\n",
       "6fM5iSKW7ksgUo9iKN5zO0e9odV8kbjqLkdIb7WKRy4+fgnzkAL1P9xCuPV11F/inMebi43rwRwY\n",
       "u2xRf2r93eeCTIHBXwUJOTdY1AH6tqVii458zUxg6KKT60bW5cxbq6KpZtThf44phKkrDySF7NSc\n",
       "18sVWrK1ZDyAjmjJqJZuaRW0dsIj0lzuWsZ2DWVSn/3RvopTWOwQ2Oj4Mt3cG9tlc+X3Ww+K1Hh+\n",
       "UeiKkhYdT5xYTXYjzHqH0GQFot/9llubdptNwygJ34ANUaMJfRbHh/krIzwvuQ/H+BY5cFcjbMTS\n",
       "gJYWIkO5xO+t5JGGKfD14UPtWD1C/fe0sqqkX9jfKUKL0RNzU/J3+Uw0NfJK+LYRbRBPHockIiHr\n",
       "fmQHGQjiNLCFHxiq85bkzPfdq9tK/aC574CMo9e37uk04MmktUYELgy/drlBQs090UHgVreix3so\n",
       "95oskLAJ7holKYHLP9lsL8YEPWClQ9UF6FY5V8VJn3c/EWMpB0S2aOD55Pnjd7Y34VveHtwJiWO0\n",
       "gUNgu6AKouFRKb4QCDNB4Uai4Ka+6BbGNBLuAxlnPQBnkKCovwB6wmCQBLEjPA2uW0zE1LJnpFr0\n",
       "Vey6iWEpPwfe7ZM0uKVKf7j7sYTUGt4NVhvRFM0/F7d8nVBltkQFRC0Ohov/oI9iG6is3jK9Z8GL\n",
       "YYvgkiSt9OwM5SSFIZQcpMykylYEfMXYWPb9vqN/68U9FdItVTZTgKvlqaKg9dKY1Hea5mhmrJqa\n",
       "ZOLylCPljXPqJb2e8A2UWqNNIKw57Oly9X6/nd0QgBzlFf/UJC9DgxYl75+QhM3/oO/CykH8diI0\n",
       "b0ZEEglBnZT5DAOIE5zyc0q2wURqb0U0ATqfBNca26WGbdSLevQ5R96Z9IlQ/4wPuZkb+WoGZ0hh\n",
       "FAOb5JTXD0A7qBeF37u7X+AiyIn39f3pfXk8dU9VLTMGZc5OBctYq/2fTeTMpqSZyspj9cUm6yfF\n",
       "UAMdELSez+qMVn3LoAnmhxZqvr4zmsOmTEU95nCqkzOah0cETDHUrGIr7bJZTxDEX/PYLaH3XChy\n",
       "8W6umYtpnvfzOTEmacUnTkGRgg8Iorb3LQIjZlkg+BS2/lvZMTPl5s+7TrVjrmhReBzOYHYQZSSO\n",
       "lEGhvcT3BQTvofZw9O4t2O/uf5d8AUmNNIlI+1GELf5JFaEjD86hW6go/TK4xB+5jEmuj/nGIBeb\n",
       "sYKUqoG8UWNDhw7lgj65x2gMOWCFE72J+MXH3Y2FWGLRwJXKV3JIujD/ZxAlLvdBjEk4TXtu69Q1\n",
       "0FgMq/78u+kPQ+vjL7meHgSqwem233cGwnUShsSOuAH+XOomlMIHpFO2cRkWnIZNu7X8XhdvnyvB\n",
       "sRqEMieyfbjnmWstaNh3ciz7dFsnQTgWIRtZhvEwnygy1NGSF3F1ylHk6OSXnrC+ePAwyfcc59Pe\n",
       "YjCvRWsEWzQjFq/ckwlG6XrTOaP1/iqFV8JUrYS+ZG8jQQ+P/wi53Y7U20DjgzDwYcnwyEoRDmk9\n",
       "HNzfxWXow1w2D8zvMU4CeSNBfA/2g99CnJSPtg5TgiWoT2z5uiURrakb5zfWvRmpZSWZlMU6NeMH\n",
       "dH6GAxi8vnXLxuvLGuJkUhcMScbPeHRtjWsMa775UjexHLdhlLTviSeL2MCRUY9MXrik488UUTBm\n",
       "dR1roFwjD1QZlMduTfVRwWo5zdAU5IcBSCYwyYvu8qJGikDjsOjQJImPYbsT45+o4S5e4Oy3JouX\n",
       "4sjuBLP9XNUZFopA5zxzMDgsh/bZl0yyvYqfP9ZAFCiVXXzl7uLEVKJdXtH4IyLqlOpxqrCtq36Z\n",
       "1MgO3mDvPR3Gyo0cf+TEd7ue/L6Q0+iZdYQFtbq2K/ZIZuMlc0cNRJVA3PlivcNOBTxtEU3WXoqp\n",
       "gtl6yiiF/iARCrCYF7JSz+dczKcxLwSbZSNk8ObNwDwnp7CpD/zTf3KAkqIEEkNmw6IxXOPNofJB\n",
       "W++Cx8e/9co+uYhDYyRiakZgcfDeLqRF0PsZWXvB8WDB0qY0KuFJbqjlWzyA1NsHHOdUwIm/n4Ip\n",
       "xSMlb368oXgCpv4RSInpFXAYG3rvu+oxqd1Bd5y0RDEdS5u0edJs1VLTf6rIwzciQbvA43bheYwM\n",
       "P/UBGTWeAfXNPO+tehdt+SgkQtISVcnbEFjtgav/uR2FtaKS/8+mayJ2rECp8uEqQw88SyYHD0J8\n",
       "XVsCNd0KHMJHeRAhRcwV38SVDP/xiaeo7UapNhdUXXDQC46Y7btcjGsW0UEvBRhk9XjNodKOHVKi\n",
       "mtjZ8OS44wZypNo7mxh+LQCuMze3XYtkHSTrVUqIKH3Qaxl/7CGYWMXbM7ql1NgRtKauk6EsapW/\n",
       "/17RRoTtwKLQrHMFH47UGyKt6XjE4xenZ37GGwtzrdUmLkzEnZ/lb1yuhB23V3jQDwFon8Yk3WoN\n",
       "m8G+n9tpT02aBR+KqUlCTd9QMCLeqRdS8c8q5yetFDddfyprrHlXEaVzQs2aURxac4vJdHzCffv7\n",
       "8fWXUCUAdioHJgMdeA0YwhCfOzPh5ch4DmCQBA7Sf0bA1fMFV1Yca8xOZrQbFuIpbBkw5DZlFvcR\n",
       "Ft3YrxZuhFySK1iY7Kqy/mW14dX3QNWG+baqu94vzGdv6UR34kHj1SZPHo2HsN3EoS3nElY8uZrd\n",
       "8YrgMUmNEiHGZP8tZVeN0UklKz9NO58cnvDjkRIXLzcat1uovTc6GwdHLj+dDVsPJLal+PKksLSd\n",
       "YE9TWoBWCY8sNUzb2wGKGWVcv2MGKAQ8JPxsSKPT3sSYzLvsbXiWhL8ptJVzXUPAQqYjGU5ciL9s\n",
       "j9bPnoOAKiet1iTgt+Xyjl5KGPGypZ9Ix/tWyUivLAw2WKep1kRaTReaHw1D2V6f3r/2aOL+tl4W\n",
       "/tMskRtuILzOvnziBGk3c1saEbeH9JhkznP6xgU8VNivduf8YgBh2susOysJy7WtU9dk7kSD0QEu\n",
       "Te55i5RLxCe0XIs3n8IpuQmD/pzPAX0K58YMoOGiOM8+UNPYUAjVPcqSZoKatub5ZWJmYIXPar1Y\n",
       "vgLS55gQXqDcMx+augQsg8jU6zo52vVnZ013+un+KsJlMs66gPMzOoVayIB0lpvsidNXfL7jceHC\n",
       "RBjFr9DXhwRvYJ7MJONO1HUUa+B6kk9z0CMXZnpy4mAhhlfJwzJTm9iGhGGyng/e6woZySjivIuc\n",
       "NTYQ7eHURO8ZNigkxDJSU14h/U+zDRqbWbYgIq6IH4+g3SttIfmaW0s9PeZw94IQhNHTzEZReHWe\n",
       "N0gnWC+qZEP8ZWPfYifxEEYxn+PCnnkm21bzWVg8zHeIlHVeVTBihXm/sLTZQSlLvl6DlEa8bwFP\n",
       "fiMhDm2SbK8XeVAWS8bdzLWREUph12g9/+UakFrDxLCwWNCT/G0riPRsHGJKuKFmShi7UWocvjpJ\n",
       "qmseYOP3VWKHuCZutRBvyQyJlFiEsusT8a0AFzVg78xkGp+gUvzHWZPN5hEVkJcrTrTfozIS0YN6\n",
       "YI6PliUl4uhE948fQweKv+sVk/1e1FPpVq/tq5icvqXmbjgMUCBBxr9hUHquq4TxVR5DyFQii4nX\n",
       "SkP9/vJj3NqTGpQzGBKK9EgrQtDemdrJULAXJcxPMy+cg6D30NwvbrteDTscmOPQs1uWu1baWMkM\n",
       "hM4gpFQqlBZFEaXpSCuVFWMMTSdNv/zn5wTgdd6A3NFfX9bPJKmBTNpwA8fMq7NLcQN3gEzc+Np2\n",
       "uorN87iQTFZjV+QOviOXWbsbJCaUxaoJIFqHNRWF9rHsKQbm0+8w5tgDO3JUeCvGX4fBkuPVqX6S\n",
       "Ve95k3YRIohbYp1qtaCLKyYGCPdrSqJtl6bAxN1TOJz6UIWZJLinMtENtxC8rYjc3z4rFcP/J7eQ\n",
       "L97+R+Lae0GQ5TnzRz1F/pTMSUBWaSHjA47UdnKRz6E6CrYD3CcfY7dUsRG1O4y5n8vg62y3CiS/\n",
       "5POHxa0NS9yjtHZ/FGiYNa329mAe0ZmxBweTx6H/opZ3gGXQLq9TjOY/7GJkH9/5P2j/drjcf8OV\n",
       "JEftaFOugRBq1t728xXVVG/s1+t+SGKpjrqOYyjhGk0xvAulVr1QIbVXXQlUxHxCYq+4TkG4xr9w\n",
       "REXZaCOFJ3mNNHNQHLdFFJ1L65bRTT9zxosPIpMOPH8E5OdUZzBLo1O58b/LKizCU/hGMQFVqSug\n",
       "K7ZJmpNv896irkP3xDF41pA+IfvrxCdY+RT1R/4qOcv0OUtWsXGcgm75fN7sI00D6VXs3H1WSuVU\n",
       "mZ0tN2iQ2ZOD7MVx2dFmj2UkbPuVcPZvX0z3x7mjVTq6BE0/qQe2hJNutkE9pdSuH0J27xCw1Wzi\n",
       "kidc5ufqe1uJKO6dCPabIUCderPPZpylYOLqWxrZVWEz5v0VSnBPECy8X10LT7mM+jDbCTI2lWoi\n",
       "VOdNeUKqSSbl7ZPIR2IR/mZ1WtYCzqHTCEdhil0aaLDsSVEjJu3TSwFSiTj3jpo9c4JTwWSuykoL\n",
       "ei3fCSv4NaCWSn8hDhdW2cfw6bruY0xtbCEHZS/TdsvFzfv7B6+HYfHjtmHcJSyLlP6Gsrc4DYEY\n",
       "xza54el1+mU60Z2PNN9XL0FOG0WnvpoNA7rSZt0oaKIso9QkxMAr8HJkO2HNrHgTnKU5OMOajioT\n",
       "pAemeMYPXTpaJRQ3p6p2fMXKF54V6qApnc9d3WFSHUwG2QHje2mwsXbfJJ1pwmGfh/lRgL9cxffW\n",
       "OirN1/tehJ+0kLR5NKXs0Pf81Yv+0D7rtDAinrEXJ1AIxj5chw4Qwxbb7lrYf2rOWnqFnUk8m4/K\n",
       "qddZVZ8wcj2d7TtPHaWdNz0db1iXkq56/s8UjJdzs8B2xzp+nSSvZrH/PovkT1LBLNZ+5DStfHbY\n",
       "2jy3bgfqDQtu4Eh6U4zrERm5I97IFexxwFIjMSGsSx06xV0ts45A68mrbgERWRJzcZeIaR8mYYuQ\n",
       "o/mFYqOcWwMkn59nf7NLQuLKHgP0mYrnhBYbR2OV8N8ybGXIuk/slH4sBxDOLxToC/HPKyU2oeH7\n",
       "Qo71Ljq4mvoM6nx/DkiViMjDl2mzoDYUjb/2n6MqzFRX5O4moLzuTmSnEeJvHmGMB8hSUm5dvMD3\n",
       "GKmhp/GO0znw5iPVbE50gRhkf6o4kRsBMkcU8hvPzzLu6NaQDbG2KIPfksqvz9aiwShIU9g2V8ow\n",
       "kigxSu7mo7clD2WTKD2cwvdxMx7aQF2sQe3+U78l1LFLf55jk2QtTBx4Tt6jis1xQ2lwM/0/Wy9e\n",
       "FyN7PBGMNmf63avlTFqe9h24KUK9FGfmxplHKIxtYkOnC1gFouP1xBDGIp9/pbUhpZRhx3kP7gxK\n",
       "sJUGWacFOdhfwp7OSV0cS73lKL2wcQ3sBGsLy19zv1yU9JZIAMiX+7nPeQOCgs4dzpWbB1rDYc6k\n",
       "MrKpIVpnpIdgrQE6JGBaFTgR0JLOVFZ2Bx4ILnpZzCaCzmkgSixGr2aDUH0pvMdnuXliTtxskN4g\n",
       "Ya6gy1rlNRuVkKYfRQcfRptlyTlyNP8Iu/GmkBovr9RZb0LMOoSMf7DzLGk5Digfv9nRewz5RLX7\n",
       "AHRrU1wbK0bZmm0RgyBfDtuh2VlinYYL9A6ua6mTBKnF/NlaTrSlkofwG1Tod7KFRsfxDJFoUBJ6\n",
       "VkdXNCds8hT50HeFCHpRJqX/8/QdJqzRakUVZH2Q7RLheZ53tsjl481zXO9JmN4+QxeOIO3IYCb9\n",
       "pOgLieIDpRwx/kdJ1wsf+1Cb8x6Z/tjvPT2pPSdMoAiSLErQsyATk1CTFN5ZLT4Ui6r4F3yD6j+L\n",
       "PyeRpnmihDMr1KQkztrXFJJHH2H7CsGx5+NBNvaihIfK7fjgjP21OvLDNaVe6v3A8PSU1fT0u5Z6\n",
       "kohDD7jpt3E7T6aRGd7eBYJEHzN8kO7eL3fcKsp9O/7ACTkinuZrSXWocfvmxxAxH1g1w06c4haP\n",
       "9XVJOnwqjs2eXmyMXQxc6phl2S6KrgAX8XL+RRECOrApcgmggC3FxE4GQcmoIt/ySVRyjxPvKsmU\n",
       "XSSwSC0x9RC2FMsFaxQt46kEg7WB34PSe896yohW+ABph+FfDZ4QUeCgdxneWfMu9/lQ6NS4XSK4\n",
       "PemhcmaO6TFJXXnoC8K844ZNsI++ci21rSPnJrMtXnNYkkFI3Vovt07oLqq1mT7rDyCRDLT+yVKN\n",
       "DXsRtbeprhPwtO7MTe3SJAzvUhxKVqNEKk4DSajGGMoB3Zd/5Eql9UsHHRWxMJD9Tu4QpUOdqLfN\n",
       "TgOhXQeQ5RYNWqt2kINbf3WYVbwSivJrqaNhbKe4Rcr3MSttqd7wvkpV6bu+ma9a/8AVGO1t3/dg\n",
       "FlQKkCBUFyBbrIvaQTajsZcIXikdqkfjFNsYdapSuQVoexJK2oBgVWIDKw27KshwqrMVy6ETMGgU\n",
       "40FWQtL+DOGWXoiv6A9VZXf+RHejzGMgfjdbEwWFJDDzfYehqmfBEgAHzGl0ZxLimaGU3nRCl0xx\n",
       "FG+9jjDm5YWNo7+12KsmlOKl8vIUdOlJrCFvQcvHtSVbbYHIt7Nj+qza+SD/Bx6GkzuPozteKyv+\n",
       "x/ch4riarcv37C0Np1BbnMgJ9ZysC7+Zd3Pw5rlyRAq0tasYgVZ94X/mQhlw523We6R5DT56LMEB\n",
       "gtaFnWz0BCX17/PYnZKtb/50Wvw9K1OYIIZVS6+UUtJ0hJ1e1AKz5iDD6XpM+1XMCuJl+/ARhJLl\n",
       "eslNMfH+jQMTT0ojWQBRtNAsJcqW/U2eTIbr2cE03n9dGeGIUsc9WAI70pqeaoz0EjMXxpwHTMPR\n",
       "Hc7U17T7TVS4xna3VGugvpp/F3r4574obhf4SDJCksT6+grnbjW15808hjzd4b6hA7EZxfny4GIg\n",
       "MRxRG1fnCWaAuq6EdfsJMWkySl5yexCVaHHcVs90DbKZiSv3MBLFGgShIoVxnIjzTX6hg5UiT+/P\n",
       "Y0bKyoJUya4b6G1G70k3QZyc16LnR3TjL+YHBiN1a7Ksh10a5r0BmPdNAPpANRetYyv3aLu35PGH\n",
       "a+5nOb8yBwhM1kgimQQI4xBeY3sEmA9vuewJJV88IgpRI8KaD+yWdCb5k/0SD9pZ5YO9OVM9SBHw\n",
       "9y8R/7JtzeHIwmb+qOQu+uU+49M6sts/QFVFERnAcaMZbvEv3Ggg0jOXfr3ODRz3VPzXie+3i3b2\n",
       "lV7nTQwwo/4QytLYrea1vU+tC7IsS2D6rnMpqaNhYOIwkWKNpGtTGY9gAdItYWahoezzFmiu4AfH\n",
       "wZua5yL84Zz/Au3WYvxq9i50fKetdUv8eeTOyy0N21LL2CUcLPqB/DnEgNT+zTiJuoyUQvr3IZp8\n",
       "Z8V7R9kEvTgw3LhUQVY4zT1pokxqmm2DPDe1bZfzFGaTAIVycxXnihszC8TLZnIJ1+LRN+16E+0A\n",
       "qhDikrp0y7XBmcZKnrN7N/JC4MEhAE2q2mv02DzT2hM+h1XUi0vCMBs3TvVV0+66eqWNv/CTm2TE\n",
       "8pXdawB9519io/e014wzM5pkmighakSVwjOmmVh84O/z9LZXrjDbr0gr2t4fovBfMx5KBLV5845r\n",
       "sLWqdUya2/Rz3kmrwgca1i5XrKIwNPjvsf39b9IecKjQ0YEzmyyp9U2h/nqYNEC90iDY93V71YTS\n",
       "cmy8jZrK0NSG8DWDiqz2g5zhY9NQ+4VtH4IA+afUPbMPbCKj80xtnfd7gsnLg2RbGVirYpd7kxii\n",
       "t6ciIsxykJbb1suodwBl4ePD50dYDdYV7lJMqCXN8IoZVTNaqPWOObYGv4/NTwX9PwOA6kCVm1lB\n",
       "zLzXNlagKewIUJTdgL0FENFWwrocA43zs9XGzHCw0hiTCkFReAzsDylyaQmxa4+smNewxCfjrCAd\n",
       "DrxNnuWY5jl8kjTGmKVJV2mOGQw0R5GaLAV8iw+lO02MLI3i9plMPLzixJ7uvAwsSV+eg8qrm9M/\n",
       "PPeR43hMXvwb8gcCPw9D7fZnoL/SILRxcWMWpNr0t7kJTCvxoRYBtGuzTAXSDVIugMjVH25bYGjP\n",
       "rSBTjCVZE545XLlCzYTvCpJG4XpVflBYzimiUH29/zeLbVnyFj8ZWp24Cvh+g85wh1TCjL/M652p\n",
       "tHd96DIwyboS/jVF4DfT2T2frOgPyBp3b0Oa7rgkMgGBWmS7Oj7hBY6PrZs1Hj7wqrw7Kv5Vit89\n",
       "Z9zbUpom/e5TKvj8e0xnanIGdmhQuIcH87CrEF3VsAo7fDWYcUDq2WbLDBvwZhpzTD69GM78CViM\n",
       "f5ageZha7gjnztMDa2SEaCjsDp9qGw6XKOlIUyjuSTVSQ//xQzkcrFHXlWnRXJ3BwGb5plarBsIY\n",
       "or+hRd7VfEL78VYcUHc1hpd4+EV4Xn83UxFg5l7ZtNlx5Zxa14F7OqB2g92kUrJg2RYVzo0gxczd\n",
       "YB2Gr3zazfljVcpbk2bj+tijk3XDKQopifFZRPyuWsLTJeiMpRAw4uPWZBG/LmIXFYMWxfK99EEL\n",
       "Yy8iGuBBFqpVtfr4mpwjJ1xNwnY5/bui88fcMJhQLwyk7aCsrsp/z3gCw5YCeDBxb1taYLOxxMwn\n",
       "YZOHIL42wUYLkR4IrIQ8EIyLFQSGtgRozEYuEJ/cQD5TR3bLZRqcYGb0Fif3BAlcesFyZRJMV4Ku\n",
       "iogPg5JhS+vcdspOc2sfSEv1qDIJREMym40CKqe/REPbi7dWTWwlZIvSbNGLsMbJq8vuHbcoe5WD\n",
       "p75dtUzgJ/zVVspuGJ+LYpq2O+DECALxOMDOzStQMVPgq/jnMX/kCdO2QGdPnfbnlm8++P1KYnnP\n",
       "M5xW6HMlZOCIY2mr6mKEUjF1qXLWcJWjKkYZH3fh3zs5CZL/aGB/jtH4PEklNzLTpfqOaICXm+U8\n",
       "qrtxE/4S5JU2z4u/I/xB2TgolCQWeRoc0YRg3drvspeThO+QLADZJz3JBWa7QmMITVpnn9bv9WNQ\n",
       "GjCdSXcMjkpmmMRuQxgX+6FkjTI8w/P04UZzCMwAp3/0V9qIRAPptpYTjIf14EIccAY70DuFqD0x\n",
       "IHm/ZK6LJQ7oNQh6A2rVtAzVXBZoErBSnZB2OHzgVq+bUavdueVZwsPp2eatbRyLeBIxwyMa1Vvb\n",
       "T2ski5GbJHXUw8XtWd5pvw4M0cnPwOlW8afQAjTyHtgVfsMdbGAVS5hV3WFxTbCuC7KcmDmb7PEk\n",
       "RnAMxIHIkJXyZ9VtAXW8hVIfHP73J1P2U6RBd0TRdi0jN/SwPQ/cCSZce8kselc0KrWNRP8nhM9z\n",
       "JvWFxqiOOJAb9uBlrkRx59NUptGgkGXJT6Cg2q35Z+A2hcJhC49D4tmbLOpadYbu4fsi5jID1xGk\n",
       "rY7gM3WXpTH8QUn5IUHn40ORktnmrMZ7FGrm+x8iTe5VhrIG13KZRqSkmno/7jhQerHRX57AlM0y\n",
       "ZMLdnUQ8muQf2nVvZysBngApXXxMBZtsxcIgReIkcMzLJ06CJfjr9BgnlBJNLLF8vL4zvW2dLbk9\n",
       "g/tMoLbW0AWN3YudpWfIRmn+0TMZmYUX01lecdIi7RnZvEyRMk6l7kXyI5WFwZg2OlHRfJ7f4GUh\n",
       "Yfn39SiVYmfEbhbpQpO9Eh9fQ6AFyxndBV50t57k8qCwiwi3FWFYN6u7vCNIi93FRebxnim92ZGz\n",
       "cekuUKhWIwIyrKjGu2oje0D8K1PMckScU+ACw9L7uC0mgxTRIQFnYEr2OJxWs5L9+S0VjENgpxPy\n",
       "ziA5ahjN1fCdpnSDGKJDymxltfRQozyJAeKbMezhIsn0Munca1cE5npO9DjaJEk1NyQDx3FDqOb6\n",
       "KQDCyZ83xEiTmt+slwIySS4Dq2ZZ1TRsJvEn90/L4rF+ty6qxGaJL466UAxDQWm5aHhZYjNAammw\n",
       "Dwx6xS5RX0RoljoBbOf6NLR94EDB3/S76fleDG5urpJ2BHNlcarLr6u7qkqzx8DlJi53B4cHxEYI\n",
       "oOCV6Dfvv+HtfzsZHyEZNl4e5a9ZzsdtlywnZ5WsV8nU9GY15wWeVGm41vCri+kx+8xxVkwiyoVN\n",
       "367EGCO9PGneDKhyLRof2hXVUvP/LoQgQ9hM5pXTPw1ISnZg4sTLxJPqvh76LCzb+u8xmjKSGV0Q\n",
       "ZuSrJ5kRWZ9JEoGAqcWJ2hHcS0oX3Wbz8Rjl2BiQ46l7WgF6WkV6CY0wBtO5VO7lsFxxZrt3UAEa\n",
       "penR5r5p9r7KH5dQbU+IK6FgGEpQNjpW8DAr4jowD4QSMo6qFM824x6Kg2Xp415iK/biDXlJzrHa\n",
       "Zs56VRmBycZGr46f34Q61dq87KDBJAwZr+uiZ75/Kyy5krgBtQGuZxFncfHxslX6jxPAIFHYbd+Z\n",
       "4B4jpyoTl/+7ZFySSNCO6NzLJ86+NMbl7RB4EZl19c06yAR8Q3QByaFFc9IbPgcdZwYF5PjtG/Yl\n",
       "SmRKALWImcijiGN3Y4Wl1ILjbU51eO807xV+d3qynzL27N9Hzea6A9M/z3En3wmyTl6FLIkDXR/8\n",
       "+wZaz49xP5DiaJRo9zNj2LIguPj6K3g2ek80yfyOUELBGHV0kZ81VzLDdFaKwpJ4xcGOBGs20IzM\n",
       "mbLzu3ryVaxiDJF9PwrbZV9YcXB28pkr4iv+i4dVTuL8MkrlQ6IBIVPRrjQwCl23P5iN2D3CBxU9\n",
       "S5yObB4+8/sndKs+E8rOUpe5+N+08/LytFmV6q11+B0kxmQw7uoG7l+cmBHzKjyKFWL7gZGbwEma\n",
       "ygnnds542xRr4XJlp692pGbW0Ys/9E4n3E5jsjviKfaUu0SR/vpPXhBIJShQkyL07/XWVxnV6J1Y\n",
       "K+mLPA6wC9OhPnXtK5TGCpQhgqazCbCttS5z4nmtJbAdn4R5BsaujEETZKZfErDZU5yqOjZoFtXd\n",
       "CAux0k+VDas6U2YXeRrqIxiPcf+CLi+WRz9X4fe09vc7gwBm1zE+Yu/bc+2v3qlOmAGy1Qpoh4Eh\n",
       "qr6g3CI8MU5Dmf/TquMNE5zeZ+IsozSBu2lrdzaWB3iS7EhGMsBvQVCHZb/c/quv8z7HvBwGiJk2\n",
       "xFQrWmtjV+cOqQIv60DmC/XZfr33xU+HqbmOlAI2FdrMN51faoyimqusgLrrChMIr0IYtCjgEU06\n",
       "tlL2/hWXz73rO6/9cxCqzWjBUKP2KNXtWAn8I73xwJTuKkYgDLoAPotc6RrX4/Ih3P0j46NtI3OM\n",
       "s74zxVfUCS+JKtMwlVaFnsWNY9CKX4g+KgAHCtBjee9ir6rXnaEThr+JN+1zwmw04wuOzRV2ucDz\n",
       "Wm4pkKwf0QyGvbTMKCQs+ee0OEw8XTBv+pmegCMZrhduHx7vlupkAV6ciqjRx4hD0J7ZD48QdCVi\n",
       "0lFL9uQtUmpn0hy5tKZcWnYEkDA/bjfCH5P7LAn1N3akJ/vDL4bqtkUzughUw8tNrAdQooaacEH0\n",
       "HGx9+mRiDa9ff0ygAKO4m5kTLi4BIh+S9KkKp2FTD4d5WbSTUIqRzKgbZIZbkwFAO1r7lD0LS+sd\n",
       "kNNw6joQqFLUzSKB39YinC8LxfOdCrPGtXuyiscdesM5lC1PoiQCsdqCkmwWvnC9u78g1j6gz9WP\n",
       "E4vCJXbYvYAg2i58UbCIPQr6/fiGaG8JIBMyWd/zWgRAcNGmr0Jz/oc3oks+AiAKQUHcOmwhHmDY\n",
       "Tpsz7vqFHXxaM3S1Q+Vl1l3XqseGahpvh36uP8Klo60Jf1YepVlqQsJCeLRJjmcXyb+JR3Rin2cB\n",
       "ycojaKucd0zrx6YYLt82eM+wuTA8WGJm6eCV4LEwN5HzFgXOVW4E3ikJhHq6EPVqf5RhU5ERpL5q\n",
       "rNhiJ0nfZtzLo89IFgU6FPsgcw6LENPyQMQ13r3Q5yPrrgiT6Z/0wHXBCwUTyCMKRHBsxfn2d2nl\n",
       "HaDDKI4idzwsIU8xGazXVTMczADR5f3j0kRrustPCc7MbILd6XucCw7Xtd7qeq9gXl5uJzHyGkWW\n",
       "kIvqLrptEiLoKXWKWJLxZKKjRKvVnebXuo+0ESdAoLIuD8BTz7gv2J34ffkGG25ngEN9A9TbrLmL\n",
       "M3AGgCfx4s7cdTNTxqVUcWyhoTsfUM6xCxirPohGLqHbgYlR7cQqW4u9g2FPTJj9UsfSKE/NiXaO\n",
       "ztiJhyyyrl4lAiLaGbBHa1iRye2ZBXqMVaZxs10ir/zPNnYelEYpKLSzTw+i2XrsUcMlEKrF3dG5\n",
       "G7kgw0qtr3wFMEqxzyI4pYyjyxe0f34owzkvuCmxqH3XFJlNG13wsl9iMHM2BqQSMWUXIPR5rCcv\n",
       "5MAe6uCMY+GVAq+iTt1qZrcUGp0j3r6J3JtU6yRZJrusS3o3739s11lUFukupTXuyTxqqif7LavD\n",
       "5t8zXnyo2CUFj7+Gqfm+HbTrPL5CTZGsCh1gHaHDTj9GQWeMn4Pe8SGoLa5oUalF+bZ7Oz3ceeA4\n",
       "EvgGFHIXUfShMnoCUZrOxOlg+TmtySggTtqpq/r+SnsiEK8zz51DQLd05BbAeyKVOaNl92CnOqM4\n",
       "vbv7wUjwHGDnsaZSHn4HMtp3z5s3ynvsZQ0Ex0nSeWR2mLItIBlSG/RLFdd63ivBgnRXsSxJfe3T\n",
       "1XYmLREYn9QKT60/wnCzc1Kd0X/c/qfdpp5TfIiIYpVDICYvUppoOfqJrwDD/cYdOcp1/yDu0qD7\n",
       "mizyFL+X+4hpNgYKiA5TKc2nlwRTjHJtFkiRj/A9bvj7Cua7Jilu6g4x8JZ2HeWXJN+omctopwVI\n",
       "lFrW+0vBQZPzTOaBo874hie9OIy93V6HXlbI9Kc9fpJ7jyKkRiE5E756UoAQaG3veQ7cAz5GHX0j\n",
       "/K9q1pgnvyWfF5B+9xHWcxg5UglQoEw9qq5kvB59FnYwEI4RV7AqWUavESG2s3HsufN/Cz3ZcIkx\n",
       "svVfJClAMKQVj224i1s+J2IdVog23yAJ2/i/7uR+j2G4oBf1oapyFOIewOWRBIUe00yH6tlJQBkh\n",
       "4NlzJtcU0cQ4ruZAGgk/qFcpz1XqZd+rbhtPtQGCM01T5I23qNdq7i+tArYI1rT/nzry+zK8Jpec\n",
       "UHlwWXBTrvDdhCBkdQ0yENz1Xw8X/iyG2S/9pDWTCgHAG5Yloe7RYpa60noH8MstWBRXqV87MXMk\n",
       "QDF0bHDdd97CD2XP5PfC7+we70ptu3TfGYOBxIQs/LlsJ4cfnO+ENH7mnV/f+Vszot8j8XhZVPIX\n",
       "uRi5B4007uJGcV3J5kwH4uviSdWbUndFVzPfj9ipP4fJM7B/UQMRtjm3M1S8VoFL6N2p+ehjf3CT\n",
       "BlAkA/YflaTSifEGcT6sPmwC0JApH5FpNkurjtWfhEABpfLtTj2iU25CysB+ppSihEARZ6IKaTCc\n",
       "qjId2pdLDXfM61DeXnDSqO05qTHAtMb9jDrL++l6k2Odm9LeCjuhyZeij8K8upB4XcTN6vfbKlRf\n",
       "qtDf4Aw7U7bph2Wj/es4pnfzr+6FXMX/tgScI7EcwwCCbunnSlKeWDUBP2cGbZnwCDFZqzQGzXfm\n",
       "cGzK8yMvBhiwSIcmdOCy9f086v9KU2RsRuIknan6X56LRQfkIqf0Bhk3qaYQjgBMjbn/7xEAg9Md\n",
       "oeLF9K77b3oexL/dxqGoDDSfEffV2f9dkJvOHKw+o5G/0gZN1vsjX7wYvPReOscxKb0ybnCQ1RFz\n",
       "edqZL/Qv+8Tn6upk8fVyXfK6p/lU+mLIKS4BnHG8VUe+MYfijZaWQHUivd1SJQYMQB/QCAkaaZ1B\n",
       "9BnAihhyxK9swaxdIsFBUVxfvZJyutCkRJvNIEHRIJTzwhY08kH41yZ2vrIxeezHZVpZGYW8wv4M\n",
       "sBOeHbmazr6n+YKSBw6x39WROs8nUSZUDH7VQVf+Ow2fhv7ShOeMsjk1bO9QKJcTIbvfhgYAB8aY\n",
       "Jz5Z9eDrkAONCxlaf+hVTNO/IDa6eLLxcjUwCo8P9QSQPbLiDOvT6sG4ktfT4DFJdy9+Ta+Wh8TR\n",
       "sQbMgDbyR2x6oIpzIofWNwWtQLh25qGXC2i1n5CoG3H/xfGc9BIUF296RwaPbt8YX1XY9InoJfUs\n",
       "PD0mGP/PPyl5k55JCnhJ0OiNYaU1SMkvELL9GYHu9N7C4kBTrozFl8IZoLDMDafXT8dCmND/exeZ\n",
       "2tZZE5WigrU++wYaFjR+6trl6W0AACUPAZ5iakEfAAAmUfqc78k6xZQEI9NE1pK/U/3SwyxjQ66K\n",
       "sfMuf5a09PZ3pLqlUwr4o5h6/FKqaA82gLvpEDGDMq6JjdnJOOlFZOgd2ibW4vu6o+W6zpEQyNdo\n",
       "p9sWZ8NfNAo7fWmtnZJkFMIn0PL7BcCV4e2ISYdA3aHv7JVlerch2jaBTA7hTnyiboRELM3OtUAH\n",
       "hqyRk/FVstNT3Y5tEOAjSfkyXdIDSuTL90ObrL+6RY8t1sr9bsE0svT4xDhWHN3qTALCHyls8UXO\n",
       "tSfEp23Ogm0UOxOdYII+dwpCpusUVvXZxhYNuOES5LsI9ZBYLP7cAKY38Mb+Qv/4AGBU+ZfmY10z\n",
       "aDZgXyJDNT6RXEGLj2AFI0+I4Hohm9ICPRWI/vMw+BQ2OLOSKpWpAXF+v1Lq/SErPS7MAwAMvoii\n",
       "8zmY6Pb1Bzvfd+cAc/fdqqidMxJniqX9bLqHNuJGJx7AY5JHw0+yPy5wTapA4XW9Kea5pXk3aGhK\n",
       "UchtR14sLXwN9Q79ram6IYMNOPkFL1wNAIxCEyxWHf6BURJRaKxbyicaHqamz9JdtmQdkcbcFiFJ\n",
       "rZmy8sPbgHO1arON45i+1AGf/Rbkzf1F+AW3O0zBsJbAxACuIFvVEYGEXZJSn3UfxjA2zDUS9i9+\n",
       "fK21HMRgfTlbMaXpIxJ4AlB+Dx2X4FLdVWNIgj8TiJx4CsYZLD77A88mk4VK43cEsgmFEkDh6EBd\n",
       "cSiY+Zo19yhGHvA5LZDZQ5d/oIXU44nU1cQr15BsNwoA3NZ+KwskGxk2OaCxVwEY5HIAqW7vRKhH\n",
       "c37Pd7ldmfi3ElxfZUIzFayfj8GUSPPx3fgpdOPAIbS+FnxUHurPfZetoIFfv//IqRuKtPCWqNMx\n",
       "++V8VZcbrhdRghB6TCf5cekFMaTfPayLNWJTGb0SJAz0aarUmo5EJoa+xRHVkPsXmwDsNM9FzrQa\n",
       "1nM0LqaXLr5vydTxsC6teQQA8sq67XUgaiZcC3/x2sYJT9ZFiDhSKbILqlhYzrq7m2dT+rE/Go7c\n",
       "EiPAc3/Z2j7Yq2jX5zCGfC5LGHa6k7tImpo4oicG2QibUDS7s0fpb0IAzxgBA8vmK3zZtzv3dePW\n",
       "Q6Z3fo9hw6Gblp6aRo/fzmGNq2+LehIYSXjWYf3kWdORjHYh2YSEATtgoJ/hiO0XihGHsXnVhvvH\n",
       "AoxrUnHqP3FYEOVxWyNYLzKUESQHN3x8vGHI27+3NY+wK+9W0R6cznHhWaRa29UkSsg37brcuwB3\n",
       "BoVO1X15e28Q+RnlCjGmjj+eU0lM37OlbqoojhLwy2NA0k7Bjvuhe1Utw4g1YkTvu9WqgGUl2NPZ\n",
       "LWmhYAeACqjBt1TCt6YlU7bvecFTADIL9fcVRQm9meqWl+HgRxwYDnqVXvKpYkzyb7OlgP9opNq8\n",
       "zrwFMcM+clEwoEBQo8w5XA7qV7fcbWk0i2kwKME9Pn9Vx4lCl+/gRW1r2+LhCatuxsKPsryQsbIE\n",
       "3HDv65gjhx8vBCcMy0xqPvsqFd3GkCk80Q5EDBBhPluhkH4mxR1VGVpkWRV2uQVCRJAiHpoVo4rq\n",
       "7gkM6n9iDEiisfzl6LyP5szcPdTTrw5lbf0Grp1uNjec7vW8gXJqg6TyD+e/rq/dRPGFYUxRqo1a\n",
       "Xta/aT5rCzjOPiOy4xbKeAb68Bu6KbxYaTwf1Rsq6HAoABj38t/XwR6wdcpK2w8a9/93VlYYxxtK\n",
       "q7T8onPAwEUQwMX1aphgsrY+H3aCQCT37aHaOkWvdUrqQYyT60slq9lNh23NeBolEFOf4q6ScVGx\n",
       "ByNTyleqbJON3ZfF5QflH+6DCYa7NKPkfc8ezDo7Uk884bdOlsBTFXP0ngwxQp0mhRwvX+4ALKHI\n",
       "wj74UmrUtynMnaN4BOc5HzMvG0VWOFptunc1keOWuLD+WVXJNxOPZePM/0no6zvR4h3ortv5SFrT\n",
       "KPWPple+RD2eH40wGBOEkW7YVvOLPOf7RFpnzVjJL8UDoRrSOmSP9Uur2/K3KOGohgvqTlYj36gH\n",
       "Oob95bFEDJFwgEYKNT+IY/Zq+33+AhY+gOAjzCW5k/lG15mNpVcIUCEKSaWC4aOjTErbPzULyqy5\n",
       "/HYAmMGtig21ufBPwVBBw3hlWqwcq1uHhIcxq9XZ1xbXuRAibuvWGOM+8TOhSbFN3l+/lq20/+IK\n",
       "wQx9jahJuK1w/Ml2LobzId5euGwB7SEfKACnA6f4sGakVuevCQNeF9hlPk/32d4XuyN1H3Q6bk3h\n",
       "X00gy1DRg0tdC+eAAIpTeyimA2ld715uHx2i0h65JJUirf0jVsxJGGgjJfmdEC1QgfJUpepBYtdY\n",
       "+QqJtiW/2ozrhrRWzkrFSaI+Yt0Fvw/EUe2H2VkuM6ELKjZ3/VGVfZJVUUJ62myNm+O2u+C/OBkF\n",
       "NeBhY76oPC26zvA7OHSWbk2nA12LRTYr2osNccrY3SgTR4dqMrFIdtk1SBYXEJS/ucFIM38ag7Of\n",
       "3mmEFs7rJ5biBgq9ZKAGWN46qCsWhxuopmUJTLiVru3PTEPb/GwSaLZgRI0bQbFa2BmxTWnWtnd0\n",
       "nOoiXHNyNpgQjSk0u/+sCjo9oiPOUMroCMlh3Ct82gsy9EFUJhO25s18qVShYIwYVPxq/CC4+ukb\n",
       "K/xHY9uS6hFLk9btMoG9u6vwe2Y9wlu5fAWP+BX9u9/tqNItvFoofhla3Q/Nt7VNvkwfNmH94gmq\n",
       "ad5TpLnEWJfDNWD1/8zzJ2ex7wgILixjdLi/cLLF5wULCqPh5SKc57NnXWHgv0zGo8Cia7Wu2ViR\n",
       "Tisk2Kxmy2yDS/UG+u/6g4tVrMCO3jknbcsGtBz9mT3/bJOUUWUWwNQ+bqI7+88JEkymju5i6u8M\n",
       "EF4sgAT1GOmD0M8RJdNPH9E2JtWQDxEhVIDH2klQ+SmEKKFJRWn6iXWRtIXVrQ97roft4QYhSeWp\n",
       "y/HptDpECTkLeY49Elk0YapJJvTzvK7s6SSUvTM1DKBfb5G/FZTY7feGR/b74f9vn5Wz3r3Hgu24\n",
       "/w+l7F1sMdsVtSIkUDhRB2sJs8EUNc3t2VGzvZb73bkr98VKG7Si9h0QcB/HzLszMgnWV9XTRdBp\n",
       "qLdV5+ZtFPe41cmQXHtQs5YjAist4IUpNBDdkdGAb/MMtWdm75QvFpKtETGpNlMhArqwJo/7xY7U\n",
       "T5FncLkQ7ySHEOIol0ei77FkSSBC7jHOa4PgyU1S8peTRcKDO1LHZANmifQes5oVmTc+VZH/I1Tb\n",
       "Qu3lzV6jrGAbLAb4LRZZs8YVP5XbT5yXJyK3bHqksQgyWzpwyz7dSHOkL9A7bDF7xQPgT0JhvQtk\n",
       "LYdvcM7VNVJjnaGsxrN5q2n2R7s1YvAa10ZyC05+pyTL9qDwpcvdnzkOagB8SyWuVFjs68wkHX1N\n",
       "lIwi9XtbmxH6yKMt9J3b3WTEDjXTuqoOtM4a1TEXh0zLHrXza42TMJngNVKajbjXtY6K0qOtPk2e\n",
       "xLBEOPUo+t+jKFAkuwJUiDXjYydTs19RC/tjNrn1C4M0QXUqdigCk5bJLIgGMIn+9D72pvF9F9+P\n",
       "YqYrX/3j4SPQcdvrAffYhhvwidFfUchjB2x2Dlgm9nnDcM9GuiQUxykHQbwsqBlqfg6A35lEFKee\n",
       "4Bc6+nU7RQBGw/OvNEzZfLjqymisXBsAKwGx4mYXpwxQE4sIzkjA5MB6R98w8xMDz6Jb+RLl4U3W\n",
       "2TCQ82MP9AB+Wl49n8LVuARPXxNoX2Tlzw1gNqajniAFQhLbSNDQmMlkHKwhCCKGnWQMf1ilf+Yk\n",
       "zCrFyvySVqC00l94dFsv+EuEDz/xrv1c/0VpMbkQnLzt0PyPCCkq2au7DypOIyWeQ9vpYGCEU1zm\n",
       "yCwvQ1D9ILST63fPt4wYCcsI1av7pzogVaClt82VUydM9D/cOz0vd2UEylMfe+2WYIF8mU+j3d08\n",
       "PS/51r26TMcPjpQRq5Ctl1uhLkEm5gtFOa+DvAW2FCMHL448eVYkna1ZSP8hiqwLjKNRT72BFkEL\n",
       "FHQiCRuh36bmeRw9P5blCt3Q6CZEnPQ00b1h9F5C+9sClg0LMB7/ygbDmC7oda5icLdyC7h90III\n",
       "atmaV15lUXZQgnbWPXGXi8F5+8WQ+EV45JGsX++AHkZRyUXOT8tanNKkGfB7ZozhcvnS/JvQFSvW\n",
       "JVdvj5xIY7aYWeKiEnBKfYkCcUsgtfj4ZHqP/nH7q0980h9kmkRuKVtxGhlLP306mNCeeqV1KANT\n",
       "YQNyi3N/fUoJIyPu02+HrtKKSiEDPL6DcjOt6Hxfv2NDRYCUTeyihoywMmc1qWpFFRAIfiQt9/pj\n",
       "jydB21ZUwt08NEILS7x1amYbGMG1yo6Bz0CiGvi/edfN6ZVpyUSgdVEGMWRTJ6fiCK55pYXm/ozN\n",
       "RgWdQTKKn+OQRmlmJrRqFoout+lorbCbNZb4Uwt2s4+05DTIFrZZcegdwCSCPOVz0kjQG0ekZeZx\n",
       "sK6w5zkARanwTQfIZNR4AeBJCPFWTHr6Oa6zWrOQ10+2g9gPAtWk+ChmkqntWnCpdZDCnjng19Yb\n",
       "+7Wu9z8ns/YAJ5LUakOcmTjxUpCkG6n5+1iR/Mc/VHYUW/NPSs0/SpKciDoAVqkJdonkKoADbWPz\n",
       "tr1sa+wBR9nuXbzPPbXYeRMlMEM/iNQpgY9lo8Buz93omdQUtHHU+48s85bmbfYPeebXWZ3CXoG7\n",
       "Lr6SpaL514jnQiapc44LHF7JYPAInDezF9JURbqiH+ZkQx+ytZCRzY2eoDVjwzlShLuIczwmJ57F\n",
       "5mI/lrCSYHWBLPmX+5PoP2apGvfaoyNWWSIxh4vHdZjefyxSIOV7ybba/W8HJopN/auYy6pGH3pI\n",
       "ws1DHTBwc7cC19oUq0YmXoD9615scH9qgNfucEFxFRmRxn65HOE3ZKH5Dc7kNchwEACp+njfkwML\n",
       "GlUdgKKapt7g97CGaThzkHaJrO9f7+W3Vg1Z3kBNfoU5MvVoYrMPG3RNbdncCcXExNJm8ubKpqgQ\n",
       "vrgIuU39eb5IJ0ufYpEAfxp8wCFdT/03glDPpEcRd0dcdWLYmC1ZQ7tHRq6xwpcdaIinpbB4o6Ic\n",
       "zdDj4q8n8opsoh3G+W9hEw8NqGzaYrqm0Lqqrv2ohbg+LFQq+4+Vq+ph8TIE/5XJuyDZrqxy8x8i\n",
       "a9LgJLushQHtwPDsoaUCGATVOTTkYGz3lLPe82QwDOYagV8FTthn8aim03P8SeqP6ba2qo+O5gCB\n",
       "oXZ1WxoJYjIYlkNQJHNp+s2hiYqwVGoCgz3p4yR89kRtwPB5Jftq+QC+51uO95HvqK6AjEBOpjdo\n",
       "RFOrdLtNX91TAWYkWh9NOwqpDEhV+bfm/z7PTLPEgjzlwcWnRtukKH0rmQXB46194TdufLvQ3Zjr\n",
       "M+gQpDls2EtFJiG61PRyTG/t0t4qs4Rg85SJyHWk2qXQCX+FrFIyuXcqPrMCbwvoCYhf2xTj7AKR\n",
       "hMkjaDjupf34Qh+apJws4gUAZGP5R9RlHmqPvdZPeo+qE/bJlk3iYA232lMIBo1ZYv3EWpD4dlfX\n",
       "1VgOkuMFtSpI1+IpvtJXrIhcX0DSQAqRsTLm1jjXfpzcOwjWhqXrLoqGsLatpNPsBEwyNClfLhV0\n",
       "9uWFLSHthm3qRZ5+1LxG4bLjLIDGKKr4dL0vgYzj3CjSHB2cCCChgWR3AoPhgVs/WlfGgPt8zv+K\n",
       "GkCI5LF6N4aGaiUysM2Vs77u8zJedAUEuGvj/4clSKaAJRqd4v8qhWBw3QfGwHbEGVR0Puuq9q5S\n",
       "B/KBqzap14zNGn8TmInHjcGVdQNue4qAlb5s+v106x3hNm6fvmCglrY/tizCT04XECIUWKI6hWu6\n",
       "k69HydY3bXBO6lCfUFqfgYwgf6roHvQBYnC9/lyg8x6LbKNIABmtbz8xEeKIaJjjwGRJI7AN2far\n",
       "H6EtFk8OgY1iICdTgVxG7B5xWWnjv8OyifTKpftDUqgm1xnqsMawhfnFM9rUVPcecQkGCgjC77d4\n",
       "nqxb9JVwbgUqvde6S75LBw4S6PUz3qXP/8hcKo8E24nfKMHspnoqOHTorNywAZrQ5IVJE4AtUana\n",
       "3CWTpgGJcwUjsMjhB+zZlZyOu+Okjoz8GWM22FoyfFoNPrZRDctWM3L8Yeg3ZR6YFOeCAN6ma7ru\n",
       "DGRKy10uvU9GX2Nt/TAnRlBZ/wVy2cRtQeJh2Ea993PS3bbyOIHcPCFsx+b/IBInDDswytEJf94Q\n",
       "RZRuTmRlsfSEqZMjBgCX5Kwq3yMqUMjf3dHbz/U9DBvCIaidZAppx/6A+Etzjw6IT9DF/x9OYQh3\n",
       "cNfb5mV6JdiTqcRcBl411e3Vloi/bpQkCctj7LTaVCLO5kKCitY+hxH2vsB2GiD2RS5XyH08mVAV\n",
       "66luKQ+9j1Rgo2303OUEte7UGWPTswbYpzEgsmP2J9yWTGF9/MCjLPjVvrZmRLUcdB+xp8tI6P8v\n",
       "jczvpmMLby3y+mRzISzcMigBnXQw7M8sqhNE34uSgaeYRduso38qTlCae2F69LJAGEww7gbc7UP4\n",
       "i+mXkl/ylZr+n+7QVeRRvgpk+cszrQluPd82iWKY9U90706wGFoKkd+RotWBraWRVz73Ub9yW93o\n",
       "K1/Lc6ZhH71R2S5ssL8AYlcSgaYtOxlpUsoXOWgibYsAKqQoT/qS8IWTCljTJOxqnalOXyHnBicl\n",
       "gLB99s33P+OB/f1qy/ofVg18g9Uid0tz8c2uRDeMQec4erL1VtR+mhzX59iD9lQBN178SQ9oZBm0\n",
       "3adSpuua20HwUcO55T5Tcajs3jwZXzH4wFn4IyB5wVSRyFANtw6zlqpMmXohdfV3h9SeMWxxTyz+\n",
       "Hk7hyEDhh5C6Utif0LzzH6JAauC7smqrxx2Bx4vPmLZPjygDfz1UBf1hiGmwrut05LZEv8LM/fd+\n",
       "V/MozvvPma8ADwLjyXn3Sw9FcI3UeIWZjutwAtF9H6lpQgJpjEorFtcauxXsj3xP6YeOFrZGwHca\n",
       "wm22uPfI58JqO+GKYnR1gXA43qmm7RWubYX5/3q64li5ZgX85zvD8hdLU9FQECN3E71nVdUkq/Uy\n",
       "YAKZKJi3N72YOhwuu8T1WwMQj8CCGGPQmRcKF3QzyFt0rPAOs+cJkjqQSIQpTDwPrh4q01vCbrsk\n",
       "2DzYVhqF9P4D+a/mXtLOJ00VpEsV7mK0KiI7uZ4SyzUbtEdZ/I4cdtdGg/sq0Jn/gNG2sQL9MslG\n",
       "sbd70F/DxyyyJBCSTR0FYFBGexWyRs15Jd7sBsJFCQrMgaHzEGDXTWlxy6qTBD9pErR5UxJfA5C2\n",
       "9VJy+Rk8oP6O7eTtXFwWebA8NwjnVHmGDC6YvZ/V+Zo1y7EPM0Sif4r4Ik4COT1Zq4gAayCTb7sc\n",
       "9OaLHxpFoFO5hsCIeFuIsyZ56tng+y7D6wynadMQRaEUxQ4atUlQF+KIC90uX4PdlvyvS99uloG8\n",
       "rdjkXG1pobKlMQu9vubW7ZaY6bR/y536VFnBAiiV85ZEH0HQoyzQoArHzJXdRjLFga+4Atz0we2e\n",
       "RxlG1i7N1DVNzwxXeBgyQbJ9LC34C+Kmmi/nlm2IdbOp/PpTIUA6/5v+hOZciABbIjWYK8l81S4R\n",
       "graLSEJzi3u0mE8iQlV2HzW3zmbfMiShmU26NZ77C7xyqlnie6DObrTZcxQX2/sv1GpNy+BDTCI+\n",
       "Fzbtzs16NnM9ASVyU+4TWlLQyBIb/Uh7X8foJaA2ZnrlCQ//vRczoKssBM8x2vs/0bUNeK4RYvpb\n",
       "PlBBq8YVJZVL3V+qT9kZEqsBJpw1pm4B/rzwiqfI1VRCynbz8EG5iTlRyxdfqs2uklCGVAAo41VG\n",
       "SwaQSeG2Nb0paebHD08hnX5nAYbvbV5kFP3MtPCXipCPIpPzoIaTtYBIYnWI5gctR4a8YnfsCdRs\n",
       "sozdH0OqDJ9lmG+B/EwdF43weX2eOtMwtWwsj83ECSmdhhvdtw+F7eUI7bb9deIFTGu4yx0MHuK7\n",
       "2JNf9X2ZJe9EFkaZYKEkxVgJKY9BymhBUznQhzfwkn8bJRK4bxRtpd6en2MeTtG7wplki4k2tZPy\n",
       "4uKwSwt2Sq6+F0LFPD/EeUdHZBoC7p2YRuJMh932Db0k0Ww2V4CsVrUXcn++8gPZS3/E0l36BEN8\n",
       "DXuUhD32zH9Xmj9UKwbwGcM8N5oIvde4uJ0dLdTk2BCHlb8k0PXr1iejNxT/73zho9qNVkzl6+Mp\n",
       "jwrYeEO/mSV1xg+dPA6qFRWAcdGxqvmf5QMV9nws/1dJesoGVgdO8c6RLsHcV0lzz/p4ICoHiKvg\n",
       "vA21FLhlgcJ2UDxC4SnYSALfXi6W5T5FHmZSxj2o1jubp+1Zm19IMtD5nfYrYFMTu6M/HSKN14OA\n",
       "oen49lF3vnCbFHReUCG88Gsq9htlN59VPHdi0iEc85z7jJHrPQHdd/JBMkaD2UQBw3hHCH1DH/o+\n",
       "J5u9Q9B+Yc2vVI+nREacyG3BYEL6nNoLvY7gPhnJkFFdtf7j2ZAJB2zIbD2yItc0/mUW1V113csz\n",
       "1kiM9l4ojrIGnwKiXLyhcw/LNBv7V4IgHC5rk4pI2MlfbFWP0EAEpaso6aEwKoR5TAnGGkC5uxrU\n",
       "9DhSXEHeRetcCv5N8tCcB7+cfxCjH/E36h6HK5Kg9tppYxTw+lKrm7YyMhtOPAnpjJmQZcTf+MRB\n",
       "fTR31e/bu4uhWhh4zdH8Ip4G7VBKW6o8mB9Fp2v7heO045AU0LaWFRUN47v2l5UBazYFVbUeVWlu\n",
       "pM1SHh718w8tpgJaunXzpI9wsiPziqBp/oydU3wCMz8tgXx4w5ipNw/ekuZeIg9+X33k3Xj/5FA9\n",
       "qAyq3Ik255l0mQwkfKICJLeawqiMVpT1Li6BB3cPKnBmpiM616OKsNo7VfspoRBvm1TY8gwU8ErZ\n",
       "uwPK9c2ndfzR3UHe8nb6VH6cefAF+lotvNzgxTOuppv2ljITk83svFD+YTlud4ZYOTRSrQHoLko3\n",
       "sxhFkQu9aVdOanUNOxBOoyjEeh2F+DUzqnubR0xcYePZo/0N94PA4Ktfy42A4vg1+QU4OHxhEqSu\n",
       "IvSv/nBnuv44z9PIf006Q6MWNOcmnUTA50wnMuWV5sk6BG7rcfeJC0QgPy3J5o/0EKnIXBw8KUr0\n",
       "txwugSHuSl6RuxTqvtEN4+X2k9krELbeOghVi1dGs2fgj2eloi2ffRF/+a9ex8utfKGORO+VUfJg\n",
       "NHGytyRcnab4juH9v2YhLqxpT88z+DUHtH1xPAxV/AHvLaNP3UDtKMY22wtPyQNrX/1ZYoBmIZ4A\n",
       "fn7knlHWWC+bo58M5OpzrymiPMYyWXgbponn9txuqYSt7RDRUjoKUToIhYXxQNEEelL97jII6Bmp\n",
       "B4gdJVd41FHXgK26h/ABf1WbduB0KQ3bY0OYbbDhyNyEew0PDjjiMd2inQknN2fRyO+eQCxVY7Kt\n",
       "UlVhKDTnlHf1GsifTw+BETOrAzpc+xn51uziiJEfzQMTWd0M+1CicChnrusTef7a7Xc0C1jJGCal\n",
       "nppW6oEfjd7gwx8ehG8KCq1HJjwLTHkqyzA+TdZD9KErl+fIsIqtZUBB1AXd6JDoUQxRA3UT47l+\n",
       "JB8rkzZYT7pEdlR+MvNOlQ7TiZbI1WWKegTZLU7Y7CR+6UaYqyVkg2NlnWvrYpTry8KVAIMEccCi\n",
       "VsXIEhRUJEzo8ltshFmR/Z7aQP0uYmjoRn+pGFLXbY62qI+e9OWmUTGflRVPLUNtqPnr75kwt5cO\n",
       "eIOr2rt/delCJt/sYvYb5duxa1bRc52dXWb/JltuvTLN+D6Jfp2GemQre5SwbqLwx+S2by2GVLSt\n",
       "VAY1NbNVEQ3nrp7ktkxwSUhPcMVKowocJs2tvrMg00tYHta4PR9VlynxJMSqRjF3KPtdKxPVDr3i\n",
       "dDs4eEpit1TYtFArfKSp94WGsF2zPYlSQAA0CzCyNUQTrNc4iSp+aFQycmVQmB3dJpIrp/A4wpxD\n",
       "OTAc7p5zvHVyrau+oMyAHWsrCxOOFR+MueCDXZ6GIxf6rna0A31KV+y41wxPDTTfpZT/r+zHjqbg\n",
       "5CnaJdfRsKcHd5Kkt8Ad+1Qx7mgKNu29tx4/tKpX0eSIuBJzIeZ8CClTK4aYqvvsiXzbywZX0fsI\n",
       "Tqnxyv2nNlMQo4QtUNXGLFoT78NTp9BCyMc4fonJXtwoI97YlvM1sG8/QhB9VMRulck4lrM3kDLs\n",
       "4ku+L8ZjCLm7u3S8Np1rvvX7G3uugYdoargJuc38DN6piXdPgFKFn2sMeGVGUMDjwJ3OlFh7ugTq\n",
       "wVZKVAqNkz5Nx0cpIiYHx2dJbajthwZt8h64a1ETww4BCeie4ow/U/xk8UvPVJrLI0dGkwaFObm7\n",
       "kdXr69mXkA0Dsq2jLh9RfLwggqVA5dVLqk/xUzV2LGwdG6IZBhEcGfTuAZS+oeR+swHX8G6Su3nU\n",
       "Z00sN0zFr/0kGbTxkJ15hyqAkBC+Nh8zCrZXsKx+hwfvHBAy1bLJjEy8cpC9qggNXKT24XAK246P\n",
       "p9LQz+hYN45FQx4Sbw/fcjseRyYe4Osi3AwsDDZXT4EkFh6ubWhZ1wIy7UKksfgWlCfbtN88056w\n",
       "BaUdiXUozHdCfMyXAOh7HLT5cKTe3G0M4A56VDruHos6vFJjPoZdS08V1zB0h6tC4vduW2EgLb0H\n",
       "dg6qgNmbZQI4OWFPboMvZSLzGS/DlpRjuTIBRThUNGCUYLZPXMCKz/ZbZTmSXSo0siDn20RXxxwL\n",
       "Mq6pTcwHukDcya/6rQGColpo+sBFQLKt/MIoM8DLxPk6EpeDD0304k/1jgyY5GBPVaKpnlihk1Xa\n",
       "Rcj/UuFh77vmRI3XqVi0tfV8YRkBI3i/MylGcDEnQvf/MuIboubcMes7uEc/8YSEXUPT7RPQOBLA\n",
       "LP25Z3dnByvQTJzZs+En0kBh1BKyWIN0WVQ2BktIQ/oqdUcUc4+h/zxdfwaweJRg8iPJV1SsD33o\n",
       "d72fuP+B0qALd6dyiTczq8jJp4s2c/OuBL/Ko5/x65EhK96BWFOfhqYVEl5WACgVyRPg0eIOg5ml\n",
       "atWXZnWj5hbrvotexn5vXBGhpzdXnpM6itHspak3oNnvoYjuoKZiIJpA9jEdT3eyS0W8Pxci//RZ\n",
       "CxEDDrH69Q+sgMrl0loUDlBf2AzLTe/cqxRXbytF9I5wskxDUad7VnG8nM0YoRWj3hGeVZWmAO5k\n",
       "OVwmiosllKkjXYG5KpFS+/4AHc4GuxhDRS2vHVo0GDiMM+7YoYmi4UrdwlMmljH+OTr1X4I7xvnL\n",
       "7NiDq8cDps1q1WXompMTQAWx8CrUy98s9tNmvSUFPFShZ4/GOOilqx9wgdcqlckQGwOi5FfsL9A1\n",
       "5zsVas4WIqpMSLBTGYEstdvbLLqIyFV7qPp0wFbIWVInQe8U83ZGU/xiSoQTvhUfPgAYin+hMcON\n",
       "n4Lt94+WYl2fJr87atEvCFfD6qTZ59rluXmzToT27RYFpGk8IXDRqGVRhZr95XRcLFg9CxiZqqR9\n",
       "AGpw9w5JBKO2VEDxlZrg8+z+fwpr6I43eGYwWXWfEdfe86s2G5rAOPggrYsmuHzBahek+Loj2jo9\n",
       "v2JFnY2HA8mH1oAAV3at4XwOoSqkVjxRP2Xpathtgo4VF0GPjJmvu6V0LhKB67ZWK19tz7X/sVn1\n",
       "dQlNC5jZ1rpuRZzw0k05J5oW6+iS8wYzd3ZYxJRUjiE09ry1m1mbLodut3ZIAjtgYwUJDq96Bzfq\n",
       "EU2ng/NsXn4wQuxE+VyOXN3Z9djCD4e0vl8Mc7FePj1/CLPfvBNDmcRrBCRZwQ8Sb5IyNQgBqq8l\n",
       "q4RxWZZN4I5pWT6xMOeWcakUV4VOS7JH7rfaRxDXjJCfn2FXaNMXSlLCFkxBD5NC26v+RFBxLs7U\n",
       "gnd96Qo641ajC+lBoWvQaRqti2MqT8drqCDsn0ixKR4lbB4oDDe/dVgxLJzLptarzMkOxMET1uPo\n",
       "XdliaKce55zcNP0UQ+yHjqD0fAv3kGD0payFoAQ8EnCfzU6vGt+8T7bwx9gv65A5zzIAj3/OQniR\n",
       "aX7HWvs+0ui18qrhVfz0awMDyfR4XU+LivbD7udJF0TK4nB8so1m3GyUhIvZ6sL7/fzEVZ1qtmPP\n",
       "wVdRxDi7sZ9S6jKosMBOnpD5wbEsjlvizChZ+7IgdpzwbZAYdDlw9SDVaulltwPyhIEeILOu5yXG\n",
       "9PqpAF8zp4ubSNDf2aC6NnsXOX0lBAMF+CSro0H6pXw0vgM5X9id++Z5GL+EUQ0eFsa4zE2L0DTT\n",
       "x8ILmdwJQe/bptcxpZK+lrme/JPTMtDj/d+qeBxGbXylb6AdJLGIp4E2c/5o6NJDzFtS+lE3jj/H\n",
       "X5JmfW8TGjs9w9lRmKcm9Sq6N3VnVD4JRG1ob6S1ePA9rGlJoi8OhD12xlH75iPNEa50dprbJOpL\n",
       "BP0cKJ5JtSno/cQZLxcqtkF924yzIfRxh0v2k8fJE0H+Kd1jBwQK5GY5DY4t2fLPLjq3YxtzMvmD\n",
       "yWVzcKt2ZoyipgiBTeruM5iNrTLnplCwcPG0c//NQRKuGDv9xadBaK4Ri76m5gWvitbE/t6wbaVX\n",
       "m5HSeA35wzG/CT9Ufbkayo7gvQ6Fj7LbKkZavyROBA1R4BFttF6qyFePLuLzNJlWUR+sCg2swtgW\n",
       "JKbyGrvdeRAx250mVfu5OkjZGEhOGo89T3JyUA3tjzQ+MGK4Hk3sn6CQHdblYADPgAAAL3ZBmmdJ\n",
       "4Q8mUwII//61KoAAOpxxjmAAITvBBBEyWeqcCPjAe90zs1oSxh2Kv2wfz86DhOYZ9R/A2ad3YOzg\n",
       "Y7K4ertN42ffhzQSXDTh3H8ImOSmtzRgADpJkbDWz/vwfUV1+dP6Oiio+qR9VFfxrETesjecGWCs\n",
       "UVhKiBoS0LbSQgVa4XSvoRcTjeobmfUGGkCec/wKKqJxiUavsT3JjnSBFJL4tbhsGYN/FSGz/k+A\n",
       "KIM3hOIYxncT+11VHcLILb7me0+0p94vLlk+V+0lMZ0L4gAjfv5WMKjwTSGY27ZWDkn7ij1AXC+G\n",
       "/moTFOUfr7E7we2l7kW9+FuGM/h8fC99Ajp5BUS4Wdog/qevVFGsV1l6lk3fMu/LMZdiCULmSZVQ\n",
       "pUmnpucJnxizEwbM+73uF3KXM2gz15ftgtmJt8HIxWHOR4nx57hneVZzex0AcvfIcibSpp4fY47c\n",
       "4+atJQdMEK/+z4FFzPSC6Fzk/TKsQnBvY4RNmw9JQNqVnxkZdol8U+P7kIksIjvRdGUIyIqpZcyK\n",
       "lAYgijJ1UBo2RGqCoLSKLZZ707SEkfe0lc0tQ4jO/ntbTCeXRB/ptE+xCOrYR4Nd276XDqNgPg03\n",
       "VYc8+TPGsNaEhAI71/qb+yYOFjnl2JmBO2t1v4z5/4RZ9piUn9ndGgDc/4YH+uVeGyf3zso2snSG\n",
       "VYKIkkan6gzBeArYOrfSxNn4EwOdqmQRYZ5ZnzCINQdXqQPYhxa4pRR6ZLJgW4RabSqX8RuR8+I5\n",
       "JsT3aYiiXs9NxaSR/1gZDuEtwmmH4g6xLg+LFE/mU15lWTbvUg8w3aWaX0wfpYThypEL1S8nV3Rj\n",
       "aRqytscG+RfgI5Tgp0DwaKAkFpVI+7bM+RmflRZMz/P8zFW8ut6BTO2xcoMcmw6xK7y6tCRemidd\n",
       "v23fyhHDhkKessElfWmDmVmH5wNrzWM3ZnRaiptInzk0dTDDwAlZFgxC4VBPXGRsbvT49QeanFPU\n",
       "dcRLfiSuWGv3TZHBqHsxFnAlmoZpG0LKynhbR3dLSyWj9TKfpgpYeyLWUmqL+UqbvtWbJmiq+3v8\n",
       "QYqL9NpSb3Pht6BZGM/aOfaMqc7yXDSW96gPn9SRc9CBChbS+cNqV0a8HS1cYBDF4Tb7BO+arIiT\n",
       "s7Zw31zdwdFjR8I2BZakgaD73Mg6qVzECIcFVguJ/P+sQf2TbLc+GxFLi1nsxSXDmB6q7m4LYC/f\n",
       "3iWN+JjcrtBw/qkg8GdFoS6s68zrp3rjJJbOJNX5IF+VNqmJ9JB8Cz1Z+xuEswZmSvSVuyzR+hkO\n",
       "kZic9W3L8WAuLk0Dx6YkB85Kd5+el2QUoTtkDedXxBVeBX4FxRh7KegB6KAZKByz2XHHKwGVBwIm\n",
       "X2tXhFGePc92GjT5e+CtAAhMrUeC2Qov56X5T4M5rnKKjE/fSkFY1kY4c3NGP/TM6UJYnnaQjVsD\n",
       "67EFJRKP4D0Xbfk84vWJQ9KbO9+wDIPR/PdnjmG4vD9+aNkjD1XXjanI3HX2/mrD+JigHPcQn7XF\n",
       "ce7xrYotDETps6ppEbAAe749nsZ3cUF1SJ5Wn0n1G4kdqKGKNs/EsF2F0eLiE+S2fz4ddbSWgrGS\n",
       "NqtDYQFFuK14VW6zNrnS65W8QpnhwHUrefwD68MFRC6Aq/VJ/WSzNefywXP8xkFzPkhCY9lfUOAH\n",
       "xm+FxPSPHCPA54EZsBTcNfse1yDj2TeFpLq9u+c4e+z9x209vR4wwZJTYFacBbSzOpdLPzLE3OjI\n",
       "nKGTBedCAGBmpvionN+kAX4q0r9sQaP8s6GiglISoc9dMDYmYEvmDsVvYdAjMzRpd73Fyg+kGfYL\n",
       "XRKIpwZxavLhs7GZgUTvB21afp+68VlUDWxnNvkwfu1DYKQhEUfXu+Sdya+2FD0NZEgsQ8tk6dKO\n",
       "p0O2Mg4UTs2wqM/31QUoDCR0y4C4yYNTtBIq/cC7XhB4pKHgaDEoLlSyTQ81vrvdPPb4ypSblwqw\n",
       "OL5Th8HfiFJP4Rc13NoLeJwiH7xloSHs4kQphpWvLMLdZUHi6I1fEmvMFDxl8RRE3Bpa7T4K1HFX\n",
       "xMdRQASB49mqDtY1gK130ZEk19HarXsgU7Uu/Su8Jv8fL3xaeSBWIhkMeGtf3EF9ooHLgEBTYp+C\n",
       "uSrKUgjXU1Sv7mb2B9run1713DPlAGnY4u2XjlxSRNr/8i9ahwsnvuzcgZu6nGva4IFqT/EVZDo4\n",
       "BS9h+q3nopFyqEK05RMokNAN4xBLzQfNRGEjTCQsivX4fHSkZux3X1Xe4O75Xf/LSbtKCmVGQt3i\n",
       "RZojcFQYBDr1SqX+Brd1k5VZs9L9uFYPMMyCWDmrkHxj0Jy9oNPetCPe2fxOj+co1VBIHqmcZISW\n",
       "AoWkKaRp/axgwwNbRcnBeHPDE0DbljPK/aA0x7pZR5twMqVVZp3VMzH8LI7n9LsH634RRz3ndQBC\n",
       "pfFaGojDFu1nmw+CWomOf5gaxG6/9ug+Lhty0BXyTz/SEtus1PecTfKqgi837G304ekan1CAMJNk\n",
       "3V0wW4W4kr3GHOHZpqf6AKMF+z+KuPYoTsxbz0Szrd6C/dMN9fp/EmhtH13XUFANh4PTbIy8mG9k\n",
       "D3E0wIwHMXgvKeon35mVQ5oR2DBbTb0HGY6ZMVGDr99xKPQqIz1LfM/UUWrcRmTD7+BlpCJcbKvQ\n",
       "Tet/5MqKDQ5QIVVT6wT6cpePHbPIoH/dVxPusmv4ZqS+956FvysB1RiBQ6zsUf26wrqALJAQLCF/\n",
       "3UhdZ+r1zsN/C3ejpTMe5YQIrmylpV4CBjGDVNRGOuqrU3z+xk02/0YaVa2/weh3Vw10ofEweRqU\n",
       "kkEqaj/lhFb2cjF012wGhVMS5a9YAa5C4X/UPHU4sppZUnOoLrnQRojIYMysy4WAF7/Tq2UgLIdY\n",
       "wVuvBgeUyUWmFg+RaCQXZMdQdsDSLTtapLOWV/V+00FwgzOpEubkvvHbcRdPickpZlliggA/+N1e\n",
       "G336oOm1Ie6UUFZLi4FgO3IoGrkY/HJI3Q0jqv+vI/AO61nmhbC4qlpCqR9IPQGHEFxF/8zuqJh+\n",
       "RUSynf6kOAR+bF4mAMjOHs/hu3P+lGOXbGpGEMhSiAr7KmIckRf9g7HN9OjR3uZt3DJooouqFEMe\n",
       "Nt4C1oWJaw4TbYrLFRjErkLap1x2CBOXqH9cEzuv24odMVNAXT6lB+FOnqD30iQuoAHcA6i0iHWb\n",
       "pboeWM+2BlwAyVgBUH1RhNof1GJc8k4g+mZ38oDznfOCMlRF0Q6SonPj/AhbSyQOHwQveQDnDbW2\n",
       "gvo+U2Q/NCOdr60l5/prK+owBfiWt0HVb44X3FWRLQAG2dXJ/vk3w27vcUtGcKzQom2nza9mpK8C\n",
       "2/A3wQ9LnW6QFc3uixuHxdXaInsfLPXUzvTr6uK/tzlSbL8sMyb+DaGIYvJkXORFUl3S1DnMV9Ma\n",
       "PZPQ57LkZ0heoMzMqYTvK12Q+95Fl1bYRDPHVbgZw+nlaj9iNPVccocbBpmdodWqRc2ZhY5zsUUU\n",
       "i9QYloQvS6m3FJjmt6O5/o0nmJQx6R38kXSIo3UBV3JBqAe630iu0rCW4rMVsBHj5obI4Vq4wtPo\n",
       "zFazwesSVhjwj9EhVHsNe8HJflqy+8L7jqnzfHWL5aCwvhzcnkhr4KVRldgCxk6Se1ERmy1nNqNH\n",
       "jsYH1n90aivJNQjmL7xrWoPoQHOfWN1F/Qx6lLOBhym6TesGzMPbCvtzXEQ7izf6uRItU7yfltMf\n",
       "UhVlBQ8/7p8bsfkqAfT+pZJ2oTPM1WoVSZu63RVTjaddtmfbVqRwCIhYjDRmfLwQ0Ewke4kmNof5\n",
       "0MCCUJ8Wc1G32PpPo2e6a1CfDz9ckPJ4XdIm0j/HLAI3pCsOGCeshP7GCqY6CnTEoas/wnHzxKsY\n",
       "Cd/A/nMiXSTBh88BTN5zwFQJ+kpZt9EovVEfrrKOdAFSxYc6O7D5769LkVMnt5LpXRnasjK6Ubl4\n",
       "70d6UJDFtMV3mMoTaiGxebvWeVAh+KY78oWOahj6g9GetrWbEO4wmNffaxZN/rondhdqhLPagFkf\n",
       "RRxBH75L33Sgkp+uU//z7Kb782YplMP/XZ28qh3SZY4CkkkV6v7bFxHzLvPIwciybrSOKVpr7dr/\n",
       "DA1pC/jEK1laAX/xamTFKfeRKPTMbXCP4wM69CKewYHWXBlTBuqGEw5WKGGy+st25/YX8O5Hvd0j\n",
       "+EvJZJldwyP6+ODsVPWizkM4KURCkEzR4d6m2pZfTn9lp1xqz0hnh3wKIsi8avI3cNk6hVpxyvZ6\n",
       "U0O6XjQlXyqLeqf5+8LYQm/FEBrBJcMraTSQpjJ0cMsMwFIcKdBTyDQldpz0/cEGPTiINz3iQ59U\n",
       "AuUYu14HbkB+3lgq0iFyzZzgIKhSpr/gpiUaQdIK9UB+MHLeZ7QkqjrVrLUKvRDwzUwirWGQyCDv\n",
       "T3XR7KXCrgUmmLM8mq0n/hRXIwY5SiWXLsBlGUH6UFpIu2sGpDdzLyuxtEitlO4Z2ia+EC2w+IpY\n",
       "MNmdV0T3E+LZG74bjHTaTX6TFpXknp07ZICg3s35ShZT9JC8w0bvdr1jbnNZmRQr0bJ7amm9plRV\n",
       "GNN5ENSDfbkJKUA3uZLM0+Z9AQjlQbZEc6FakUA6n++w8NIfDFECEkfQTSIeqjdijs82VwuWm/MC\n",
       "Mntht9RbufPVeHo0Ujbc1Kp/XXM0aBO6jqcUQ5Nma2fHZT5zC+e6zzBorGWLe8FFyEvlk8oWa+4B\n",
       "b5hpZDZXYAIhKBUu6glVlHMCokxT7EKqrKyhkUMKCx5vedzMouIVIPUWHDJF8QAblNdcC4DodcEi\n",
       "5TvQC2CUtrcbk9Q5R+ZsW5doSENTT9wyisrn5vqQwusLm3w0MfxLtbFTqfONJIhxacOOb2Jhbqlp\n",
       "j6wVo27hgeY5aM3AP+CVWQzQjjWdss4xOn7RMlVkjjjsiokUcZYXgCKKTTQy98dz/m0aRATz0LoN\n",
       "jl5epJGXUS51Z1HT7axfCdypMsQGtEEgB3YMtYoTkj46NTqHPkyaJ6k3W1S+dtsg6QPia4oaWbQ3\n",
       "WnFT3PC8SK+ylYphcx7sghHjItiHUoj67kMkUBL3u+f0coCatIlE3xuoEB4H6IBr5AfOEpKDleN7\n",
       "0wlR+4xJymkUJkBnIOiRsPn82A8qL4vJehHOaDp2fhAARxqONRR/9oRQEzuQcBssl/7plIPpAcCK\n",
       "yOBr+1FG3QdSLhvSoSMWd5yuustMxDi3o4+1jOPHtQEvTAvJDzK40vO14GPMrPiHImMOJf2j+BWR\n",
       "wGYUm2aGNzxZ88zN/ZRA+g4M4lVPy+BFag8zmR7r6zJhFlPJdc3BM7jv+Qxk5xDgFSGgLKrTiijl\n",
       "hAVawoBXV6wrnH23nOwauPDiVbKRpdhNznFk3yIb0jdYTGDxWx92xlgIYxjwfFPgJ2+5E6sKQjfT\n",
       "CCeai9dnu3IaWm7oGsMRRdmPSp3jE8GquOG8IH7XP4UV4WfqshSqjuWttUzRNrwQwayz5NA/l7F7\n",
       "ZP/jNdvu+9HkswiChNp5RyhWdUR1Hr7+od4HiyQZ03TvEdtlpA2zi755ktRO7tO7sAf2gK7jlHnx\n",
       "LbpwbtZmZRZ9JNT47l5BjboQab/Eybva4BjHa5cmDOj+2C1ahHFVzkYFwSg7+mYQ7ysm5Z2sj66I\n",
       "ufGg5BwyXipEo1xL/R5lzig4drjmcBljGow5xKPBkVjCs4wbg6o68zCReEsQx5nRefkvDBTnO8o4\n",
       "YQnifdqL6/n8meXzFUrrg+vLFiM5DpKD7LIvpZMwRY7mEO61D+tiV0uu9MYvVInY7L+yEbYChWXb\n",
       "snXIxirTGdDCUds1cpC7Tyminy2MqoKZs5M52rtxWVTxyS5qEZPKPuGp7GWOfnJAXZZnQDrTVIwh\n",
       "iIUV0Huqg+jbvJJmFUeEqjcB13yRSuOsY7huUmabc5k/6LAlw9TX0EYfcj97EitPrV9nDgVP3bvG\n",
       "gUS6R2iUAUvEBKsK0NGKuyxHizK+tHrYDz6dAYh+jPtHYYFStpaAZfdQgRT9KuJ7I3VMQQw7d8Mp\n",
       "fuurj4jMrOzo9DyCqoVvRWt+HW2s5dXyx/lZjfzTL7//HtLch7Sb4omT+sarGUoz5YwzvjA1pZpH\n",
       "psrL0ByCX8szOqCAlIjHQVJhqOuq5VJiDubdwba6I5T9T297sEENYVJ4w2C283iBqaz2sXpbkkUY\n",
       "6uNhoh09giNHFR+F1fAC8tuJVxFY85IiWLlHW65w030P7csbnnQ2WRA42gwp6/bjDTL/9OpYlCiD\n",
       "oFUaps8xlsqTokZ75BwdX7dGPuP7HCxVx55IP1pdyk+S0HpHjep3ngCtvDtdC4CV4DlvkYrSjAWi\n",
       "HpvPFqKmrsuLVBDiQBjLBTW+6PTnx7RZZfnZ0iNiwkeXzC9K7rv6k6Hl71EPrDfN2iyE33MDWcac\n",
       "Bbhtl33yO5ZUb5z3vhESunoC+AoAP1cuoamW9iXHZy/IbpTU1bjjn0ktJJ0ypd6fbsPyly4QoHIK\n",
       "9ocxh3rHqsUfM33pD3/ibTakDBAHr/JFKBYO7PyRJgBo3mb8a+setWMfbZ2/XpPu9kCLB3iU5cq7\n",
       "+40MR7j8/RL4W0JxIPxpPkzHjW8OGV3k5t+EHiuXWyby9vry8rjRiGyg4epoJih40uNakevjHikf\n",
       "KXHSEYt1l3djWY8KKRQLZ+ZTbZRaqd6TRt4WDBH6mJdSbfm756pL2A482HvLk8Z43icshElA5A2f\n",
       "JMSXECD7BsGmlec2UOQGPQuddQU8RMnTt+twEl7OUR2sD09U+Q2bM8TxS9L4d1o0AlTf5gblAV7A\n",
       "OFs+PYj+rVrWCZqVjr7FP4Ykp6MJLmIXYYDUdPYS3eLxIUOcBkYHWZoBpVTXUtEvddhaTnT1FWOm\n",
       "Gqnng5fqw49WnXRvC7H0o+W7u8klPOOIdhIQ38vhZQCZ9a1nPYvqOv3zEeUMQmQOfQTOnjaEe974\n",
       "Po/m2zeG+yL+UBsoEhbvN983QhfjQ4A+Kj4b5mTS8alHPPfVyrtWQ+D3WsTe2ZdnmPi/bTnHtVfc\n",
       "94Cr71YDcF2HgbHpIHsKCrVVZQUu5hL0fQvRbiIWiv5kb2pNvOGO8FelTCnMJpJhIPwKVMsp6Q71\n",
       "Y1B14u7EEdiUlekrZD+CClN1bADCgZvVJtOR5hfA1EdQiYyDwiAo+VZ5Ap1/+m3mfOHFrbhUh4jm\n",
       "OqEZ+xBXibndERBT1ndxGNeTGWCoz3yN+x0UY5uatMvsNWMA3R4RTO0s5QF1YKoSknBNBWMrX9fU\n",
       "U13vtqq6CDB/hUB/Murp4Zq8P+96QSuLGBLvioJ5MNveV7Xjx4iWH53lGKwtGle4EBGTQWufjogy\n",
       "e7WjFSE3+94xppCgn7WnGKZBoo/fg8gIHWpOW/ng5hfFiP2ZtSp/9rdxpvOFI6OW7dxpaNruhcam\n",
       "k3Qff5jBPsVjlVxfYFwygsz6HFjs61obPzeHUVE44NaSeDafpXyNfm4rZ7JZ1dr2Fk/BydzuemwI\n",
       "GYnGE3k7z/KEM373e7l8caJliAfnbJEVrItlnFDxthJ99/TDbiGX2mnNzO11E+hHbJZbHxcU2PpU\n",
       "0hvYj+11E6pLFhqex37eQ0s4bX0mUUg7eZdTS87/gysqXH2i38mN8uiFtGa6OAmU4EnEoCUC74hN\n",
       "8SwoKkJnvMazrSe70JubZn6G+v/p5Qk9oP7Q5bHZ5HB0yk7YyM0cqboTlk0aPXcUoSMW5vHUHxKc\n",
       "HKOVMw3qWIyUC64ua0Y+1MR7Cd7FHjmbjSl5ggKxWSqohjIUTfhs1fwsZKqeFdlSlrixxC+wvk/8\n",
       "6G8W0hP0XNYA1ErdlTKy1oC1GdeN7Beso6bh2zkmKblU2cFs87c7VWAc4Pkr5TSdlrN3eCZMbXZC\n",
       "9jrO+AEgzNHGdm/C56tg/+ur0WfQCeLYuYjrr8FeauZeK4XJkqkPrsu0enGjQ+OnoTcsQlkZsq4r\n",
       "CSZg2bjzIWNUd1l6Q25BmlFvgd9aX9ku1vfHFtNK76YUTFAwtWEgs4cD43xqhZjOxzWZtLK8vUe4\n",
       "eK+6H8VywG4qKxyNrCzrZPmalYELj9BbSr7639FRMFedNeufyogNStqbnXzvnPjLxYvuysPd14BV\n",
       "HnJFVcPEJ35ERHqguKjMdMiIJPtm8RWQsNlgGvI4pVggSQCABACuFyq6qY3yzSw1cWyCyi8n4oJ4\n",
       "a089sZNzV1jtqv+8PA0tN+6/D/hEa1GjE9VsSaWf8eRtGqBwouO3CkVIZgsnXqZufwRl4iUp5VVc\n",
       "7brZx3uq/nHzbzIG+K9iAcq3AlqONG0R366hJnOuQ0VH8KctltpaQW+bZy7wXfLlbrkGuBYQTPwi\n",
       "9iX463Dlxh68JJdlnkZzYnfvFlMNOC65rWYrPhVR91XMZjiPgO5TTzT8v2e3xIIV7b+Opw5Q98oT\n",
       "eh2DFwZGr1fHx2ScSr3gpdiICLk3kSHD92u2eGkdM5XSye/4vZ8NQUbkZv2FCYsKkZxD5jdg0EVP\n",
       "I5CvDxaiD8DMm9nlRM2BquUh5zaaSNe0xeHe3cHcebveMwIVWvP15TYorJ0nzNJOZArcxA3apqtJ\n",
       "DSFeHbwmb6t0Qtu1dosY+EPTeMTd7KyzafCfX295/JbGIAkSG0PSUEPXqEgEkOuVHgxaWrlhOo8Q\n",
       "VIiK+pTsOjyMiK0NPFC5mrKteWfJH5yiB3x3VTk+3qgylQBPzKMC62NTTz3c2o8kW53i8FpXQLdR\n",
       "vc0M8MMCGYEQ2P6AzPZjTVde/4s2a61kivsfXhn6sI6aHoJFmMSSfHvIzJPXjL3qf8DzGwSbb5Ew\n",
       "VUGAobmh5CSToVgDKBqM9PKkNCwrZL68cUE1UJo+pFxSnLyYn1KnTh7eApbGlsNdbICBs8l+iJsY\n",
       "f/gyFhVTZ8vSPeHkUnvYHH+ccbaq97gVKzb5wzghyoDoLuoWpvbTss4OP7kWp4u4x43JQIkVafgq\n",
       "7b0dZ9kjVax6qjQEE0ZNiMDGS2Nv+JjS2lfniHRVd3lBVXFhVF/cEjTaxZyr4GtXQEe5m3bjBQgd\n",
       "BmNzNyoymEtHWINHZvMt2RB+s/CMS5O7yKCLUwrXDpc/3rQcXQvPcpyV8WhCBKvwRwWDchXgquro\n",
       "8s17f7O609ymL0cKAWvYGHkAWAb3q34bzo2CQgvv7OQUKSLL65Rg5MVeFSYVCAvNTlPgoJCdWgQ4\n",
       "g/7apLQDcEuamC3qOWQnibU1EDG/dIL0LRIr96cOYgg7q7/232ygLJuBRaQBNkjJ8Gkztr0ylGUz\n",
       "ttMxvku2g6sLNr7Kb1TBqlZRdac6HreRMuucIF5Jo0a37UzjzoQcMJSIYszdGOA1CYot2z+8Abur\n",
       "mh3M2KQOeN10TbbYQmH18hMxBPLO+tuzt6d4RVDSg8rf2OCEEuYmCn/EgSfL/ylqEZfgcDaMJuM9\n",
       "KhfRj/r91+g5hHrb8O3EDtkGv8Kb6Cxxv2i32fWEn0VcccC6aServoPqF9VFHXTjaw5frwVepAeO\n",
       "pfsmxHSOlijVXN6vtwTzD/hq+n0V5grEVzXo+i6zgkcxZPoAG0UoCQsGr2kP6/2MFsfnd/5lSFsy\n",
       "GlAeZyeEGkaJgXzsIDmquj3qN8YEUAqwAQD3XswdZ8UXzPSs3JbpRJ3hXkDVhAE+JmEXLiaQrF6Y\n",
       "p1sE+YyS8IFxtKpL6+d/OIZPS03h75BPw4AfWsYO6keW/pZXFm/0WZ4KBmNDF811ZJbdv7BIyuYy\n",
       "ybBPUguIUYH0dMZPtbnn+7ruzJwmRra2l4C5cbqA1aFLRoyX79gOzThD1cN3mfxQ7oSKAfDlGG29\n",
       "JtGKFqOkOpKFm5emj53/XypRhY+0UYS9+Yr0Rts6WLX2RCA7E64MqU5w3AOIWEvOEahbuT8CR9hQ\n",
       "bsf/uwCPh4upKrw7GpcsPBnrFRaXVsFyjneS8j95DcUYwpCRJ2zscy3+Ky79C/avy+Mgtv2B6WCQ\n",
       "xlae2ap0t0OcOw/PD/B2xtETa5QmQTrB5+lvmwtS1UfUDAFS2YQyW7+uAng5hpaiQ/ds2Iv+79/F\n",
       "/OeVdIaiGc2A7Rlo8Q/XRrJuTDteTlCFdxJ4Pnej/G58Lnd6gLT1BfXi7cayfpq2wuymnSKhcfLH\n",
       "d3rS4w2jijHqnYx//K+Ej2WJ0zQ9t7Ekn29CO3heQVffbsiyUhpZcqINwcIbGXUb7oMYjFrj4RBB\n",
       "qRek8tGeYn4VpdZZeMTgOf7LtVlRlcvwH59iSga+K0dBlrBMsxp4Unbh8OdRzot+E4qBOhv4k6Vp\n",
       "2Puwf7lIMBHNsnoTbNh/To1mWsS/56SoGmNjM/r6b3vmKaCz5Jb+8W5fGR05C73TClG4t851Hla1\n",
       "M8kl0gErj6VcObGqVgzWUQK9kpto4Ul9la5YejKwP7/SbwaYWzVzqSx+0aUmd3TT7MaoXC6aN8nR\n",
       "7P3KPAsCQwIFiuZKa+zum+kOXobYk2wTYCeKRKm6wxBEY+s+Zhbwf3FyEzQr4CF5l+bTpOH0OxZO\n",
       "AoS/0XCS9/6CWiYuHV9QdvXwQvXoHEAcCstatXJ+oiLw8GIvYgKe+CGbQQ5XijOjE4ak0IM+OYlf\n",
       "JzSStvumrcMsuYahlno8ABVhZeNGWxtXQBktLHU7TLmiIniqi1Js8bjlgay9nPiVDhCQNhAIjDeF\n",
       "4VvzBlpxD9QjUzrUJR34Qipd71S+U186F5PI2WmXA7AOq8JrZnKO3SnSpXNte27z5IsxHbF+DwEh\n",
       "1UXDKvTZRu7yL0PLTsQ0qn11bKikinS4D9tuvcrW04Vpd7xt97Xz0NfRXpHwA753EcEBSrpKk/Yt\n",
       "43X4GKmF8sXR3CylSehvDKmkYvGfaIP4MB0NcRUDUej/qLPrmuuVkMOLGSKDEiZ5GtWPQNP+BvJ2\n",
       "2JW8ACSQJMt1pSwDKO3kVpM5ZmUm6b8qSeD+OfnxbV0lRMuwp24Mm2/IcQ9H5TW+o8QkTrsuMV6U\n",
       "ep+iSiS+/k9IgOMMrw6vKhd42VkBW7hYY75lMWLUvkzLgUbEIfPp1jNneiTRagVU8M1qnHHx3EzN\n",
       "4grRwxa8ob7S1RJxknGA/G3JNE4q0WpZI3vwADeO2+R3AmYvQ1yigc5d7QvIzfqx2CJso2zgpGCa\n",
       "tX4wrn2aEXbA6fT+cqEIu3Fu7sFtUWx5mmOLmbJLhhKdOvzrJFp22cJJiaiseW9BgFtDWH1BNl4h\n",
       "lnNRkB9rvaxJ3xtjHGjmRbSCaRMYu9QkCbpr4uVpYnaj1cpTiD43pC8uaRthAYUuwyWnmH/ZQISP\n",
       "SBaspUXOqsttAUSxGc3e7ASS4Xtogg5/0J869aU+N6Qpmf0a/m72MrbpBUtXiWN7B4Y6nwA1uJux\n",
       "9v7i+uzLBSaURJuXvgTVLNFFniM07L9AAGh1eR1Kc2WwIaCnqTcLc6pTI36bxrIYaMfXDLJvBMDN\n",
       "OHzLNrai6FqrFEuu3OZm2qXcCW2ZvzfSOz0VIfNN3hR1MMV3wzH6aCpjBKUjz1/B6T82P4Y8NBOR\n",
       "laogGbSGGLwevdxNjvn0UpzyEQDyn6xtRjQIjPByuoUBaS9a4XMyF7fmyXl1w1hzTKrA0mPTo0MC\n",
       "vbn6OVc069XcMKQqRbyjKbQblmjY9QPGqQPXS23BgVMXi3mDSGDprubvXPvbN/3FSzBAVaGsD8xQ\n",
       "rYlaX1oOK1b/mUBlYO7IympLCTnR0mDEtyxCTucGxGqUCR7p3fhzfsxLlWlAJsS8iaue6lbXHYMF\n",
       "LROSdz8hmCZ3S8iVGC7m0zSipcDPt3GfpOFB/+i1uC0UqKTfAD8iEhsRSbEZpCWo2lCFOKwXEsrw\n",
       "MP68UqTuRAdPZOsqyNPqY8m+3aNGjUHEBaxZ705VINlfpLBCu12VXaVwW1L5eQGT5TvbUY+OpBVE\n",
       "xLVW9YGCYeZehAo4wWtbxjTRresEPqmUxCYBxAYTcngPvvHz4pXkBc/BcrKIQG/bVKm/GpDCCoy5\n",
       "q6/4CriCWZNAl2LFaFXSIhAPsIOxio5x8QnB9og+fBxOyppez0ufIAg7uHBnc9JPia0/bJgVcLV/\n",
       "g+EzTPcbhfRuQRRjMFivTdXyNy+aPUwbDGDuRPFN8aMKuCExbSe2o/MPoaLi6fRWJkPbvecgtiA+\n",
       "8dNkVLsKxTUbJi8AYHejqoX4eU3tua6sap07bw1upFoXlqEEEw0vaaMqpvFVAYUYDn2LNdSNf9nd\n",
       "LRKry35vHek4dDQO555lO4xWNcAKBqz6nNv16Ug0AvqajlEBxi3ldw0nwOE7cZmA5AIeYGi6Ek/n\n",
       "l7PisT+XX9rfyk30elykUjFsg70h0yvcCVODbfh8resR5CathLXogtc582k3Ti4/TLqChKM1pcWB\n",
       "zBkG+IZ6zmpjrcJUcWyuS5l3yCgP5sf8xyG36F6x9Xl/4DQhToPOkH8wsEyokpifqkwa49emPZGG\n",
       "B7L8NXbtuA7nsFrT8SKtyOVuuitbdBm+kQh4W++X6/fQaOUQ69ZZnLWYYgowVyC09WwuvgWVsTey\n",
       "q1B7uzAgd5xkwGF9jnbegHzwIsbTCLp3shLkUeeNrGg66uIsfFwXM1MbW9/2vcPXyxG9/UiIicdY\n",
       "K6/+hfjsc7Pq1xyE1ZBw0p5dnQwBKcMJKXbwsgOsj6AgJ87P/Cuenmjm1/PS9XOsThBhGTxA9qaN\n",
       "A31lK7beq9dSarJUaJSn+zQOrWiz4dHEdpEWjop2UCEoqRLZ9HFLpvaVITdu/qguck9ojIcLKyHU\n",
       "w4wzDPI+tP/bT6ml90DNFYgoMZ17Yb+X8uWKiwA+M7ZZepHB3v8mzW0mgKy1Yz8qFPZMxUXfqs7i\n",
       "aDUpXgrtdEZk7ZtpOXt3T+8DrWZZc9M6+gvkDAiQQ33QsKJiDYkkAWBEgq01dNKPOLEVCkFGys2b\n",
       "fKSQKCIDhYtXfkb0+JQjlMXdd7GLXU2zUZxxNmxHAZxY+zQy44bFDXEZw+SYUqn4TOzXzGvIKnlz\n",
       "iv1u8Uu7wQBHZl2gdqbNVFlsdmbz4wliD1CUr2eBtD5hMm1h7zf7aM6jnUt0sEh3RQzVuw8fjb/p\n",
       "146aLiP8jG3NNAR3JMhBEwv4xaqroLXe6iQ5RNGTTQonBpGcuzX3ZqdH2d9MmpCFlcjgycCS73nf\n",
       "++9JiNUB51xDm9c/M4Re6fSYY8A3mpEAPmM1QD1sKIBL6eVk7FOwrX4pNI0ZPmqorqLBfmT9tKO4\n",
       "rNrMr43OtMNezxCgqTaqAg/7Z2VkRUVdObUWJLJ6jmXGsx7cciuUZIlqcAvEBZ9z3EaDl75KsVeJ\n",
       "nxoHFP3OGAlBn9zyePkQkW/ci9AZrB3tCJj3GHhSpNgNAUv2kriqrjmcbQOQzQy7mykaeorf/tFy\n",
       "pIKZcBZzuxze0n9mn1lOe6bc80+5vIdyJWU93jRRZjy7C8nxVgPZWdgQJHJvne8rlyjRopomX+W3\n",
       "7vkNQoEuR7XAZjAeqAHUrsZfCkX5uRHkLdqUDOctC1bKcQz2A1r4rTPsHS9QEdrcj+b/z1yUl8wA\n",
       "Bi3KA5MWRaIaNXIe5LCH/3HL8Rg7GyvoIvXPcOWq/k4V6UEkPJ+jDeTDtAHOdxv+t+mnG+6hpvPt\n",
       "uW2TF0/PkbWOhx+d+s++be//s5CmrJK4dOjRhzX0kfn8B3Vy8pyeUDjt/nzLuFLoOsHjmmcyxXRn\n",
       "habMpFcWxNvr2AXVK9d+VuLqFwyy2hdM8QmvZjRGo6OTPFhWwAq/xkq9vIc83ze1KOcsuUmwWOtr\n",
       "W+t4HClC4aSCINMzDyKpatC8bmJ5EIakH3Vi+Iik8NxnZSsCaklb1bb2B5DZeFXiK0zhq6dto17F\n",
       "vfuibJgxJolZXM+fXDB2rOr9exPS75sjGD5Kog5EoWb5ebvGoOzeOGDUQ3tW1+gdklMSAob2c3R4\n",
       "uS6bZFj+YkYRYGUlGBWZvy0I9hN8+Iw8b+QvrACSAoVBkJf9++BR5uirYRVtY3gNizbm7ubtatJT\n",
       "fcLjKwaaQVYGTugZcKHgXkwd/BarcHWX73sP04A10kGrIghMEsqw0mTD7RX/mW2Fx7/VYXNcTCb7\n",
       "KCUTzKKiPEzLl3CO8roZsLC/sMkscVjig9OBn1lESzPE5r6uxE/01KIMkSbcYOLk3tRlaBDhtud+\n",
       "L74XRBE/nEnyUZsGlYfeF8uIRQAVHK/ChMiIGfgaBILbMFUO2h6D+ZFewOLMgCt0qkpNYc6ix4tV\n",
       "11HFhiOm1KUOKeLzyvObyrVwtDV51Rs4gV0+HIEStYzZXlI29WRmoO7pmPdpe78qCoAaNNMPJ7OG\n",
       "SHyzOVm2efJg1T1OUcj5SQOaadXlh7axJijpMMJjNwTnai+4oNqmGrd8+VsrDHOGtmUx5M+WlndQ\n",
       "xXzbl9TUARe+st6l17bsfQpkiqNL5EjooqscZLvyXb1NcRMYcOQtiRT6vQI9ZrfT403i89noTdAt\n",
       "JbmxHVFVpBrr8ODyvyX6beZO2HYlt+HRtp6yWh2W+nQ6d5Kqw2LVj3g9KCk9ziPnZ8dScTx6bGEE\n",
       "tuVTSFrvpkyS7pj+8vURhWt40x4CC6gz3UqFwYilj8dEEyCYU396Vwz0SePp0bdHUjGJm+IxgvK/\n",
       "mB2Wens7uH+/6SLuB+t4KcdRdaZje3uQ79Lc2WoedBb6HMdfGIYfvIk8F777fyVR4a4xClcv1kOP\n",
       "2kNE2iIaiavp9ryd2xWzmU/FF1bwsuCa7cc2QXBruBWccnpspJ4MxZLAlIkkbtPk4SummbqBY6aU\n",
       "5o3IrDaxb7QtneUpU5OzMWkNnNSgL7LPmscMWdJsVoxmFzy7cwu4OYI+ZFQlyq/gW4iKtT4Kmbxg\n",
       "TS9v3pB8S7XHqrpFCUlKE0cMjump5FoNMucFejjQoSXxHV3PqGFKzIQq/laaSJvo1Xty9yHA5Osa\n",
       "sFzFSO5W77KjLJewU+Avi4UaDGgBi467fCJM9yXKCAbRHMh0P8gNoW6MWMvKAQ1WhxgHVd9NAADL\n",
       "eJ2Ct+40qx8Okyz6jfKrotTKL4GVmyWMk/zvAbEg/vCg5x+8ftRS5y+pwyO+FC43cb4T9OGvkGNM\n",
       "nBJ9IQPQfp3VE3GfTG1Bf/s5YQl3BYLJOJsGtIBAVbEvLCKd0nk6fLCUR6oOgU8F+8BsHH0ns6HO\n",
       "bqb2ERAnyUduZM5gQpPyisQt7xKoxknq7avGoGBI2YbSpvE3wB5GYBqOBBTeMxciWKKasWqEEebv\n",
       "63+gVMSEUF6oJluB/Sa67RepJxG+gaVlEcaqS0xMZ7Nlbb1kLCBOjpxLdeCMxV79NrVZzBKfi88L\n",
       "OhCdwxQonqA2aj6aralCjjMpGrbIKbSU6OMKCIG6cbT+N7gxsLhTuJuRSirGx/CKeI6rL1QMbebB\n",
       "2Gq5Dz4bpCu7+3CHrKEEOWGFKfIrMBn1hR5Vj19ySEdMi3cbF1WxGpTC8L6w5Qrx3Db6XogtYjVD\n",
       "iTkhb219BCbMZU8BYv6ZjDnSNi0+KpGoS50ysura0vpTEgrxUdYv0LzZLkyWwwn8+s1yBAWTm5mD\n",
       "SadHPm5IRFIrg0ojTG2pjZjXq/PBi9/r6EsgUyL09uNi9NsjdWP/Y6mqGrLu873QfkOHRmIbOiou\n",
       "+7HnTZr5rAuo0sVDBrxzwviLaMHmtNci73USaMkYXMzhpA2Db0YQwPjRFqjf1lEAllUj2yWVoZbk\n",
       "OMtU7DnSrVHJKN4DWeX1OSuNLmpBaeHl5+9VC6iPlqeVhR04ChUx2zpma8Zpg/AQQjpENT5d+2vR\n",
       "oX6YQfuo89XJEyjsAr9aYfJL/18PgjV9nJlqXeJSCdN4RIoylQ+aHk4vjaV7iZcTD8/R+pKVae7f\n",
       "WV5JSacykOPCO76Pbw41buczxV++MrnolZ22ylEJN38E1zDsSVUU0kP27+DslQLxCSrFBGjdHkiy\n",
       "fPBALEVSbOJLznFVXRnic9XREwb3/tmlQ0QIOKhZQPBrFwXFEH3rujeIyRJAhAeef7RehVXpvSYK\n",
       "NwMMcbSWQTX5pqPC9IBVeMo1eb2AZ2vl7ilTRT9JI5gmuD/fOs8RC0msFagFqgKKTOD9neb4KqJf\n",
       "S3mqeHwZjVHo7xUpyxOXNsgpMmWIpEBtz2gWMyk7w5KEVzlkF72TPWoL2cYI/KR37sbTATNTU4p/\n",
       "qsEXfJvoljZ6HAWAyCit8IBaboPH7RZzA7/kK8MYL6JwWiLG0DSALrp0dmmb5fTvbG3kOz8gwq/4\n",
       "t9CZKDsiouDeBLonYX0qs1de9uPETZcQr608JPpFutnSLJdRPMuIHMZlVYGVDLMIQitSqvNYHXiV\n",
       "XX4qH/ZczhAmzkgnRh3CaP0eFVnJXS8yYalMtTZmJfGXw6raKY5UmhLYDJ0Tcbwk6Q1SqpKsNmAo\n",
       "gOqmbH0AADSXQZ6FRRE8Ev8AABz11UTuR/1OZRzQ7Ft8LoBSbWH/wm6HyYguoeA7XuR77zufNo8f\n",
       "jZsxa8PZUwgLW5W/ZXzWyJhBt68QvYHDev1fskixigNGWLCjLFQMgp9IaJYUkRsB0stcAEvgsYuB\n",
       "I1b5df8FaGb54qTTvCvJTPooilxor9GmF+Wo4RXKzflYGelH1KlXZUSk2UvyKx3w0nE1xKYn1Rnk\n",
       "KH3thkRkBUGkF4UfGyK35tuJ5vyCGD2ViAtgKQwVzu869nMwy2x+GF3pZFkLJdqELBGg9x9tgtWh\n",
       "hc4naWghR39A8BEumMbYDy+wEcwBHSEKU1OE83i+OjR0lStCvJkIe33NW7JsJUNyzB4rZpcXFUN4\n",
       "TBvwS7p0AbcXMchgLL6c62663hhBBcd1zKa+KWUtrMEVTw8uV+J4DU3U3tIBhEeg6Lh2AAAxxnJF\n",
       "N5UT1yKSNnyJA09e1XONIxnrw4i41BsGPKCRBPeHO1jTJlwvqV8siNcL1q2Dc4io+t7X2Fz07LOo\n",
       "HynxyOIqQiUsqu0qlef1S27WTsKY2KAMQ24/kl5wZXVN1TCJs0tM0lct1J+hG3YWojH5Isxgaw4K\n",
       "6PnoUzGtk9Cimx6bSfw1g7K5klQO1tTJg7leXWFQkP/xIBfpuV4q678zAt3XcJMd+dAt+qCOBsAQ\n",
       "+49YitHzZBFfcSlsMINxK0b0DgDHaVrMSmnJXeMfOdgjjIjKIkhelR8H1RqIl/f9m3v/2rZJlMQp\n",
       "v3lmiudnobesICBsxTM5NA+IRjkfManXkoGm+Anxg1oJTbJ3rNveva2WNEu3afQhFXr1OU3KwGcr\n",
       "6K4sb6miWiKtJeIJHraIV3DZAU7mkgUeUD1H+W8QI1cEdUIOUxqt1EvnirzclgFsKuCT0ITpt8gP\n",
       "mjDbPSZFDRQEy9VzZYdKiadXmL3J/geddzDq4PQ606vawKPniSAv0NRKWADdBBY+HHEsjtiS5SPJ\n",
       "Lkr4ayvtLMjlgsLkqqdMPFUbgwfN9wSs0tNtmSnG0oA/j+pv8FfM1Ct8NQohDpW0zG4DpR/4d9+T\n",
       "tMhiZFxHHItsTy4kKdf6ogSqX+n/yrdQn70leNKf2nGyTavIgcBTeeOUuhankzD5nyXnpP5tTOQV\n",
       "EuJHTeH/2F2ShwZvHqxXcWyNQfQEgyvLy80BLyLc0OI1iLM/9DJ/G9eKfTYDGubKOHywWkasuLZN\n",
       "BqDiVpGqVUSqBxRYR7rO+r9H9iN9ii6vubQtKX+3Vpm0yXZTt1/qQ875yHI+QwwvFAjUO8L/IQF+\n",
       "glCdCKe3DvkDqgLdLOZcI4/2m2uoLiL3SsvAjJ0AyS0BMnJQ0pmNragVTxPnEUyvfmf4wSQ4veMW\n",
       "2cJwyGM4VwXDlOU8nv2VU0N7Sc7g4IfgGeBMpdq4WmCatkpXa5lyOIEBGOy+6IceJMM1YPnSsBpl\n",
       "o1ZgPZfZNsz8pIZqNzFuH597wX2sEk50AgRjjNJzc5zefzoMFGCqeAlGL4W55H7y5Aqk3aDxPjGD\n",
       "HR9hClXaMNT6dmgXfrOHNNvgHUIK14tuvpZ5R6/QqnhIIcA6vcdgaMYOMdYzLq/1064arxecNx4Y\n",
       "dOXx0tIaDJXcmVUwCZWt34BbpUV5Zqamz6cK07y/xWdxIHNwu3wQcotn5uJOL5OLQG/iXWXbzJA8\n",
       "JcNJhtCXM61JiJtqD11jb4O9rhu06Xqq/qRA48hME/0QN9H94MoE19TjayJripnSnB6RHZR5DGXs\n",
       "CK1o1O/kJEb8KRoS+aGaopCfTZ8bNhGeM4dXtojaZ8fYx0LYhRqvKKzPfCxG8ZKQAHTVlLEf26l2\n",
       "KheVhFMpGzkiKBYsvJLytKOhzg2le9PV0Yeq9qlpafFbIEkCO8mvJC6SEMGGrd8xrTnOyAKBAWzs\n",
       "t34EpI1SuajD2ljFYAqZbRARAoNPVAre5puaWW6xo8KmFKfEFQAYWGCFi4P6ER5Yd3i8rWPU3gh8\n",
       "UEIutk+I3qkJOcEKqJammMnbZnhIIrQZ+s79R6o3jWyo+08msryrfgKNbH1sgggwnAS8i+G+y+jG\n",
       "oEzkJqDwNaNTO1+yBbaK4jc9+cuNTgugQ0TTlC8R+46WLZuITdjeUQt6qJVlh5+uTqs82FF8P70M\n",
       "gam9C5mnSIyOpLmDd36pHAeAQuJ5BzSrtnInTWLeRU1QKX7AQ2n03qjHXaeBY8O55KpE6ynuWkH2\n",
       "/v+dXtm/4UEDpFZ34P+wzxWi1Vum9HGQB6NEkyaspJYk/peKW5g+B3XX0pFhMLfQFYOMffUTX6Pa\n",
       "MfigWAk7PJa5gHkcKMHgnbuVE0Er2j5LvkuZxzrvHd/JB1HdxvAeRjfIaOVs+KeZHaEGtUV2j7O0\n",
       "l/s+CPMJ7+dwsmyZY3Q47UIe+iIK2VuDr/0XMCiqnsB/ch1HNj0HuLHgQB2maS+MR/Uk4CxuKzjw\n",
       "01VhBjQnRmnJenr4+qqEjL7jgEIn4gXXPgt8GaR+KgzEuxVWajlz68fh7Pv2lTnXC7nYryaOvikm\n",
       "JWbBaDMHnwwCv7JCgd0a2d+b4iBPLgkDpd4OPkLMmd+oWuPR8VGBgcjUZ1NDsXv0eYNKa/F8Cr+I\n",
       "TIEvP3idvXMT9jAaUqNx0InheqdcdSWlZsYD00JRdiUpbYT7Jd+t64EKq8WxEINJ/z3xuh6mwtNq\n",
       "g3u7VGCQLyTTH1MkQp2AXxxd9WWa9aEEhAuCatHpjLtUOxZHIl4roeOGjRvPEkHGfIu5pYSHnuef\n",
       "dve94HHRDnFYb3cvB9y3ycmZKePVzsCW8PLNeNyNIt9qEM6aikCV3Eb/MXdzGXa4vhwVw8oWzQJf\n",
       "Dbc5NGy5UiJQ4pPbrNPPvXtQdEZyYf6iizs/++JTkh5VCM+BUWH9YLlbgak+6KFAtPxBTzdtdVl5\n",
       "uH0fIcX0Kdr2rIGXFZkwrtZtgoaX1rOGRJYRsOa2QxGVks63+qkG/lTgU8aBxDTNXaR8ksUjTUgJ\n",
       "Q9MhwFxPMt0SVQbK2Gdnn8LKOrmQ2iAXg9NJ5gpsJdvQzIopfIrNbEYfGw/jaDVs4tw6vBWiA8wl\n",
       "VeieiZPe6KRHpBHuDVyKIAoYdvdpRP/00dQvKitG9MKQfSIC1E+mD9PAX1XdsKQVGMgBtvbPmjas\n",
       "ZQfqwAX48CwDgY7woMWbyOu/0pnA4Gd+Nj7wxdNZyWl08uLd2JGTfiZFWbrIKaMpM3+m5Xr4vKLI\n",
       "M4CCgx/nyRTtgJBkamvkkf4/w6QLggeMhYv8Yq2dzBFnX6Ow3SCbUtyxjxfmcgrNRh28H/xRGzwI\n",
       "wz6t9czsYbQ+XIoZSo/qnZM0pAqUMOcHID2IlXjRzWSBlqvo8brLNr1Sli+ZSBPmpujfBnanx/M3\n",
       "y6nk/ndpO3RdBFLqDddQ9u1Vwbkwd++S5vuFXUrhP+FvY0kTEDUAwMdEnNYhzEBojPtolk3zTgf4\n",
       "5mNdoNQAZB/VvngiWW7YGfZa94wp1naGmKnjYsfcx3aUEnleow3/PnFwy8MmTylZ38HJCBAxejPY\n",
       "ZOLnLK9SBekZVXK4loa/sM+F68FbB2o2hAeOS/YEJqDIAYTi7Dq02oEyTRWRlOXqz8WnkgkGi3Yy\n",
       "wOeXBX0pZDl8t2+wflaD9Dxrb41vTSB/nrIqNWVDsOsiicrER5nSX+41H2uiajcsCH47BLG/TQEV\n",
       "NZYYE23A9sTfylraATwNS2IXZ0B9DFMGu7FaedvweP6Z4/exskSqGhZU8BO2SMgQf6pT4Bq+xLsJ\n",
       "zPMj9i0QQaoBjmGGcpHk/dEtjPZSH+MAhMptHfwNNFWXXyvKy/xHLM7feUAwHMbOKu3nEWTT1Lcs\n",
       "5Hdva1ne2WOgb5NMh7QhVffD/WemCxWXxQn5oxFD36Xtn8JCs6aypV5z2vjXUsZp5sdcVAnWMUHw\n",
       "cL6M9bloC1OOCXHY1fxLWRm14kwA1QyUA9kNOaxgzWU0d0Eyd6QWUGQR7/i3ibtWgUkHRnQe5Qqw\n",
       "uls0jdqNAGGBZuP0udUejSy5bFzEJX9wL2mbsKqkF/cMT/e510CPk8ESe1xv4OPQWdFTvrOJ0al1\n",
       "q3sxdniQDJguyoAj8qhh6xqCJp0jcHIlH/FgUdK7wqlhSocWlUWEAcUUzZDs1CTAmfYI7U0UN3yW\n",
       "lNym/JAByO4kPL+2EabCowmRqqCaruwUeHxBT369UJt7AhbEsZYnP4Ta12aFj2NShfczC2fDcuRe\n",
       "6w5auf3/EabX9c8Jbxbkl/Ll7UN+AKr0NqKy+IHBhSJe5GR+ZZ988zrQsEGmIZ/mUTgiliMahChY\n",
       "whhuq0m5DQx4NMlyBGCwPSQvpxVZJGReBT8Y8bJRRGD1yrJbOi9goagxEH6oCtTp09gFU8BHUTw1\n",
       "q+wAzuY+q985W8cocfnFlhpl4K9LBZc1QvRaxuBr3Mlcb16qdowyeFcdfYZXGM4aliAs9bTdzTyx\n",
       "8a6g9EDCQef1XKcpHEHNk48sylrfjlcoJX0lFx+M8JwXbLUlpcthy5WGTdR2BngmA02gSd3dBWy3\n",
       "Rk9FMpr55/ylrrS1i9ey7Pu2StWMleUf4Rm16e1OBpbPwl0f2eiFoNyPjriZcOlXdqWn3yCOI6+s\n",
       "9a69/RHit9ejMg6ID9tzYXPrCDrl5YAESIuTU/bu54QBfCrMZr5nFqocRCLmIgIqWUGKJS0mP2n5\n",
       "YtyJFiQ/p7jrSPscU2KoO7J0N8cOTJLtAT+RMAAWoJTA69ldMcxJY/TxrBJhOmlEQcU79eWIpdMe\n",
       "qNw3lPZlfIR0uxfI8iiqpaygPAv52P8jqCD6pm25H51HiVbhe19Yr5QrGLAYvWoflMsUe8G1wavu\n",
       "T9U9EFUq6dm9VtKzkQ7ft8AZETVadClbs1gc8PnLDO/3eF7ePEk/zTCVtx7hWsK7awNx4voyqwzF\n",
       "PkguTC3MLpY9lPQJOAZuRfSF8NxVSVXfIkAEuRuAZOqj/zQlty8rLFb2PpZAutHY5LcaMuIbJBhN\n",
       "YWGjvbqM/7ilPiOD/SfJUZFCpPNfgD7DEwWD4Cbg024j/m6D+DZBU2MP4Q5LziGVORX3wIjowD7P\n",
       "OR4lvMJUCfYXreLKGDT6E54ykHH73qhzV0bvod2ZwFoyfP/6NVHJAQGtDIvDFEXTwQO5Pfc9JAS0\n",
       "Gg0HDPjKn1eAEbYDEmZ8Pt9Slh8uS4NOTn9/Xp2TVmhVXVNpW3LXf+QIHVVuphb4i76Y1RZpyGQP\n",
       "5paz1SOTnFLgyC/RgGiVmsqarjOKmrSK9dHdbeD5di8OPjhwd5KLoe9lAMCKfUgSGo4SyqdJeKmz\n",
       "hGuQ5qbJce40nqjVEW2HENAmUuXxxBaG8zgpYFMRMcZn1mMr+pmNQzgdtNik09HVgV0yb6HPCU7m\n",
       "0fyWwMTNXbXblVowGlVjN4nZM4akpsoiAGz1gxHWR1ThvNLPN/7rjpx0AQQZdxR+chkROEC7bbfA\n",
       "/c+DTe/VU3PpvDAMlLHTPD1tnbGvFa8Ew8CXRdMFG80834VkOlE97TvammMLAkq9q+Rjp4Rb+GJH\n",
       "bpAQSoY51piPAcouvcHVfJlO9AxlzgeCXjKGsqWQOuCOfKQ3njQ0P/ImKMrs37T8xDMlBPqJeyBl\n",
       "MfIBzhcfA2I5OQHVNveUTgT3oKm58J1w++DaOGiEdL5qxPS0mmjLYVRQWMxhBm4mlhfOZE69ncIQ\n",
       "3d5vqKtK3OTQe/q6pU6dMiEvVn3UneDB8yj1eqxFLUfeiNfeR5p31J/TsvPYrmKDU+IjSetPVREE\n",
       "KjoyLZ1Qfw95HukPlVDC8c40EQ6ZEbrIi7bBfR3RJZWlXkJe4uv3hRqHg+jowYo5OTINRaR+i7ac\n",
       "3x04k1MfefFi67R+C196pxIAlQ+hcaIfqAJdnGH40LybAcoFdNypRjCSny/eEKIKbzg8aJj7e14e\n",
       "6xixkaG32YvGDStQU/xorupxG/Rlwq98J9ighCu0+q17pwpHC29CnKcmSFqHQrND97WVZlJrPojB\n",
       "5Z2AjasxGIxNZDKpYCPRwbIon+zXsRgGpOil2qBWt7HziH4925yooRcdLPaRb7bfkr18rqt2jny1\n",
       "wxjew3zMi8O21s842QIjGXo5I1CnNJYAeAIqSNROc+ZBEZTDMc3k3ZS25sibmdhJTEAT4+k0OuhX\n",
       "AXhS6Q1XRZT2xXdUGDmb8j0IrBesvFn6071dHnkilRhByvIuzHA1bbydxp2Hl00ubMYZapR1ue5a\n",
       "xo/FtZxt51LJr0CR7oRMHa8lBfnsdliD5d2D+Ef9pxuxM1DV4CAyzvdk5WaQp9A0GdFGkuymOu+W\n",
       "+EJK60FJPp2V+f2S8/z0tKdUX9T32Dc4UdtFDZiQgrRm3OAMIVrcVwmfS1jZ0YtQ5Ct/zKdjrfPf\n",
       "pp7EujT7V4o5WGjVbJAiHfv2Yty3A4sMoP+92HnCbuvEEk+VmIiC20JPIch3sGHgXj7wwlkbupeI\n",
       "Ro2F7BZI1He0QU6RgDIuOLvp+vOTyujO6cgNECas0O6D+WbU+kUdeTF+5atAmZx2IcNsleVFVAEX\n",
       "kPzQaIYkCMWPCUq7NUu2SjnJqytLGlOJbUgKbsQlNl0Il6dz59Qt9f+JBTY4CbcjgPnwTS7Tqtgy\n",
       "CMr3Ny3vDp+zNs3UoLiPVs1hhY3uVw4IX81KXrM0cHhWorIatxH6gpm3Qb1P7PLtGsulo9FfPcD/\n",
       "WyOqoUHWA6t5CRCS2ySBcuIinhkkO3dnmFt+TW/6oFueK9/c+MRTpudBhWGcum3jdy50VbDzSzya\n",
       "lRzfIOUMToPRVOU+f2k/hxnpZGfawEtdCJOAJ8Zt+cqPirAn4M7FU277ohviuFmP5dQPIEjJxebw\n",
       "O9fSw8iYqiOHXVJR/QtaE5+nRGqSnZXCDJ9HLfwS42xyY2SMulTqeQQKXqID+7KkdVGrxT1l9gK6\n",
       "9nPdaFaP+LkbG92z0hHpwanTLa+FeZblM2aRAxkup8d3Y59yl/rrBC033gBt8/b+LJhLGWFM0HcY\n",
       "mVYgIAkSMwyboeHwKYW+9B/jDbmytnzu/b3J7nGL6uDjnuUbOl4VKm+pjFUn7lhlGoh/4M2uMtsj\n",
       "ZXOIejGKpMs02V4iUGWDjF54kTdgGrFHBOPVCJ22/5m9Y0cvqp45z+zrTvajylzGhwVaALK8xO44\n",
       "tGKIKZxqRLiQs/LDAE4PzT8O7YoTKbnP8x3Kn2A/66cfsOQU+jlH22psGkKmYe6OvgcdIjuudHAf\n",
       "e50t4e9BBPInKxWi5WdlIORwp1Av/5JwOnVdVvSJEUNu9kEFMifxbZ/XowiGQqUN1hy7eTtNTAUS\n",
       "A0dSwydJwU+Ulf09R8fahKsLOceDQdB84+zmXyMxeI/EXtJuwnXijSBW+F0mZBIOvQknEQuhRUtl\n",
       "2YcnVy9fvHwulj9yWw/4Ciy2LpM20IwSQQeYUYnB8Wkoi/tSpEBtkZ6Y8EVehZDZybOa1k6hZpc/\n",
       "2b4XONC6j0OV6S8mfN1KonLolspTkAQvT+aNzgg/GkCRPhChucjG6Ipa8nQ/XP2Gkxs96pwSHrz6\n",
       "LEMxY629k6z4UhdFwfhUVFGl1uZuPybN8hvMKecF1JHdebyx2HMF7TQB41Dg0Tc+WwWXz7c5Z3f9\n",
       "OUacnK4dhgDhfMWLioUiZTtIA3o2siDrSfQTy6II0u1zhleXI5GM4TSMY3lMFe05iIWN/klqbCWI\n",
       "CA9NsW9VPVPfU3KxEK+sysiZpEzcbU0ggkHActJz8oxH+mQWHE+uJmP7ataV7vDMHUfjckhNGlrG\n",
       "eln07fVsCk1KbIaCta4HZf6BjJbv2dBicL6JVRu7NNplxdt0dsgRlA8jPV2XIxLbzdH6Cc4fZQ8f\n",
       "U7346nR4Wo274AFBmO//5VaNb8NJY7lghmyqKKGOSKMg9YFb43OF0fAMmmTlStDVYlHEglgv5vFX\n",
       "bolLWC24+5Um6KPsBqZYh0rlmN5PxJPeiKD9LHsbxZ8fxeNh+xMCsx8+kPbsi35mw6IgXlUixuyW\n",
       "w4UiHlkPXn61VPsCI48ruo7o+ifuzQScr/DYFzCbpwJhXSjFxXDUGe7vhKNTWhhNGj52R6KsQcye\n",
       "YmkqM5zE8G4RjTf6kHDlPRcoCsLtCOqfDW1cSp9pnpk9zv8yR5DQZW4OQtpHoE+pbTXCjeI2Ux1o\n",
       "ViuugYQRD0IO83Bg7avqgAeHvg8T7ByguP6K8VaocB9VD35VtfAUtqAk/KQM21V3hLAExiQJY43J\n",
       "f8EENgXkz3DeJNcvpSfZsUH1AQw1xRY3ghW+eVEQMMglgm3OrfqGOWOkD0p7aGfoN/SsOXCkw1Y2\n",
       "aQVYwqK5dK9LoXp0g/NIDFqhzKON/FEJug76OEWr7oJ+kLrxaCg5jFyCnMhGOMZDQsT2kq2vIncL\n",
       "3pGewp0/SVbuzazY/oEIlP1erDlBHM6FDVQ7TICAF9CsqFkVxjy6h5JErQSffONl8fWpI8Bo0F7J\n",
       "+OAWfOn7I5IhW4XSiQ3M5ADP6+0/3mmKpTzbPa0JMRm5qFHAPj5jTIieT1sLE2e2l6c7SbFez+jT\n",
       "EzHX3q+dUyhXyhgZQFAFlPmKRnyBNEebsuSdRtgbeZ+WLrP4BlVvwDCfw6CjGrUzSWb8DuRTLs6W\n",
       "387j7yeF/13K/jd6OLImM3Lqbgd2b9sPcB7MfiNgK8K657yuGgu8+jBJAroi+hJZRb49XgE/dvGw\n",
       "/RPKs1u/KkCQy/MA1VCKr6j1/sv8pwj1mXjDJ8KALWhbtMnNwEXQWVIjSNg887F6aj3Ync/oLgvl\n",
       "c8dBf+ZwdDE7F7FJkosH7My96xassx6L7ow+1l955SCm4hGYiSKBIrsjDfmO7dzU8ksfN2tmSWq6\n",
       "bRxsa28pnXrdfWiShNMLX19uI6FF/bOKV51qfbmQKSP46u8A7fa+swubf+WYMTch7hdef+aZrx14\n",
       "ww6c8DrDu9dO+Dz9SC0DRWNdc4BRRq17c13k9rJiZLqMl9aQAX5WHICSb4I6UrXlS2Sk+7XHpYtg\n",
       "J0FYK6V/8E1V3hid0Qpn8UGlkk+bz4ckkjgWpO6yvcNljrfxgIluYQG3HfD1hCY0xZpJDr8hLvaZ\n",
       "Au5BkafEqNMHYAat+eIldtVx6snILUxF8n0DrL5xNVnnYW5AIyANBsxcchQV/1a1KLR8Qs0D/4Ka\n",
       "YYfmyqR2bhMQUSZ2I8fXcHkqtdDbaFBVhE1K7C4bxROIXmyzJxMMPUlEnlJebZomx7uK77FjNub1\n",
       "FSitWXhc8fCzqByPiMHxrgBl6H8HJrjGjmwXrcKqE6257mHlp9+upYTbceuwna8ufvotFDwl8MCT\n",
       "D86Y02j74OmMl7/fxw9C9gLH2fxGCK9TkOKqsuBHaUAU3D8wHXn71GhCUythLLjmJTD7JC4KQ2WT\n",
       "drPtyyF1JTvc/lJr3DTvoMZTSDPx3uZ9OFuOMLi64WNTD/DOWEYRfk9mgAOPEgshqi56/omAcSb+\n",
       "7CzjEGuzByva+Rngbawn1t7SXrv5ZkRYRZaLZ6ITumclpc26u5i2SErr/e0eK4Q1Rr4hl2k/Cw4u\n",
       "273yBUw9W9DWMqxSx9P0iCeRl1nnxXCN1CzUC0l8/4zzljIPdzIeg36rt31r2CjT0xHlMeePtCmZ\n",
       "2pMGP3ut5TVv73XKeZaMq4FPVDnu91MZjZbce6o1EezqMjJODunsShsbUEwqbkM/3schfeBDCOah\n",
       "yGBL4PHIQsQuOhfUHMbesvTGfKoR+B9yJ+/zR5SLrN9ysLNAyPD+B8QOjbD70cYrKas7Pbj8tD8R\n",
       "iM5+TL1R9pDpcq2aUk0X7tdh659jU4nAhgriTnVpd49URAYS5rpufqsY/Jm/gxqBxvE/lERxUs4b\n",
       "4j3VlkhWrWSzYGxkGdZuqdTcIl4KTEPMyMwyKTwF63QCm08Imfo3coFb3bmfHSvf3LLkFh1R6MMD\n",
       "2vVDb5GYaKp5K7+EvZbG0YueiR4GoPCkg/NOUWktSXaGlHPArprC2+MKN19fiq4e7je+cGzdYN0N\n",
       "uiiy8sOWA8tqowsCIYRYlyDolk7sRLb1+5ZBY5+Zdi+sZjKjs9RuYPHoPjt+dgCqPbPoEHWiBgrS\n",
       "foZPPUdjLNGXCfWMqVA+jQlwstKqL6kwyYTw7EJ2z0pOcB3fX1vi5UJ6y7I9eXVYyC5kNqcqRQ4P\n",
       "pO3CISD7nNu7vBszQgwVy9V6dN+pViciUEma9sy0Scl8XaEjmWCidiqXuERkMrDeZFd4CMG/B6vZ\n",
       "IxRb+9P0g3+I684Pdj5InSbRrHrAp4l95T7Kitor2kHJbIcwfrvtkAYHL906GkMmeRJhXqUuMQho\n",
       "a7ITdqoZDe5ctZ2w9yNAlZBq/kiAH4c+UWKLUZ2lDiKid/gRU/6uK2DB79CN7KWwn0PlH9SVjVgo\n",
       "EJ9CR82zSBE8Rfy4rCGlWLpLGTrZ/+Z9R7AmKU6DnsLwozep0BU/1oMj97ZigdipGEH79VEfYIHN\n",
       "GaD3eBiSU02siCK3NVt5eRY2QRk6X4KNopWFsvQ0ySldz0zTSEvFhpaqRfICI8gRO6h4WJUNiJL3\n",
       "W7z80IVcV9GEjPITgSRMkKHMZCVIl2Ee8biveExr2DfxA1EnhzRilQsTXTp88TBP225o5Raxw3Fi\n",
       "KpZs0qAby8wh5AUYdTO8Ln9XShcZc2tK/upuUP+BDjDtlEDQ2PmL0FtzQUiDGpjGtkLG4MjeRQ4Q\n",
       "h2KiqSspdLohEPSsOAn6fvewv+JraRk6hoLePcpP4JclkuYJvmU4QPiOuVWFPlGLmsiBb0OSVNix\n",
       "r2A32+naljiPox4ilPEruVOhwZkHt012Rfq6aKoGB9FUS5Q+bQ/JIuWGfC3LK5EOKpAlgMiNLACF\n",
       "ETdV/WVcvXcESBJoOog4rufcSSURpSrO03nC0lU5SZAL4hBRDvkAYP3sZzxwiAM8KS//9YmM4LHH\n",
       "nonu0KpEnqoszfRXvwApzaaiz1mDHC0cOUrGWLVX0YbyY/5U2Kb/8NRjYwpZOg0dQ+yjH5+3p7Y5\n",
       "3zNci2SZU4a58ns+FBObHqDcjtFYuJvL3Nl1xpsvWuEvIg/xG7+t6QmZJNw3ZWjMQrily98D0S5E\n",
       "a6ZjScdtELGG0aR7sDlJpDRb7rlHd8RswfkSB6ayBeSmEi8ncp0TdqvMsOF4Lo3zXxOJzhUPWBSR\n",
       "TFSGO9Jr24PyYAKKK1wbs1pLSLFbpDX5XyRwr3ZHJSJcC9sm07CyBqJlaarKqBJWFXa+fPWzKmS1\n",
       "XwTvWaQvYIMj9DMOvyJu8MQtTVne9H8zBxC/dO6KrNINuKzeQ8egOlPibBNVM9bCesMKFqE4R78I\n",
       "Y8LDL2VW9PxK9fMk+UTwfBs9SF3FTEnmHnf/AD3weTbMsoVJANietfSvtcNdKsOdu7Vt3Xjt8Z7O\n",
       "SmZa0fn2JaoHofgqQmTViluvo56ZbZCO0JfBhV+jKCFiBzwelppmxXPS+gpmqONSTU0QUvN6GiKG\n",
       "KRSoIHFrL6Xoe7O13taWMUqB+fvPbm8l+iD/oEQOfiZZpUoppUcbG5nBUunDc9603MJnRXe+9OSi\n",
       "ZVwmEWjC+PMK6zQKO72D5YTqQNaRGaNqqKMe+5WskSPkwxjaKGYiZj7ksF5euTzpQX1jxnhSDuPM\n",
       "goCTJCG7BZ51fQYnER9/A75Ls5BHXo7TNARPS9nlvkxPIaSwZ8o1ESrKNFwDE9i89TXBZfDGm0kk\n",
       "EyrCI2yxeuLKJGyCuTCSOusXM2Kxx3oUI6rdv//XaXnHwyi3rm4AtygFtgqDgkjSMMMc4xslPF/S\n",
       "fsGV1qeaYELGVGbxmO7xRNOPe07MOfQsDAasEsYSIUdfoUbTE1e43Hc2/xxuJNwcKc5HRyObau8F\n",
       "0nMIy820XbaIz5FevbJjNfVpLqOBJvGlqPD+LqJuCsgtA60ylBleZeisg93TiRHwtkyglZNgMbwb\n",
       "w8AeVtzbIHBYg47aExLNOFDnNIkR9xjVLbZ/aNYfWItM75PW6btRfuCa3Z0D6U9RbVYsIns1RHgV\n",
       "Xo7YRD35d9lujAczxXnDduoJFp2Vj2kcJwV+DTUcACDHbpfKitl9jD3emAtqWRFsGwiLzgQXHf64\n",
       "nH2xax+DnyLWZ8GhlB7rP2T+s4moUhhI/3vH4E4MSHH3c18armRCAp5jnpY/1GwXz0ClkcqG8EIv\n",
       "ki/5AWSaJgepkenr5DgJVDf6Xf7vZuMVCLlCqjD0SuIh0Hp3uaz8FKgiYWRsJCuJZ6WIxTwlq/D6\n",
       "RNPD0drVfuOvOoTlEg2CSQi6wpbQIEtH9Wk74e2ActWEUNhmurVnzFrx/ZuO7BrySo3SBqDIN/cR\n",
       "BEBIlbwzUZINmWH3DJpx09jIVoF6yF2vcIFe2Z2iWF0Gg3+iKK1V7pOwTF7eILuS3TaLnbusmzFd\n",
       "8qW3+NfItbPKfCXhzN14uDLWcsPVu6n7Ns5oGIUnT6d93yjBV/4jVRmG7xDxSUBBT/ExtiB8aD3O\n",
       "vY4/JVu2iOqST0mK4T8imgsq5R9wHZxB8nfu8lK1xgkAXFsbYDe+Ce+D84BM+BGpDa5sxYwVbQ1S\n",
       "eLaQ1FD1T5WEbB9S6jpjIPMeWG0ZFUMdH/JXlfklZ7PI364JuFXh7eYtrlRkaRCkzyCGsM3Ho/iP\n",
       "HYNod7LsR3iUd29LMaCMyN9YkVCqUsx8L8Lq/sIWn9/Ik/yE46K3HLF+HBUqd6I79Q/SmBg2pWzw\n",
       "G7SnUWdaIf2a0pLoX77gy4T//BO2yuOvY8mfPIODjOJEjmOdm1NF30xBmTm+SD76eT4K9jMlhwCQ\n",
       "bIwUdh3TW2b9HBE5qyjzsKUZ7oR4D1ifjGtj+3BIRyrNvjfpl2fs5vnWHEDO1OATcGb+gpOO4Gyg\n",
       "H9V5VdeeLx6H7MBQXWUpA9fkBPQmyROlRMJ+/ZM0Xj2Vxy2TAUk5hVkq6MgLRJDwYLMKLATp6j0n\n",
       "BfF1jaOfnhF2VdkdupB/AtcW5Hegyv7/Dy/V2Cm2VQqJrgHW12QWY3y7mhm68UPqQh1XYB6iKmiT\n",
       "mgA2AcxPnZNdf8ooKkXPQlYBOGd46B3GrkqrQybgNLHGB3rgl99qxVS/QmBw97chw+7cbpJ3G/qm\n",
       "LPSBJRFjHZbZNwPykQq2xAIYKlQtdyzJ6bO3+GaDZbdr3+5gUqelCnzOrvj17PCuix/UiEyv9o3S\n",
       "ZoHK5qNr69Y/ejh9zC6QmJGtPUb5CUMSQLTgVqiANwcQ6bh+3CRAaklNRzHCo1f5ra9KjA3omaKP\n",
       "TvJLP+nBu9F4vKTLgpnhgT3c7jw4KLX4F3LMP7DRDrzUIWg670WcmBqr6z6CDEV71e9zU9BjCEnq\n",
       "Ifh/SDE+SccVHfS8Bsh8ooyx7I+NlwQdDvPB95DPhUGvdu9Uf5lSHrkHBItK3bUBVRCDF8iCdHhI\n",
       "YgTe9xjbdgRIRZsV4AZn+6EWF4tNWx/ddSTEge0lf7Iv2BGeOry6GIv+faSA7L+6pesfQRbWvwZt\n",
       "k+VIDmkV+fNDU/x1xSD0IZqt8KhQVAmm8jsaqeftjzs1Xp39XsSFsjSuBPjIb+ZonH4J0/GNtAiu\n",
       "RRp85jckjA818Tw+LSw2SEFICjvsR0Dn/9DjUyU+PoQTCdxLaFfdLHIoPg1IMKDGagKQfB2no4uy\n",
       "SgD08GeINLVU8muaPQg3fQVyiLPyf1iK3iwuDsEcQryjefCd1tCvQj3lYQMZIE81RmssQFffIif2\n",
       "u4nLus3lcP85lSMEqTun4PCtPPwchtdgt+QSh+DTetLG6ztjTy2co1ceweCXj5px8KAluu4LRv+P\n",
       "szOEjL5saqxUY6tdXzNVv4Cj0LNnNYvRdbAHGyZdO8BaKGGmPBWwZ8cx64DyyvGNmQ3OCWXpJeMW\n",
       "s/VVIyLEJr0T7oRWTBuwkRX8Rps9BjIJwZ6S4zDPIdINlT7BYNNjEh/dCFDZrR+dnQt35G+5JFxO\n",
       "hCN7rRXSDkJcl7kXc8xyn8vyHSzxa1rcKbD4OwZ91HzCaPBalAAnekDiOsn8nZKOmGiUoUpw6nsk\n",
       "5nnJVuljzKjN9UA6KcPy0WTVlpBEz2ls39zDGAarvfhjZ6OG5x8wmT8zp/rG6fn2/Qv3HjithVLL\n",
       "mYqrQBb4Uw5TSNnutpFl80UwadB3CFrM0Q7kZYJY21JMmcmBCTFeiIUnMWDtgMCTcbyT6pgWQAr8\n",
       "5RxdJk43If3JJyWNFrn4xrr2Zesltmp1QFjgeusHw6wU7PWRTq9E6hunQSnX/OqsX1j5V6wHWkIp\n",
       "/t9E86q/JV6EtU9RS7Etmt//PLxKjYmuwdV/RvTcUGMgTnSb9C6aKjwyPeKgI4rtWGAngdvGE6C5\n",
       "yIcLCDUdPsqJwij2DPjewqYqYgB+/d2YF3ObxcgYFRwKsfrTDzd1kYCeBF6cs487t/Tnqwyu6ZYj\n",
       "wIJnPFMJmqCRf1vFOwtJbmcR4vedJXKHthvcTfs9BpCWodW0PjuiMwZWRmnxPe+fmlKDpzsTlkT1\n",
       "tT/Rgmy8TvL0wJcEK8+erDR5Rzhzg2xYxg4Xz/0IRvrOoWBUdMB+dmwhBD/G4mS0+wAmfjgM34Tq\n",
       "9gUYAXiRd40j0BDN28mnVd+8IrGNunXHuFyh/YLtr2jnwUOZv7mESzdpQOmziFh+FKFU+c3S4WU3\n",
       "LJL5poOap4j45tN3xZWVFIOBpL05jtOQsMxqMGJTKoFoEW18DGnHVDG7JG4BVCZ9rulmUpI1+9af\n",
       "ft6d7DK69Vipq19eHdGgNAnS148F0tdu48gms7TaKFWjYeEOKMS5IgpINE7Rowt6+qPDmVd6iFjO\n",
       "P3t7LMvglC+GXiuaQgPZmWhJbF394VTusx5aaBolOIbEuenLLOtwQM03RTEIDcwj45P4u+fOWXNW\n",
       "wkVnwbTSFHEv2kXEYBhLoz4T9jVhTRisn1liWnetv3BUQby0zkS8SpUyO2eT3vSP8iKBvQaFFoqL\n",
       "+yyFbJDM5QsKVnyRQKHQ8/htiv6ckoKTg6qXsguoF9522v7eJZloQ5axKgCcaHXASPwBDqkgvAF1\n",
       "smbQmVGd78+RQxje5A2i5x/xYMK1Cl1jwYozYbkGMZT2ILb/ZJJeaixalbv/N0CH5NgeYWp4P3Lt\n",
       "ak9z65/R5Uzhlqkf5HP5Mor+t/H0wOuPg4kc1fC+987ghQhrWkVNVW9u/GHUXYDOC3oSUfaYj3Ml\n",
       "JRcAWqFVJF/fgdWbIoGmEOu+u6ZXQiYZ7vf/vs8gU4LOvxXDiXWSvxKIyZxb2BGi4bSLoDn+lqLJ\n",
       "zoxNNrqLialItGAYH2Mqn5aVtzb5EpDZFQ0ZTf77Z/S3rEwA1WrLjbTIiSql9Mfllra+TPuRjcdR\n",
       "tL2r10k7G9vEtoCSWpY5ynow8jpAQmGzhKtkg7XGtDCpsgPdmuVDsjl43mVFqJ3Ni+RdOJ0Ks3wE\n",
       "WR2DEvPADQ3tPMbe5SInTkotbIFOP6F3GNvrlcYPnTICAxMCKrGj/HFktEsKvz9IOXwBzyvgg2K+\n",
       "utYBj2/+VPFfTNVHkTFeMjqz6foZXWPJNqNJ6MMKvNAMXMNe1H+tuz8VyN/cOn4ev4r8pikkNwpN\n",
       "mQ3OWOWT9idKYP5YEoZ8OZZuaTdFspoyhlAjT+v48ZmquYYWj/UG3Hotz+K0Y+5H/kzYJKjURhfR\n",
       "b+GN00OflfeBHqqJDIsbOOOxNPUIVipmRb0aRGaHP4AGYaiK7VRFQkznP5HdLGSu323XxklfjqVB\n",
       "MFj9J2KeV1HHRBKjmYJGl591V4cswBT911/YJ1t/u7bnI68NobVtbWCOZKj3YNPdjejjtLsJQIEv\n",
       "+X/Cc6Dl0E04/QTdXZP0/4F56HceLP5tXzAQlcwTG2LV2V2XLbDmRmmD83uCRlCJVJsB6e25RPQB\n",
       "8OtDk9DsDukIiVah4RmzzuC/jFJbGVDYiclhFLuBxneuQ5DMSLkmQLOuw04pvZEuBYKvv6B9LCvt\n",
       "WVvowYx6P4kB7rz89w3a6BWtf6fXi8VkDSt3fcJXNtlgknDl4Bg0Vsd+MToJjSLuJYboP76iFjS2\n",
       "gEncPNhQzEI/gOqRmVOHJB0H0ztZ2I8swid2MVXTZ2dUtOHuoROCVrvrYAoXHcyKR/yad1ReTD0i\n",
       "MRN6aNcRxFHc6ehvcEGq2Vwgnix9pS4hnuM/eE9MzzeGr7QX/bU94ct7k/gg7K6gCJn3y8+vQ8qj\n",
       "T0+McIXV7k9ZVuP4bgDdJ5bhKev/uhrLDdzhxOMPCo7+lVOSHQqfKI0ejs6VgagZpX8E3BpTZGmx\n",
       "MW1zJ9zNoNYkZWWmaEOtb751xdfwWFfc2sbXlJsIQyhfhfYq7BAp04I1c2W2855vfniGV/T4+NtQ\n",
       "okMiF3e6rd01brMGI/VUl65txxEYAQcFzrYHd9YrM0JsTiMDQ4f+wcP8Ku+1egRb3l4e62205VP8\n",
       "sTJbUt3nNX6hQ7ra2Q8mQn9Cm6VVS2mefPAOR4FPdcjaveO4Dzhp4WAzwzdSgvLOVODz2kkENNDK\n",
       "DCgO6C/7+ef3nhKmwPZDQC/cEamHu4dwoUdCTrRsKIT8VVk1kZsJB2Z6VFBIej/Q6AUE05bjb5RB\n",
       "K9jz09JgoJpGO4y+0le+Pl5scCy/a0aHmgw+fJd+dfR78rcwKRmIEhjMaSnLiJMIQENNYwvYF6VM\n",
       "1eRDkwVZmlXpe2oS0tGuoEn/l+FCXI8RUpPbhzvSUkbbz1nGL0lBxOHlR9ThwiqmOMfCj9yTVTmw\n",
       "APBmh9/ylBAT/O98uVzqI7cgwCdOIFUTz2fWtAQ2xab/8j7nh/BZjxm+7zXITGJMbd7bMA7FSztn\n",
       "i/aujjfYIwI//FChXuxMNiyeFX3JbJd2VhOtEVN45zu2YeQWJwnRdGkBgHd1Z/6P85VKSAjnPmV6\n",
       "KkWCUsE4hZaLL+aPXkgZw9ukWBrfV4NoUO2V+jzn3WmmoEZ5KJncWPu0S4WnDb5EH2mtu6ORGwQ5\n",
       "Z+q7zd8h/lxVf3gle+Cv1fSXjaI17CQkbctRcJc2qgB3JZJphzwHxXENgen7qLL70BZg20dC5YCF\n",
       "+WyV+6Ju7WD/wfhWLU0mGgiK+Qsk3ahy7T5r2niHtw7E/e/2Ml+xzRLdl1bSkr4C2dj+8x7zANOs\n",
       "4XPt/wBkkvw05za54EgbSCV3YPBK9DOEoUzgYuwE5xAXUaal0hOEIWBPTO/RDIAf0YV0OIN5SCj5\n",
       "VgdyhhEkp+iCijF0F4iuZtG7TIrW8NeKndNcx51J/uTP8z0MZA5KZZrJPX0J5jA1RwFfGg2ZUhn4\n",
       "SzJGqnvkjoqxue0tUXQu8XI1UOij0XnrmZ0ojf0GuakwRmI/UqZYszpXx/WqRPqAyuPgGtxMWLJw\n",
       "lm6mZBkqyhQIkX5H304o+g4JcoNB1gOS8b4qo+h7EBmCL1dfaAjp2nho4qZ5zd0LAGvEifijTWov\n",
       "H1xmZgj9nhoayACr+ueVyKUr0+wQuLphEzcXE9/10l3Y4NHQIIpD9LBrdoPi/wDBPQ6HEZbEpWte\n",
       "Jr3s0p7UMSlNR81eumhAXzlAjPdk3B6Ah8tg2XC3AJvZQM0iQb9qkHzveCLu7nOyJQbxMt579/SV\n",
       "WXj/AKHhH4vABnDNPXrh9z20NVpXvzQjH2/5dwKNMcqH+RSzpQgb2RYLSKDSkgwiM/0FRyi071rm\n",
       "9yRUfNDSOJVpMDg8b0YTI81BqNqslBShXxzMKAfqcQhS2D9bXvgMU+J6Ne37DMCRhtROZ0xmhg59\n",
       "aO/yP2g0gbltEoXwRTBsPNW9fdcKlUue0+p554iYRk6R1VwTzYGGnjoGe+ME8NLg2ML6l+Gq3YAo\n",
       "9R5P1p/J0yJsDBnw6Ifjxhx+dr6BDxVVc/S6NjHeH16Vcrf6RwFwM1+4hnsTVklX727GvcNDl5kh\n",
       "SyMY2XuuQzAHdxB0Q7M+mTa+isgiZ4Zrrk5mdmn+BWeTcHnhD2gKrE9L2UgtxbM2G78U0upQIB+1\n",
       "g5ql48u8rp9FKtLrJJn+aJ2eyOgYbXNVBEUzy5StKjpZF/hsvqKTGqEMwXw5zsgTWXWhJHhKb6YT\n",
       "Lq594iHLfglvSCWmjuBAxt0/MX5iBiKsBXINNmdWS5qzJtSilWpLrs4fJVUmCbdfZJlLiNU2UQR0\n",
       "baVJr6hT1vZVpjTmFrGZ5BcmcKdyqI9p8tfejCjbuHysG87sFsCyvxs5iakZblmBPNS/W+vmg755\n",
       "aysiqtMgEwwTo1Qx+DAMt0gAxYEAADdyAZ6kdEEfAAA8sUVRvYJ89awACXxAGXAPk/EYzZyi1aAB\n",
       "ALhV6dIaNd9UlkIHByd+oXvD0ztb+7CUMrr2pqC45m8EtisAiRSzL8XtMmRk7xm/f05/nKTnJYfj\n",
       "ve8+C5Fb7zmYzwvhciIiGvUC/RQuX4QpcWzYmaseMek5FQwJPJ+vy63UyVCvTlQLb7AvTd/IS7dn\n",
       "peVIwFiw9YVIVjtOhGqicAnVGuIrI9WBbdNk2Dt219d6ZfCrV2vyQ9yAj0O8YDPVW+bHmeD+2CF/\n",
       "AkTpUx89NT4J+TJ+j/GMAVTLqZXBrjuWqHE1cqiN8xdAzLgC4Asc5m1GPTdVBLYmT7kRmwdqqqsq\n",
       "vN2csAZo2opxcLl5XGt21WptcLFu15FC4a8c4JszWbDyp+FiIml2v+Bfuz2SwtqeiogcMJ5rGmgz\n",
       "5WSVT/EQUF7G50da97waRt3sWFRCkg1BXMsgyJ5wNBOmhv+x2yL2/RLGhzNXWAmpGC/Ac+owAZ3I\n",
       "7FLAvNaNu1DxpjAWHl913SuKJcPlAXkl6CBbCrIWedKRW0UZbR1zokOOZ9QlMhGb0t0beHFoISO4\n",
       "YKNct72Mo7JEvF8HHNS14kZpG5EoKgqZ21DP0biuNZjTKJRG9VAawKyW4cLdb7ThO2/fXgKsGc38\n",
       "cgMun5zxK8AvoX+bg1Nvru2vpX+hK0KdC8XmPAEoWCQv56PJ5trtSariQrDPYVxBLBBeUqNCAi3t\n",
       "IKiA3hEM8FgUuXTMGbg8A0S9KSEUAY8IRQLOdui4923m+vWIkfRbQ+rEaloRe10okiqrzfDkqd47\n",
       "9cKur+p5MILOVxHmpahm9uHK9wAfUM8uR0TiA3bujeRhRjwUmkOGC0mVyvz7UpeYo8zhVkL/dwNH\n",
       "Bzx9a4lryZoB7ipKRiTw9DJT1P18ydX38Xl/VQbLHHtavpjk1tNBNZqOmFyP3/xFlkEMBtpAgzJn\n",
       "QPin35Ft4XomolJvcK/rYQBQB7aPNVhi+Dg4+5e6hO9ZMGvTu07zfCCZ+THgN/7UtyyDqaAfLXND\n",
       "b1HZjIer8GvGdAEKsF1/vvHVLAJMBm4BywI3g76jdUwHh+CCkNuHre27SisO3LzQlay8oL+MT0Xd\n",
       "Z04vmmtCuagH3jSXfKvsviXRpHk/DCnCq33mS6ADPvgitzfs9EWSbwQ2pOG7Psy3d6OxH9FJiVH9\n",
       "0mivKmnCJedZPkTiQHjOZd3dlFGaDVZo9SNgXDkqlcqcA0qYyJN5f4xDlIp5fc6c29hfPzboka7U\n",
       "OE0Xd7Cwlhf/PND/9SVR4wEbVfHxp+2KXzP0pW6Tv7Mp30ZQAbrKwJlylhXuAdCl3t9Ywz6U3RLe\n",
       "e0LIwH9ar0mBZnTLKuetCVVnTk82Gj9+df/vD1ZmydWuKpdfL4Jqa9rZoalQ0HJiFQy9rguYunqX\n",
       "vmW0E3JJhuXEIjAHzVIx1qeZfaeZAZn/bZih2eVf/uJluUFBLMIHg1nGeSQuxgBM2emyIWh4x/cW\n",
       "30aVR0TTP5D7CyqcSIjKMkix4zFzdDvt6gD+k37SW/uy4Ue7bl8KtjE2RrGoZMTH/tPgRwcBi9NX\n",
       "FuqSEpzTtVdlc6m8g8A9v7EwkKWR0lAaWKKXWlCijUBK94yFVaJCZwTYvIqf4h7hG5pLtdOPeAsq\n",
       "L5qTMxudS9d9bxfDXWtm95yCQUjHDgexN+30qXKP4xdFlexIUX7I+O317AKF95SdA8+DUjKi97fG\n",
       "XHMGfOrE5XvxAGZjwdGgeib1wdPB0vD9mGC1CYxMU5/3LW7wTP9lzwFyVfmy+D1xcGXoDcXXy8gS\n",
       "pZLIpO8U9HBIbINumdubI0e1ZF0/7t/CryQkJAG9odYGdi+2oQvIRO5MKCuzB8jfcoKaYV2rAOuR\n",
       "EB1rn99jo5dwl27bLDPfExOvFDhfR1XSiwaARrsAIPmMtLUTJ4MEkIGbW6WsOrtIk27kMm5gRpK1\n",
       "W8r6sXJomEp8G06IVKjkFVhXY01QjvAxg6+LAvD/ag21Ophi+mXr1ZFFhUQ/7+Ou0VH+AHJywkcG\n",
       "wLUzj4siHzqMeKaMeo7nCfj1iNWu3vSg4iTQuLKeOO/FU53azhllvlEUFqdU6nJjrbhMqjRqLDlx\n",
       "zTIlZL2aiDX5M/FeXcpwUTkc4lC/RiZuXBGLq2y2FD2mXwRYynLDLfaFTlvloQVNDp2gwk5ddM2L\n",
       "BBPmH7rYlP32RrFVs4NJAvJLFbhUc5uGtqS58lD8WXXcZWY4H88ZAaLhqQRcL5xSj9/Q2CYWTGNq\n",
       "A6OZ8zXfnzzdWhSdkhZF1Z7g27S15rnZPrRzGXUyr7s875/YXesyOhvigQgbs1ybNstKRkxVwNhn\n",
       "QdCzfLT42h34ZQDwOn4Rmzl7QBspfcHBfG3X1MKOfrvd7rn1PyhIT5BaNaD5Lh+WK6tespEr01OF\n",
       "P4g8sHGsHbYkoXv7ns1DAgV5q2IbFBy9xOW7UEmWR6vLyGCrzzNfYdO0csgj98c2IvCSjD5m4N1r\n",
       "wdN+39rGpupPsQemEQ8Vd9d9JrleGVFBFffP8YvY0Df6i1MxD6UPVJ0dN+iP1s1wRf5fz47U8R1G\n",
       "jfnnEDu0ubpqo1qWPWJ3iDaM4j/p6m6VwrGDYrgsIhTgMUGJ/YvZXAgmoUIOsNTNwARfSk0ZZPb0\n",
       "Botg+QWClr3az8oKdcaRnuPUQBZI09LFcrGupvCeXbyTcDwJrndqvZ/667xTgk2iUtX3PdT6NKt7\n",
       "EgENKD9Itabg9qVyhho26wgoYf31ZDcER3hqOhrqjICPe7oKfbdOX9WiK+Bqt8ElwSGXmrgQfoTZ\n",
       "YlwOGNu0V5KQCzRaDnMwSfm1Do6i2mghXiZ0zKAjsQjS6S7PffbPXontWyQQwHMwDgfHcnH81SGr\n",
       "gGGdYxzIXsExstrneWEIIPVl1MQOM7LD37CpoFq4bPeEliwjSk1QSE/df0IbccHxkj+nSjaVBxdt\n",
       "WJq3Lu1xjX/zeA1HMcUw1vuVzgIyq0uRGC0c6+SGS0f8arELdKZ4y1pPGlZOOLwT4AZ32PCb4Fsu\n",
       "3U/TVlttx9mjT1LVKiBpVFu4RLYVgo5Z4Xmxu4k5eJva3loQvqX+ukw6wMMCGF8wfpYMDUOzesYU\n",
       "0BNXA5YYIzW5ClmRGp95XgT3Dmb7xcIyUpj0kKubKXxYp2WskC1l7SLbj2vwnWWyZXz+vY3QJhFw\n",
       "Ca0t/+BcULFW9Fm5uOaYXjF+Wm7U7Co8rmcvhRXm4RG0kJdN4mkFL9Zj9hl+VnVq9+w39QdTcRj8\n",
       "78MTEnkTEFQMg7RpkOEZDO18A5Pb+9VwRU3MjD2coNkFcnj0Cd2Z/YTLDZC7jQU4vcFgkM3NTmpw\n",
       "TPQLWPnUmzTlefcBWhXFlzJPK9hCaxVCxGhv+25xsMDQSSR8W5Vr8JhwEaSqZePCghnWG7+2nIbh\n",
       "ePMHTHhL2z9A3lcDjtfrNMTA/YRg/bs+2Z7Vo1BzYF1VALHA5004YcmuJBNFfWeTpMeLONxlI8Y8\n",
       "EWWKrHIDfygNZ5P6i2QSnKRXujLdYAGwGFgee4AEFI5WFsZnz4w8MpNFhBd3o9tcwIfy+XPtrXZ5\n",
       "Lgtyb4mx1+oUlkBN1ZdbLcJqQ//P1jL9eXU323dLVRxrplt7vpDmsGl5vnUlbmg75Tg0ieY+Bwfz\n",
       "/rNxpOaO1rIcsb7Esi4Hx+phNmcSZQa6OKnwPiTIIFTy1vwjomoEQQlbm3SF8BDlQLub9xDIg1e/\n",
       "ZpgZZlgko4IOhatoxHIDDbNy464j8CuZFOmdc4U/C+/PhWbsvZgA9ZfNEJvTC92akoccHKU74LIy\n",
       "fPbwubkBIT/o79d8rndVPK164D4fABzTtxxcldMExmmdOQHmiDgOrL1sD704UEAhvwRJUjFp/uRs\n",
       "s1fivY2gb7d/FmadMRLove91V9D7iEWBWomWJl+HGWI9CMfMHRMI7naNrlZHOrVbp0nkLjt5EP8x\n",
       "7OsSV8C66vT+CDKSXpyNJEEtFaQjLDzLVMgFqEO9MMvQoZoHw7CqgU7x26xxm2lykMzq/jbcip9M\n",
       "cgt/bjzWr2DPzeO633YYUp9wA8Se0rBo5IiSyiCz93bkiusLEkc5x7ZBU1Pc4RvW6axKc7BydF/J\n",
       "eZX31mKABNBz7X5XUYs0uJiUu6DLvIun9tz13Ld8/SPWgGbY5z1zTRtkUu2mj4Ms39GC34G00S9M\n",
       "kwJkHfCXRdmft1Y1XgBywMAPYiYaIdBFzj6HTpYgmRgavG6CE+otiB4NiMw3CHVpSRvTK9U1Ctyl\n",
       "g+oQfrGME4fejsLmzBcR4iZ97b/Mi0RdULX/pZvfUiRF9Ms8TMdsryi+3ZdrObfmrQD1T8pkz16L\n",
       "nqw1Q5V+pnabXOdQ6wjJb4hsgou3oyQL4ObjWRbNUxyGodNBmj8Eajx+/Zt59zj/6LDWFtKoRkpa\n",
       "30hoXlo5W29SToFKVkpxdY+ypPXKc6xEDTYHpC6GyDrffK8+/ja44uZtTGgHAddbpHLDlQK3gcuj\n",
       "B9sHya4HUj47hW+mR3tetVdckX7Bx3Ub6M7QiVL8QeRIAFqozuLNT+yflOQj6lJbc9voIjJY3mqd\n",
       "/ozQ45ikS3YEEtKeNc2EJNyb+RUJANQGTojYirvAWWUmNXYK3nJnS6KBrjJIwMF4vUxyS6mIf9nI\n",
       "zYx/gRKYSB4Bn84u3dHErIImUl7c8Y8nk76phiA7VTKnVBWntayGhsFoVw2UawyyZLLHNP5abcPN\n",
       "LFPDJ04mKynmykEr3IuN5CLprIRkkoSWYyZx4TDTONjoYXDwCBdS0ulzpDFebZafftR/wnWNszvh\n",
       "qrmXUaayWFGsQBsS00nsCYU0lpB4EzyFrjy9d/QMw8Bp6gcAdxqj0L8KYnqsCkFs1N2aatXicKEz\n",
       "cNnIRhR9jktVQrXpsx3ikPqZuTKOew1JTIHuZpWSsb8leKxhNOpW3+lkhWQ8uZQeSJuL40m2PC1J\n",
       "dXE4tJBIci2NC1NcLQm97hDEF+7EoyozkjqRFVyysp2g9nCFgLz2PFK4vhcu3b/svxl+RWlSgybt\n",
       "9A1If1KtzUqI0iCcYXmZ7lhn+fNxfOh9KUGo/rINsjD2mtx4iUR7hnC7PZHOHgMgwkABo2VxnUoV\n",
       "L7EVMKIAUKA0B2whwqbxV9TlTd0RgQTyQw2dv5CTbNyGdVlIX3y6k/jGoZkYctdDYxSNs0MTJE70\n",
       "bbD96qPNUIsPX8gR0y22tB9IzYnUH17nUPRYtmribJW0R1uZtK8qjlPw4JAMsomT9h0as1SpdJ9Y\n",
       "guZX7K2Z79GuXCSE7wfOREgFS1FWsN4imk7R3WGQg4fQuR82IYsgtJMvw1iITlQXob8st9NTA2+3\n",
       "/HWmiU/8ddZRJ3ECEo/4Es7LM+x2grvwkRUtnNhN40t/1Mdu7m31oa6zuxUdnjpWUZOSHP1JSWP9\n",
       "1nY4eT91Qjk1JiZBUNQXIWgZ8qEY1kDctSqWiNTSqjUa76iAB4ncsYlZ/eaCOhWfoIBW+eQHCZEm\n",
       "/vgI/wnXOyz7TM4rS3qd5pj5Fu/yUUBIbkiGMS/6pvPisULYqAPChBkDjaDla7uKm1d5F9j7x7JA\n",
       "Hgqg4m0u4c17A69ABFrIlS1ZQ4441h295ZDSJT3Oiyv7WNrdGFAiPgJJP+2zJk1fouVgvdU7aaK3\n",
       "N6HWpdm9qzH4ZyOki47MEal1i9gYltvyY1byUjYTbd5c4UTMBpbNQr2wh6P3LkFqXKAE6BN+eljH\n",
       "atXf7oxF2WcvJnCdxjErOR5KvWa0w9P267meixlcoHihu86sZ/l5ebkMJYy9xUd0AjGwrtEj/3WM\n",
       "qfFlOuZLkfBP/letW95SQpFass765JBnnJ/qOSJvGDNo+EWD1yslmw3gNpKvyo2bKVhjEG3hm6Ti\n",
       "a1kW/o6uLgsEid5f2TdIIgNfwnqT3fBz+MgSa4qZv3dqtMp1yc26yhqafnNv5hxjzK4Jafe5vsBd\n",
       "luFqERl2c7SR/G6QYkNeZDcbTBux9I+XDpMLRWWjXyqLduQu3xcqJR1T6/wOadjah5sjK/CvbzVt\n",
       "7V2UKfaSJCKZXWKO7achTAWLxsgC0RyvU5nhN2nVBPHGLyEDcjtZ8oFZorjJ4zR84veJkzQrUkJd\n",
       "hwwYuQ9qIxjnpjGE34BaLlJ7vzB9zpxDqbZFhXvYuKlIcfpvFgI8SCnQqv4IJ7Iobl0SGg/kx993\n",
       "VELB7AJPTnsEM2/H/7sDzyivwzsJWVRVVn7oPZFmJCuNsSsTGcW8RLb3PqX5nnNVoYDhcdZL5EuW\n",
       "nhwZwq8kqwcA2u0xYDyXTRx5yBhwzpf03fRtjcFkDvE55I4gZpDsqN33TyjNuy/Cfd5K/+r16+yP\n",
       "7vwg3N1w6mzEUxNiyTDz2nILAnqwE4mvXjoJAhVaQ6PMqqnoATO08lFAu81xaLvTGC/pm9IkZ3y9\n",
       "pqVQKk/3S+kTFg8Hy/9h1johsCbLeEQB24gJnIjmZlzQYNewBsF3djvKAu8GiUIFmbQYkDuvXujC\n",
       "VUDJbpHWFdamfeGWErlL7OMwgQKlTPsUFAL+NSQUM4wZOjQwaxeKXHPSFETY9r1ZIP0DkZ3krfqR\n",
       "GLaDoz1EixTvXIfouLER5PO9BsbJmoslmmWvG+Cwpwy2dutYWbS6Lr8ntMqrkzp/f5VpVIBJNKoJ\n",
       "K3r3CL8ht8OWDu6jGmd6JNKPyun/cSWxftrEsCLb8zedQhpw90Be0vserBCxMypk+lkoxW7SB3B9\n",
       "jQITHwMui3kFrEOpvPqxBeGFFhb7Xs2Q5XNGXHjWxadCtgI9PvXcYRWkMTI/gpIcK1xST+16lZ8+\n",
       "2rF9BJ67qTWurPqaWJ//LGwaOi/BNwRHgTw2K62jqQma1V5uwi72QYmheZNWi6wIiHUnwRa9rXsi\n",
       "BAKK6FklFSFlyhROgO/8VooiSGZi+aSP9hg7qaHfj2whYQPrqBdUmWxZT5iqzDG31PPljeUNLijg\n",
       "XWV5NB6vJeovKiY0sCTiyaS0AHO6FZP+0ZMfzftRTv15T5GFf3tzLpNIFkdBxUDpnSx8yh2ExOkb\n",
       "mKqsuj+CKZYfMTkiJxzJT98nUg2JOoqTM7c7hmNkHYME6tYtYL3kp++wqrCjOIPfpDC3k01hvvwb\n",
       "tQR7qcOOwn8Lgw+2fC/5NnpmbgbrCPp9Isz7hPgqahUtBO3l9kvfHauSMkzY5rkl9+PnaG/WIuvI\n",
       "xoB5s2P9Xl15iOsrSTFQC7jvl/0gURWP0CqQJ1kErSdnzPRNlpsqOFd+jR1++PWmHyh3lMZ18e9T\n",
       "dXDUw6HXMseoIhNitrzAHJwBPE50ls+2KnvXS0YBgB9p0tPsmdYw3nQ5LK0V+pFFzlJisvcJjEzL\n",
       "bRWb4wuDFNJstJs62FmH+4ENPJRZTJu2UPmlZCZHabjJ9UgRpSrClVE7GmitM/N9fTIpTb0n72Fi\n",
       "OV57lCH8wTRcFX1t6F6hz0uzewXaySKUVjns0L3I4kaIam/xtJbv4pYh5jfMR02FPFcuJ8azznNp\n",
       "OFomlQs0vrwWNBxvWBlEfWamCblLTmo+j3YjG4ZXGgiftKM05WX+UEBwDn0nWu+WhHjFXmVjJUES\n",
       "8mbMiRvpr1UEmKrDZuEdqG55edsh8oo9293VLxRfsUiwIXcNVDndIvfvlFHvSk8ffYs6In1cmMdi\n",
       "aN0avjMPNKwE5tRm8DAbgQuZUgk3vRQbSAzD5MMDaxSxuDxFD7yhALP+qWas6Auo0wQa6pjfsVjf\n",
       "LGN3vKv457c1ANAdVyXtvAfOYn3P4S6NRffXVBs1WiR2C/6kdltBBj3vW7po6x+lcTDOtnswltlR\n",
       "+lYeC6uOWwd2OdtKcBOEfZGtD5/HV6AJCw1bl1v+ZHNlS5r7emVBsnGGBcuggfDzWnJyb3CxZTVi\n",
       "ftLQ1LPNXj3hzG2PhzbI1n5z+rjNMg5HyPCktjiTv5QZ3ujTqXyfLc7ZZfW0Qf6bfRkCoqOttaVy\n",
       "aIqL9/iPjpUYWIiZJU83LHEoGu098+IVr5GVhe4ZsFZ5F0u7vDy5ZsK7j7OJ3EYMugLCaARO3AOj\n",
       "DynYin0t/boWjnKRJ4M3NfWgWt5l3JAhdcAIVTR7H/i3UHsQ/5HEVnaZf31Fd+h1tGnwoHLYxENa\n",
       "kRosfOAxmG2zm5bXfW6MM1o7Y4tfD2c96rJOE1v3SRURcpEwSNAGJ18DJOHV6LcSChg2IYDiTVPZ\n",
       "U841R3MVnONfy8zqVf+MwuU2thdeBQzQ7iqF3n+qpACjXDLdmFZZ4+zOQ+Gnk9h9F+gvE11b28Ki\n",
       "AGh6emoG7Whgtm+zzH8sYZldWwwEbS4myM3i52pIIamFzca0aH1ol5HRd7HzOoMo938OCsq5o4cc\n",
       "RQPsI3mSkcCrGasubU+rfmkscC3p8SvM+SsTz4b+HJB+HEWshdT8gOfAcVdcq6DO9meCADK1cKNH\n",
       "IJQecBTNo5rczZFiuvKAdLwLBvnb30gUffcmNluTUdJyW7TP4Mg7g6rFHZ6rdfjhMqlGYoPZ30cw\n",
       "0DmKrWygh4DogL9obQfrV8CGGeWtLm30WVHwl8M8TY6fjt9+EW8IFZl3UYO+zsj+0aRKBKdyIPRF\n",
       "vVIJqBBBC+w5qoBZzjlo7zKVsZz5AKXAhI6cqBtYLa9w/BkC0EO94q2SipX4HueYK85J566HL7xr\n",
       "LPM0TAdLEXkI6uJCbtKwU7xQ6cJX0BSpIDW/jpjsYOl+hDhwgO3PfFwsxfUK8IPWCj2VoV84EWwg\n",
       "727M7UxUiXH414l+K/0coDxY8eIpCVKPgxy6ovUafS1/rCgrT6oK9avS3GeOp8SSeVdyWr8GF4ko\n",
       "xN5fgPWLAwG53v1tHgpOSIibPFK/GxDN67YdTI5t6HwfHBfXk0hqnKxpDGEikAv+Lqq/0o+YXRJj\n",
       "DhzMfta/tAwNMfBGi48hSyTmkS+3IsgecaSuyL9oGodQwXS7yrfW2fgqHJQ8teCPmBT9CKWMTRxf\n",
       "2WtbzFILu8KqAwvrcoQJDi8nEpcw7ChV/8R/ywwl4rehvVwxRVcq1sezwhbtLMiCv1ZW8uWhIcvl\n",
       "FicuXwQZ4NpWPJkCPQLnBfSzk86okiGHbium1LTptrQDUih4HQQ77zpFSPMpJQpKTpW1wJk3B8VH\n",
       "ATanmCqDLuwDTp5B6S03U4gQJOjwSAy54FZLlzKyCMUcOLfiffcFN2hBTt/p2OTGWGZfbbJbWkYZ\n",
       "ZOVpZO8zQevnAcfR523sw6XllZjeqEokZTvtA6yzEiiCAfmkWQ2qSM1pXk5IZZZkkyxriOwdjfbD\n",
       "iNBA/7KOh9HRFJuySyArF2MT82fNaKz9NjkVBaiBS/UDaVIWn7498x4uuMzA9cwEGBBAG8/i8IGa\n",
       "kFS5IAegWe/5SysYaw58GIRs9oow01pGGp0oo5ZE6nlWs0HCyiEQ3R5lTjGtLDGu+1OsaEtGD5Rw\n",
       "RSUBMe7E0NQF60nomxyVMrso4aQz4RNr/ubED4LQ/dxwfbn8E+6iYc2JyrpH+W4Y2BWP1ZU3Ht2g\n",
       "394x43fI0iE/2q+hepf3/C0fWjpYScGv0JlOrFMhEdMPSa76vFhH6FaM0NYgHW1zt8zHa4M5Q28J\n",
       "9TDpZ2DPUgBWoVNpOA8CE01UaghWLdEU2BPiqiMDd2JJjucsz4qsd/r+uvKEX3urm326jkGxLzCq\n",
       "ddhh9DPZFjTj5rJobWDcqBwNWINJiGJxKDpbzS8R9fX4LnITwNpQ00wUn3Ms+mogF/K4b6mwk3lm\n",
       "pFVUX0ciyFB16un2tV4HHni6hCdWXAiYNbwvbZfEQHeyAhsjzVObDb1TRKU9Ve/df2fUE7bufKyo\n",
       "f6Nr+d6vD6kIizePRAc4d5RWfCLGhmhRE1zzQUiB/41uYqtMNndfHElSUqh42TDvQcprQxH9rqXa\n",
       "8ZmQSa7arrm3fYos6oQm4WYJqn1e1Pb+6jPDsnjksOpFUbHDXXRmZ4ADXl19t5m41ImT8DSx/SBE\n",
       "21pEW4U0V8iuxhSuIYdVdlF2L9DQRUJril/qLEkt3wfZNpcN0P4apepcCnHxpHFYixjzdVYO5vdi\n",
       "X5hDbAUk3t8/4jb3on791d6seJfHsBuUrrsDbhvzggw84L+22Tre3IsfZ6XbFcz9nk14GP0TzTC7\n",
       "uf8v3dUPqqioJ0/zck7O13OaUR+kchljcSzm394P5/oHJ8OIvtp/4iEBRP2cqyPn+6VtufZTJ5c8\n",
       "CFuf7URZExQhPHick+uVKlFuGKvWyIDP1AN4OUpFs/1QbkNF+U0JHlJgDTV0stQtKxQJUaiJfAwv\n",
       "K8Gnz/hTnQ7ivlmBJTW/oSDYLCBcz8gFWZTZK+OzmJNNWdyEFn1YANSHqrsCJDcGV0QBLQ9/iYuw\n",
       "Xp1kIJHUGCUCi4ZlPzhCfYFWN94BBZ/yYu/V+iY8hjHO0dMu+xgVRJyEZAKgk3GFTRUHj2v3OGsm\n",
       "f897XfVBX6/noH/v46zvdWfNmZNcoEwDvGogTz1vP3/S43xp7VZ/FzcEkcv4HsVXLWJE9FA8Zpbw\n",
       "FbxWVeMhVznc4uVuB5SNcc6vffswfP34gjcOvEUAXS63aePTnBw/KXNdTy6B1XAoo3ADnpfVMDOW\n",
       "0QmEPosE9xTNR7uF3Hb+LQDec02wPuhmWM/GV7DDS5PZSgPX+SllPw2bkKFncJfVUaYYTtX5xJ2i\n",
       "fa8ZKrTVgJC9uXL7JwHVhArLZm/IE9+hLzp9vWMrps3Qa196hi2VvHN+ccA+nD3zT52RESz6DNgZ\n",
       "8wfqhebYGdqHlw/GQ0vEfF2YqWPWalqdNO7ZD1ab/maEyNp4quuhZPEv9hJ2xTk+gEEwYsHdbdvz\n",
       "pBLWEue2wallYlqqryahCG5Pb0MxS+1WKIBVPPGrNh2axf78bxoQKkkvY6D4Z2AUyHa6cmFSIOPN\n",
       "KGNrhsd9I/BRBAOWEoAy0pQ4zt0lOR8Px6lQ/7jO/8aLskwIsWsQeY1HhdBQB2yKHm/9dPEBQzg4\n",
       "zxInTXf1Q05TqNGMmJHsF9mulLpWMrNA/GFRbBRMa716eMO78IqWYPAtdoFD8WzuSQH6o2g5o4jC\n",
       "gKaD2tqPoB7FiW6iqxNvE4hzBJz85/yTC+CovtxO1faEzp8Y+R1i5ypOVi4N1wMUzxLuBY/+3975\n",
       "FuOlr0F0vLRA3jQEYgR9heCoifghwsbUbcZtdjNkIB8Tq1o0Ay8kZy4xoeq4/Kfh4jt4Dee58cUN\n",
       "zjP9xe6Ld98rnbNw7I/U55CdOz0zuC11NIAwS/sR8r+8e7US44EpgknSwVjDeMiLJHpEsGACP8X8\n",
       "mSWYwQKpnLazDUrNKMwf6WVqzWjWgMYZoSDq4frQpfKSA8+QG2+3bYntbs16TMn8fmNUSWX7wSOx\n",
       "/7RrVIHcQEakJbon3t5RBSCGVnPUtwDoyepzqtG4/22DKrwmejk1rvKjWWHVUmPL3hydpAa2+NPe\n",
       "gH7kfz09TaMg8i7+sHpGd1sDuL3xq/LsI2dBWLCI/qYIaTMISJxNdgCOY+Zw0dBCPVabRmGueNIb\n",
       "YB9njzK8fXWyjDWd6hVFxaNE1s5aguPIQwHvJ4JoKVUrZ0YWzdRUM0pb7OUI9wNmc5npkSPz0Q1g\n",
       "lBp+5OTXHJhd1gyo4f7EHvzMXRahF38i6tT+Siprj5knIw7WAZZ6ZD0shyHEtnLtc7TYak9+dipd\n",
       "5Dh/EcZhXGNPkx0bOBB5ziSIDXTolgMOLdqDPdkXXsrN0pWXR3HcwhW2bglWuRUZFYWnUsbM5b/D\n",
       "iHiMNF7iyIwyFdE80yFBrXgI7r1sWihOhVTXKTteSoWJaSJQY6xyNfswjoTMRT25ZCvy3ITRQkRA\n",
       "xAVUeqOyMHwQB4sfXSdoXSxHrpb7ZhXFN5AHhv5hV/sVJ/ml2L0QZ7sLFJg4yxSwN9svH+Dqb9xo\n",
       "k+9XSI8KBQMpAdZbSMQ1a/QcruW8JL9ebaCCt+7R2GYWPYkfimnZdrjxCmjtmGr+OL3otlR5cmXh\n",
       "x+kgfc36lmSnFNTOyif9hEEJJkQj3t7ZT1ODJ/493uolIATlWQh7FDSPeCRdUnsbsfFN1TRdmLCb\n",
       "ol+pNcP7+6v/u+iHcj96OQKgJxDX86qwMsDG6NVOQC0/qY9pAAUR5bVz9wO6Lfms2SmBJabCpj3E\n",
       "Fb5fF0LBvWAwpH36IHSPFrNDOSWtQchOaTlmPtRuZ34Ou/vqBeLr9GyahoiEr7O4OiFbYf8b10Qb\n",
       "H5gfep9lW0Zr8MFg+FYAUbdESoFA07e4HxEYNzcDgCxHhNNGmMoVzgWg3A9cqUOmgKoP/5YVmBrZ\n",
       "lf+BdHAak1WW6bqJ73smBg6Z0byw9KC7dmuAdU5kCYxBXFpNLkfeRbiv29wHQ9fs0We//WkEuhpH\n",
       "3rOhcKOX0Yczj/Li56O81wLRST4xtX0bjpIQsX/0T4OCRtdw7JwGC5/RiN9Cm6VNzGVXnRi+0U1j\n",
       "ai/0x802mj7FYT7gDgqXkbpPlW60QOHntzBu103Wl2SsHvHMd9eudhh0Gc2hK8Pupl6t0TwASSgt\n",
       "K+LUA6lSuczxYJQYj07mf8M76SbXGm8cX1eFqt5fiSAMOvcnhk7jU62NDhTLQd0BNf4Jq/5Bwtxd\n",
       "UQMDDVv3m7tZncC+DAYtoDZMRw8alLKUjJYEyf1e4WxavXl2IJG51XUm7kZdbKZBvn5m+96DsMq1\n",
       "+8t+xtPd1GR/E6DtnzM+xp6+TtW/JYljCzR22ZUDjKHs/qQ8R/fN3FVXgrbE+vtCUdHQtQMt3nqg\n",
       "46xXn/nJXnBz64uBHs5ZCWDF94vVlwwo2Y+i14u4+eMKUOrfI9jPsgTP+sp9xf1LjyaHE9N3EuHP\n",
       "nVzblLW3k5ivEiOEbauJvgMf9n9ERDHzbzhYs5EcTtjphZE+U49jzOFexiPCRgCTI84k25UvO3Ih\n",
       "rW72xnP5Pmn2N3IsusPAeDQe6Uj6pz77zBJ8DITierJ/FUBgVIuV5nqfjCOBNOcmnW1VVXR6ObJk\n",
       "/KprZeSgRJL59ELmkOGwp3eRd9qHMlRZhM1iJiyRg05J3EC8CB1SJWQfyQTg9FfDcPutQxFPVo69\n",
       "P0IhuUaIlz2AvEOnBErLSqQvOYZZDYmF9MyzVPnQ9cHIxRU5CbT5mwlNoQzUVtF3LbVf3HbBLuTE\n",
       "s/DFnMVKs5WrIq/PbZVA+UVrjDXVD6YskH7U9fsifC6b6Il4cpnn5ApJDZa2SscwlraahLpgKlVd\n",
       "S2x2W+Ch7avZH+eXLk3yxcQ1L8nfEvGRqZ3oqq8wkmMc/vu+YR0HDn0ezo0F3xUwJZh6fPwT9okX\n",
       "h5NruaNcFr8F/szr6etipC9VVuMMOCWdCbPvC7mjzJat86q3TbA1rvc7ioq66ArU334+PxkFlzU6\n",
       "VcFMpheOivnO3yGCFezCaGD7keH+bzYLfJcBAiQZWbvkFGOnFKdyoAR4Swvb49kS4x3EvlZeUD2l\n",
       "R3jYyvDM0Ll1extvt5O5bBE2U4ETQzmNaMHX+H1q74Rl/2eovHUXStY2nUyP/HskusO5lNOfLu36\n",
       "JG+NVbsy2axXcIVR1dyxwZd23Q6///A073TccP7vODdxXByGjURNDtPq5LhHUg0LWIhilFydg1xz\n",
       "5CuB6qESXKHtqxW4THpQFhZSlFtTf0/Zb4YgYlxCsOM9xsPur6IlPIIITwaRmuaWh3wvJsz2PuSr\n",
       "nCuJgFRqiA9ZWLoF1njTS/f0HtHTyAwizkctXAh2K8JYfKiSWZxvlVtaO/evvV2PaTqc42AdWYB6\n",
       "xc21rt8NmcnmFzmwSK+N16U7vNmfmLe+hS0cP19X4fyjIQddZ92GYInvVQ8EnQHydm9yb34McBCp\n",
       "jbpRJH6LBxiIzh3apYvPB2H+CrupSHlDe/4CxZWPD96wA68r0psQd+g4XIW0YBhgiDfUiyGvg2+Y\n",
       "Hp0L8z6q8ELwOShQrwPE3h7dONWyS8y32hXfFbWUxlMoJoHtQzVd/oN2yYPq3hZzcWR4uoA2TrUU\n",
       "aWcMQ8WLmC8skQmgCAWIqZ43ShjU7yTBP1j+/SRZzntqyO3If3D+XOPkX6jbYBOX88jp6MFWoWzX\n",
       "ZFCtERroYVdk2ZCuRIoBzxEgaxMZGSVRLtGceulXxAuuj0qoNa4AkaMYOUT6G3ZL6xtT5n72AYbL\n",
       "8DE+Iaqhda7TxRbhS76ta0BTv6oRCeS58qlkAAZT9RbzMTCMwS3whrEe2MrjeegWzD1s9G6ZVz26\n",
       "bHAxXYQjTsYRc0f1xiIdQVU2TFhLeJAbUJWaGIp4sSyKCjt9eXKpGM2fGfQ3QQBF6Ws4yPIGNec4\n",
       "suUlmcIMJ/egP3niBhkhb0HGg6zy2okNY58+sSTO+gr1eBrnAIyfDU87ZUVszz5T2ciX1twgGtiE\n",
       "IcKjfWq0C4w6OSZG/GcAz8vkpZHEu2Tx7Fv2nMAVHxendHyqqTJy7TMN3Ng3Cb3CXHt7pCtMEGyw\n",
       "fGl2Hy10GajCfYkhezj7eAL71XH+nLYd0+XdpxcQsXmqNlBty4uqPdJSIBlaTwxXncx7XJK1w8X1\n",
       "UbQo6AaDJOqWWMjlwj86r1gN65u8Q7KF2hAvU4dLCdQejmFX4MSjG+mNAKAj0GK2KUQx23elH3wW\n",
       "RfNngMtaj1+UpNyLL4bNtK5mzmOAthRCovLOIDmgTC5BgzOVmGqiVxSCdlPhifyurZ0gzV66gFs4\n",
       "yBD6zUUNh85gfaNouUmyS1lvfUHsv+xQztffDIKShWopH0PGA4uNFPQgl+wmiaZighutfjOYL2JT\n",
       "m55oWxjN6HJA6+FiTp6ljVGzeLn0kz51llS4ES2VobXEXyPPUpLVmSp/n2SnACQlBw/zvIhgcv9C\n",
       "FWlY3/ga8iLWCRK8S9bsaYSOOsHJw6xmv32c8Tc1384bJVj0GYl2mNxosFQbflFQIPrrbE95kS6T\n",
       "hpUgjzElZkwXiLxSP17lHXjfcGRG/95mTFBeFn6EBp3+vbPSrLFXqLSj3iLJ3+Q14K8ucjd15Z1X\n",
       "4lueg2RmdYRaYsIejHhcw63h+m9+pfvmSiIA/I2A+iMlieLILNzdzVnRkR2v0O2I+FsSjDNeqwQL\n",
       "MNGMygsh09abJn6bHCaPJOQtmveyGfGkMG8bCSV6nTOQqIIo55+gjf9RA7IQ8LlHUK33xTteZRpH\n",
       "LEc2lz3UmX8cMWveWqn0QwnmNk/smAcN69IUD+iPIc6nQXhId0rjaZpPbo2m6HH2XojEoNTLmz0y\n",
       "LWicDT7DeDEHNMycp321/fN7YlMimfQxcClInfDX0zZBj7avIMdG601T6NlqUPi35nWV0AvHik8N\n",
       "lIbI2Ocl2ncrLkvuSW5MXA8YGKgp0Qof4+/bcdyO3wAwxbA1nbfXKlLswBxFpXOF6dV0pZE/tJ+h\n",
       "hDIF571ipuoB2jNsaNQUciML/J1xXDx55UKFb5EayT6A2nk/xqS2ULRhoJTln1Opk5QYd9WJKLX+\n",
       "QK5KPcsCAS2qD9QM8WZ5jgfEqDNnWzu0HLIRC+asy/GUha6Zl5HrXJrhcH3iMS48YdVJZe30+ye5\n",
       "EjY3C1ZN3jGKfzqQoG80oT4TXUCZyGTSCVEbnXhg/yiyyqz1x7qTsENr+PO40er3vTdsm37vXklS\n",
       "gmIXqOpu5Sm11kKokUBh+EV2wkpP36QwHIOZar0CNor/e9OMQkYac5Ei8swyAMrLlr2k1T1IUUZK\n",
       "aYioiZFejXosG10SOA17wcBrwd2WeEr8BC3gZANcwkaA8nqquDTyyWC/xwJfDVh8qevUcWX6iWhq\n",
       "cOnmCp1Sfpzcn/MBpAEnW1pJ8ypTjAgOh6rVCEuuxdUtFmskHcoJzXaYfQWwmuZTVRrL1KlRJdrM\n",
       "Hd7K+QXaW6iuwEPVRnp7GWHwo2pZYLnPMFoAFJYcqz0cnqe2YwXFTaeCAi7mt9tC/o3CnsDWVqRn\n",
       "E+iCRuM9vPvBpX5felXCfxWvRTaH+MQ3GLvpCveGK+PKKTrar61OrRDiOi68c0YgzRtg7c3GaH4z\n",
       "a4GLygXtLFSQkUcCp2pDiylV84ZsmXwSG864Yg+67NaUioz2zMuGctqSJx1TeuAosenIWYozGKCz\n",
       "SvOtm37tKoctb73ltpVpokN+cLKHw9ssxbZmbJDBq7QGyV+C+vJi2XYSzf2xBUlhVH7Z65kga5uW\n",
       "0qywXBUHlcSQqw7cnkyh9IXxpQNE3nfHffQ70+Y4yen1bdYANdZekIZdHtwb+LQV52/Icgrkpihl\n",
       "gtjR5fp78iUZsrERCkF64HZshlZxggMftfIx+EQGE+fqJnWU5Nei26MKaD/vxRQQ1zzuOgGq31Jo\n",
       "k7myn+kiYurIp5bAgKM93gd0W3MADBCNv6E7q+bO4ZhCeGmRIFxhLdN+7/ZNLQXJc9uxh8yPBqBV\n",
       "D1ulxot8ZNOejEg8sMTVJZ+suot8Z9UCd9jOu00pYG3M7ywL7dFxZduwwhnoxcBtwhnkdOwCMfn9\n",
       "M7e6Rz7e/1zCkAqsFLF/aysI05PVJY6Qk9NshA56LAgXdjJ4Q6EWkXiotNtihWMwMGAycV4HT5iP\n",
       "c9NK8ZwPQ0dcQppoix3qWlOhKYYcG+SgQ4WwHFWikA5QVAlEN2qJd8KWfyMGAkh7pM48lFX9TTDd\n",
       "nZsu4FNwmrUFPgZI8rdpZ7ILtd8Y4iIK0kLbxMNHp9h1JM+oijm4VsdEiyhzF/Z/qtcYjG0xP22q\n",
       "ZqblNlenY/TyiBvHfbjpaIIatwgMfyfR9l3NixSozegSM1raQ368S6BlzcLjSFIA/n/zQ4q3jOP8\n",
       "DiXeFAmiloF40SiReUdq+FaPD+bokINx0ZRBA3S5hfC+qqQ5Oha8iLRFGHhoFt4gQ0M1//xjMEaN\n",
       "j+2WDKOXl1egRY/VHYCqaCglGqC6YMKdO/TYHQs+zdcSy+Sa3zZf2hALDVEF71QI+FvJu7CpvDhd\n",
       "iHeQjRDi5XtSQrcys8jLO7LSrMMTxdb2rfCPHc3Rl0g0LrkpSbIrPXcTMKFUs3Y+Iv1NCR3GXmi/\n",
       "yRnMH0s2VG/H03gKKDFvBaV0s5Y5q5WuAMfX/hiMjDZNZt/j/9QxYYKjrZFdxasqbyn6ME9adh2I\n",
       "KH8ZQDVrrmn0Hp+nmbBiX/VTu0CBGUEtBnrkNhPod0ZGPcrJqOXMHPjzM7Gz9weLhLuOy5+75d78\n",
       "qawUp89wlbevm1WkcHOjRObuCDcVEI+pDsj53D2bU1Iq2TKuOW9CdQ1KPvARag70FO+esGzBiWCd\n",
       "cx6FzLcE3idb+1TnsqwEv61Th268nyJOPctGfbP1yiKxqrM9ZgXfgHUwhyuinGsXpIKOfjkrWYlu\n",
       "q3QW24rhWVZFB5OPDfoHxNR0tB3ePLRXX8W54Ogu+Wd1yGQ53Wwuu3Uhok6a85gEcQhBi+f91ZTt\n",
       "snsW/t5fFqUsi12TUoDzL+foV5PPuduYt8Qu+pyL3HObr1ZFTKlUR65MqCOnU/D/npvdwSorm1Qf\n",
       "U/HVS+NjSkxImDvWcKk1De26dVCwOb/3f0xFH6jSO8vDEYfVwpzJIZTSsE6QiiBTzuxygAV8eleJ\n",
       "1LkMVxkS0BZnQKx6u+gFYc3MgE3UIWqQn29t2Xe7+g7qxjktQZXwK8Coi+McjZ+R1HKj4Wxt66fz\n",
       "oaNskX4senhEGwHZdi6Dp2n7oG4UzSVqbLlPZECxgOfgzEOOoVw2ttaFVX4sVa++KXZvUaGYA8Vf\n",
       "j4PuQouu08ZN8bpkAG/cfvukYxImyFLGoxE35WYVVV68VAqMmviQu95EDrG1jMAxbU7M6wqbCT1u\n",
       "mV6GoieatAzry9FzL1CZ/36gTOlb/7yfvSuzCTvWLTOowMlQjyZ+2RvxZ7gm3PHVElzYd0EJG6pc\n",
       "eTSN84oVR4wB4/J4Rf71Crf4f7RmGoj4hrGZ5QuiXWiWtt0IAGsmDa7BTvtb7lNxCTA29r/hDtPC\n",
       "TlCnqTTWeGtGo9oZoYBHeIdwR+neIVMwNvAi11ToOEMNexeervb+jro6XOMB1xL9qUKCNOmaQv5B\n",
       "2e+6qGFfWTbSSyUJhA6YhHLyiLJaHvjNri5+Gmfcx6/l1bEdk8pPQD/N4TVqTDydWU8Nffn9AmgS\n",
       "hp+Y2Ek/yZslvkI/R1fNHIYxydy4PpZ1l0ZOyUJSBJ3rbLSZWOxjx/4zB1G2urYPsUAc1z8a5Fvu\n",
       "2nL4r8JxPm0sz4tOndI6HV7+u9xTB8poiFaqwz2o/i4+pETdXp00DaQAiHds3ufzsM43T5Ic36le\n",
       "+dbDvUgo46VHk2Q0GIfETN4cm325PDpWnOEO3hpCDySDfv2DQs7DFPyRWL1iSIKO5K9/b6KNr8Jc\n",
       "eNw/3EzoPGAByD/lZyNrEaYEiuyPzYaPc+9JReAzaLLxbr+9P8z0aIVGEbwWUto03JlU450RlRth\n",
       "uo4TZt9iET/XRdbmNRs1Vf8sFDtOye6ejqQqf81Sfc28QIyCGUmVB8D9rPygUeaTFxQWD9u5h7uc\n",
       "Z6XHGZdZ+Ezarjs0507+O/7rVGVJc3MepVjghF3hDlEuw3bSe+iCuIPLCNXJXnILsme578encMBw\n",
       "R1gXNl5DL5qVcqVF1iywEHtkE9Thjc1ww0e47hlooTgA9Ce+TUG7aFy8Tiqtrk0ci/0Ai8AaB+Sc\n",
       "NRk9Bs/JZnCR/Va7/N8CyoLTWL2Fmxgw/wf1bXccn8Wf9Td0PIkYSwJrOw2oZ/IiooBIduxeua9A\n",
       "acJJ+1vajStGA/d0RsSP4RgN8or0kNMylnUH7X0JW+HuHpvjgJaLmzPzRFRoeZSzvM6dnL0HoK5g\n",
       "Tjry8QV155lmp0F4tFFPmJaQaDiSTAhoZvksNOHTfjuLZ3BSrp8qFflFCgtTMVbhJ06/KOiHYsrq\n",
       "k0WMoV71haFXy00T9E5AEggjW0XcjmsY4pJTRySqRuqsylnmiG+4jr9smWeRFTisrdIGtkDb2wmj\n",
       "IJpMSPmEdYEfR7jyjRgoag69rXzkgkPNwpoSw07Lbg6nf0WxnE3RNJK5i3KuQR3yzLzSk9NRrxyS\n",
       "ttk+qZlx08oqS4rwZ9IZ1259UMTfMlrMMzOxnqjUHKoMuH0tFAX003reZl9RGhK/XB82XL0wa7+u\n",
       "L/3akSk27KZ2GuzGVMfDQiEfPVDpLSTuvo5pp9y352/TI3GvszbPXgCuBimFvRIW8xiRuibWu4+x\n",
       "2E/vp7Qsr6H4xgB2hT18x1D+RyEUtWADuwAAJsABnqZqQR8AADysNvp+TvjeR5BngDJ+mfmDLDZw\n",
       "0fWb37pOcNg7wjVrXqgZ94ltHG0Tc03tytgxAimFqIcjUmJHrlh2oWgalEzE6BpkbhyHtPPf7O1o\n",
       "dFKMYn5rl8zR5pXA6e9/fPZ668C2KUmyszTDdKG/wOn6UAM7JgQN/FiTtj5D98K5pIlwWGeK6KOh\n",
       "rz9ydevg1ke1J53bDbG2ugWBGvnmoWupJf1DUjx7xTCM6gJJxf5YxY1CjFQD5IFF+66KYfCPRz/H\n",
       "odEYdZKZBZ/R30b/tH1W9J33MG8EXzr+54TVqLLq2uxLJWwZG66uZglc1md0zXcIQBrho8RUn+jG\n",
       "iOxZmMJYDs1LITDdJcyJwX2uczdG38XThxYEhhp5cICW1S+5iwAnOm5gFODEuyGqhz/incTNWcMP\n",
       "OHtpLVcKIZgf9IXfo0kltRDcuyF9aA5OG2bbRmL6ih1Sl7kcKMRROU693FRWQLeBP7f8ingfHF++\n",
       "1fRXMSmB8Gx8w2Srsdbaw7TTUmGWq2wROPLvu5uGyGKw2up9VXJiLtfuyHiXfgVFDodhlTp5XzKb\n",
       "8XNdlRV5IeoX5tEIu7VGMpKMfeoqa6IxrOEq+J9oXzcNJjgPAfwVWil2vIUB4M1y3sNrf7DmEV8g\n",
       "ll7unERTgxBlGhquyBVIOaoKgBs3kGx6Sw9hTF42RgQLZZWqWJsah+/iIInx1vY7vI48hTXpNCij\n",
       "HP4l7T+LmoP88l8pjX+IqoKO6x0frcW6G6NpGW4nhNxNW0BaxTMhlzlljo8a7z71tFz2mB/UfycJ\n",
       "ZJp3N/Fqbcj/SveqH+Rcci2vJweyVf5lY6J8oeLynaESRpDC5ZIq7pceN5sGqgB+O7t66nicjTkp\n",
       "4rL4O3MotWtRwnQWSXIFuokKMrWdZ8iV4vW1OTn8vghbkmlrGar/GEZ67RPP8aQ1y0BcJFIb5zbp\n",
       "i7PIWQ8EnJqwHpDfPBZD+o0H1wN68HoM+GvkseaFi954ixAUCX9nEUD0LeO1CUbqNmxqGDAFDjl5\n",
       "bmQbS7zsl17/cEQvHuarBO8HjleZFW/C+n6q/re/G8TAVMBHbWTDNWeu+vhCWt15Ek8SJ/Is5KrA\n",
       "ozhb7D6AoOLvqH+OkCyOMZFFAJh0FSRULAAhAebPrxRc2N3nOXRs1VyHMOp5PmvddZtr87/gUyeZ\n",
       "sXVp1lVr2JE3/p3xPI++E1VzC/hbfjmaWYYrf223s1nXTR/+YvZQuzMnGIfTPI6sz2oQSYVeF1U4\n",
       "rHHL4kUkUlnBrHvh8he7w7UN7cCJjovvaTnTzNvtGBQ6RhTSeVPAzBO9chEYhQcb3OOSe0RWC7Wv\n",
       "WsgFGQLmnC5t55Kp8MSBcg1JNIwIO3e2YAO0RTdzjqUdmay7GBq4P3rljDQVZYAUrpr5F+kiO3sK\n",
       "jpcqlH5I9DrVzya0ZpvB89E9FdCvvKsJt7K2jp9vXqepmArS9KMUsgBpqHGBzixlcmd5k4o4NJLd\n",
       "oq9uKfS/Lqk+L56k8NwK38Y2tZYsxOq3oRlOy2bDYw3aM1rW/COSq/GTK+y4aUm+wqPOUx0R/OG0\n",
       "gDbP3orVeFYUp4QDa+gex4SGtOI9uPuD54linebXa48vOQ2DKU6Bk4790vJIac8fmTGI4AUPgQpW\n",
       "1eW0IhGx55aGR6IdCK/sclaWBhwyDQ3+4zNqvZxV1pVAGGbqhxaYRmASayOziYI+HErgGkDjM7BN\n",
       "BwRLpF4uX79jhp6NT05UZyfbOEQDQ2E6P6/BtBVsMw6yHsxoUptQ0XiJYUrlVkVTBLIZq2JZVySe\n",
       "p5dsne7Yu4zlHROzMH7e5BEgjUF/FcUpl2lLPMmSDisVemUzQ7cPUpFyvQYjPoHuuJTAlUeuLMNl\n",
       "p3S6X73yMVBX40T08T4rFnewzqFeCR8OQeYksmbrn2gQUn8CdoWYcywkP/g/BusPPMBP1A6zA73M\n",
       "TTQ7t35nvWvsOsstHuEj7AYp67e6XEeYv/det7SkBFzKrsjlMiioaDmPvolcnaqEwabGswKCDmpZ\n",
       "LVGdfF2UOeybwfbmUlmWHvfITdj1HXVkSlp+XHuzxfVG2AId/seww8yk55Cq7EOP/tqwO5ZM07xJ\n",
       "eBA/v7BjkD8yZHkodIJiuF2jb1SkBw5As0/sWBfUGFu0vO1bV7dgnlQzdtHxGci5QquYnr7S0ikL\n",
       "K5JLgsQoNsYL9BVeRxIWuHclosH1aKTxGGzt/FFpE2BTA2c7k2qpNbSZgkRkWH73O/LuqqpT5moj\n",
       "EbwzIqgrWwna6NomIk3Xw0KxCERf7jS9vk/xyxPAsdiBYG5q2rJdFN/fdvocmsARJwgW9EnCoELl\n",
       "1xlAENlQYs8O6euzKSxy2urHz6olHXLRimJwaXFnt5PcshUnSYBc+fdkoXnjVXAAyQt3obbCTiQY\n",
       "F/wP03Rmymcsc0CPEE7OLMVldB/yUx+pxBLd7Sos9nlblIF7Z1l7othcjPephSVVs1ulkEtk+GuL\n",
       "vwQzoYnDRzxT5PEr84m6YBKtHxyQQRzrb9DiEhtFMyosY4tzZOc0Z4l5xGQKVfSdYjkLkq5Mer6v\n",
       "t8LFxzpNfQSXNZDrtKDEzoAu7xrTpmhHDZq3WYGb4ohZeVbftaiV7SP8ChPEutQbHtg4sCQK6yJ1\n",
       "UW8DxWoBely9Re4dd7W8ELAHmAjWbwnX/DjwTXKagW46dicH5qdu7enniA6MPe50xGmyxXXnBeUO\n",
       "8GStJwTRf/klotictLTIb8F+as0FXjYQ/gwHCs/g9uoNEs631f1B75tr8SBbhixDc8OsNQOQ8PVW\n",
       "9QY0TzMl7aG8RZ5kPv1NhCLqmpJwQ7lFng8iD/VatcCjOfeQiAEh0fSPnJdh+Hp4XQY4rHb/zWbF\n",
       "psNl0UWwllEjNAeZ9HayEVtDqlMg3HhpqNwUW7+d3MwIFwU82CelSIY68LqUCpdPmY25v26RJe5y\n",
       "fP5bVy3/f1IOPaUcT1x3E0myCUYlebhH9OE0mOu8YQEbzNNA01wT4W1VAzpEExUx+BNI6g2KoIZF\n",
       "CnV/g2MuPHzASpXbveASEvF3S7U78emhZHy17G3oBjNxBbYmiVsPr/ENSZXU4RsBstXtJ6KAkZou\n",
       "prNJoyFsdVX6XytNt3ylqwkvTIDO6qxbupJhelp97EEVrnSfFb+PiAYHzu2B3GldRh2JuXOqh1de\n",
       "tkZ73inc9GsPtTuppzTIwGRcy7qe2qc7t/JrkAu+EDNbym1dy2pMg47aMk0JWxBN3b+K6mAJGFe1\n",
       "Hsrf3a0OfACCp3QnBmHzoFxurXjhetU8HQa1O5ADzaUWdpn6qFaZo/koEnmPx3X82fZwdX3D+5WP\n",
       "vB76qUALpRTwGgAAHV0CWwo+VRroJLKVKIAxftawfosDwrfwWLx6WZrZG5LFwIgiDa9kuvt2XELS\n",
       "25q7/yPLcnbL3Yc0za7zCBbK0bhEoIFg55nEdYGsAPEjzvYUcHnKHy2RL3+K7TfvEhxyOSqr/gs0\n",
       "RiP/IkpFx/OlUnI3B/u3aZ5EU8UQM4rDbc0auFgLsiz+ybvs9GrULAw5Oc4mu8UBzQXQ4YRcYLVZ\n",
       "GRWsgYxjUndtecOfoX+FgCCHmvE0+xKSHK82Krg5NZESdkdXhsGHz+b+jiMAnNiRbmT3JBiGpjq/\n",
       "6TOML+9GTwjdPMlHqpl+EOj3qDAQP8x0FaPnfGlEajgjDFtVYg2NXrSD9Olm21suhuHqaTY+VD19\n",
       "/Qf1bVnZm3MuA8ARHxFsvJRTx9KoPHj0ON52gcDA1toP6/cBrhLUrnvUJbna8UJw5DtEDj0rkVtX\n",
       "Sa/XZMK4u5YSsc66q4jeZ546qCUkvPJgoJUYSQFsarsqYFldCqGgPPXNK+v4kIjjAm2+cDLoXhPV\n",
       "VAhNxada96OjywT+mA3eSshjMRPYf47dyTcn9qeweb8862ShiCzQdv0ooz0ipl/8HVwry6fT9PUK\n",
       "sGF66UB/SnZGRkc31IlgYefopT+4nYmP2YIG2mBbJckCFTSsDdQy4AY8SWvk8P3HEn0VK+MK9Qrw\n",
       "jR9zGm2Bayvt6RqhqTSI6VEOww+YIqL+loC+uzEGraqET4L5+AMiAVdwLi1zej3jXHcTbwJT+Zny\n",
       "0C1ac/A0SCnQObFA1p0K3NLQ0gUzLDFm+KiYVGGzG8bt4tC1X/DwKCsudMojjvK0cjMcvPlK8wA2\n",
       "Ml8RpcGkWCJqYG8eY9T/r62/75x1KJ6RSkDSs7FV+TEWWsLXCdcYIpLj8SWc7KjlVfpUeVO0aynQ\n",
       "D4JoLWU3kSld4krVfG5W71ZKO0VA0+sgrWx+PSaoqJtWobBIFZSbLuz8bdV7PUG324emAPwNHAC5\n",
       "i02F5hWpGJdcszgxBtXt5C+LSIjQvZnhMxd7OwgdZuFnN+IbSk2H1UKQ1hq7+MLNip/sG+0wqN6J\n",
       "gymQeHyUkg6aUtQcxnyTVy3+AQWIDgeUnFz31W2H1gGKJMEu1H8KYjfXqvxjfPbcOI8vwgEt1qO5\n",
       "8GkhjvoKwpYm9n4vCWorN/JyTemL1YGVhTekLIzuqmRAmo8A8XVU1DyO0xuigr2PPowf9XmAVT1x\n",
       "ZHmC45Rp4HfATOoVhnjpOZvtRvp5kFpX6F0ZQoRtAieWQTm9xlNx6Thx6bCKlqK3+TKuCpunzOSf\n",
       "Tn13nGJd4bckxW8a15oP+jDouyD0pbnucBFz/IR8CLguuJ3jw4jHSYOXJFEcCTwqU/HyIbkX1qBh\n",
       "hkGqQrg5mKVQQAh4K0L61tQYam1y4TT2hYFxtHicl1Amw9uxCrmJMbaKpr5iX+2Vns6/Wsjw6H1k\n",
       "xXywfdO1CYQmpvO1nH+hTvMTYkmIzWg1pG6WvDxj1tZS/FArliqP8ujNWZacP/wLz/5LfxVYBiEy\n",
       "9JdmIT9C+4Fj2rxzR80+g5Q3APyRi2XqMBdiVX9ZikX7HkIKgtZ1X+VvLDluEgdwoJQ8PQUVQyET\n",
       "RN1OSQTl2W8JVYOI1C+XySgNUMQAU6jKpZCBmJB51BhaAwYdQl64wHplgXftVBDWnCcuR1/UbMEE\n",
       "VGVsOI5h3b6kPA89zBQx5WLBgRNBsNCJz4zhSN4bUjjVDJWsQHwJQpOIzadjnOLIuXOhv5BBjySu\n",
       "PzrzHw4TPvn3tu/zYOpcCZalCt+lqEvivVCfjy40NVyh482RQrXpQa8Fw7jSAjzDtg7sOpicZp9J\n",
       "VO1An39CDs7iUC+4eVsLtS522ocE/R60idI5hUd2F3CF+CWp14mWhTENT/ZA6Z04tCXu6reZV2T5\n",
       "UNFSlGmxtezVGxHpEfjLDCMjXZbbPZBYCMViyJk59F/sUJYNVPzpv/iho/X4kk46eoOTWpLJ7mCF\n",
       "w65CyMX/BELXPFBGe7Pjw06VpWvDaPt8BP96gFcLrO1gD/cAo2UfUrrQQumXZiSAnUW+KtTzr1Mc\n",
       "p3sP1cKvGSFC7jCKSSmUfdhEOlGRhBcfhaHAwJ/wZ9kPa9Yv9XDbpFr4Prf41/mcNeeWxW+4/jjY\n",
       "FatGuZ4SVCoAVvWOxdeAPnDGAd9nBJ08h2hsos5xg5RAorF/NCcuAf1u8bdJsuZuCrz8IIOMoKBz\n",
       "7F5i17Ove/v05WnBs/8MOCkJg42bOfUCZpbmMAUpAY5sGLCXaTXoRKc3R+Wkh87KyBZOebIUCY19\n",
       "WD2d72AOqPStN22361sRDXKWvisc+XqE7hP5UJa3FT7Yk4cIFUP7YcG2rMBhZ1UMHSlUb4B1qZ5Q\n",
       "145/K9XYqmg2qm/iHuqj5yC/S47aVviuNVQ975jOB6PO8wsNAS1yPFwr+4Ff8uJr654/B+uBV9Bb\n",
       "NkuuuJT1P6QDrTrSE6IxnGku+YggewK/N6CiHX0ucpSs9/5+rCoI/p2xFIEpUns306z/G//EOIEY\n",
       "eX3qLW1LHtgHdd901O2DAV4+eA/2fdUhn8xUmumqwbqdkK0WWlUvI/cJVhLnWeAngaoZLJ9sUqFV\n",
       "avHyvhGpgJthQf0chMzOrZaHeIi+EbLRnGabMZaPs8TjYMevPNcatInPB0KarjHC+EQDM/k80S7d\n",
       "gieuhEnwTuLziWBPMrsgtGvVLwwGSxoA5n48dgOzIu2s1ut2cUnczqmB+dU64P0SX2A6P7Wp2J5I\n",
       "XSF/rAi8h3spRlgEiF1N1fPwAkOv5zLWSY+Ki8WvwX4Anr5zvz18oGht4muQn6LIKqc452sS2dyP\n",
       "n0KzI62G1lmHIaDwQjyvHA01FKscaUNGK1mXVhNGGZhXHHgXcEwj3jdGuHLWURLyN+hVHhSfRE5n\n",
       "h4BTem4sflLq106efYkb+r0+i4yfor7COJTVH4EdfIWOrmbErZ+33hYCyiak4OAvVo95iTMgsoZz\n",
       "Ibb4eEZZYSzInGxacImpruK3lHt9wvFC6J739JEUBeybCV5LkfMtYBSqffyRN9oFvU60szd/OlU0\n",
       "8kEWk5YpI/vC5WHot2eAxvA2INBBA02dDZZhVS4i0frQ0N8/U2qyg1hmKIATjMq/UFaSliOE1R93\n",
       "7AxYWETjU1apM+RYKTbwp+t5iEa4a4J7qWP2ZcOaMWfrPXnt2/BFEV7jRD6DLxe9FukyHChGQe4X\n",
       "dm6lCfjsTK2zODHUFGJix8RWwlTzaMrsxlmovuBhtL66OKo6Rtmvf3ucx4axaOfhPoTgbxYW1YoK\n",
       "LlnXsYpU1OkSRbQzL1aPC0OuAJnC80xaCmti0exwlrYs/6QmglRc8V5JbFCJqQorPV3ZZtcwLNqt\n",
       "Li/64Wd3RFti0+nG03HxA5AII9AJj10eTH8fXxIjyK8RXM/9va3rUfu269Yiww495+H510bwVxTF\n",
       "5GW8GVi/5XDKgAtn6U4UvTXhI//5OJFHYCAg4vAyYVLx6qL+NeIi32OndcYl/OsfdLRb9hMkwEcq\n",
       "M2Iy3TbPAS4RS20/bgOUpFSUX9F4GDDFOZ9xBdZMcygXxO7ske+BKlxgcx/wFnOgGzRaeS+h1bBV\n",
       "c7ArLC/8Po2fGzzjvSjc2sxQWmZkIuqqNcxkQIKNR6QS5+nPLnbgQuL6KNynMf8O+8WO1g9D+PPY\n",
       "OKGHc0LX5NkG7q0l2ZMn9C+2aP7FM+43IF70DJNv7se8AEZ62sYdUGtzCt+7y8MA9xsTML9tJ2MR\n",
       "rcWfM8UNnazAf0YPuOvB1qKV76lcJ/bllg+iDJfm+lcnyIACdY14W2aTWmVdfD1iHe76I506+2ij\n",
       "AuMPt5SqZHlAYVDJO8i/ZhdEDyyEpama7fhOnTN9SDItsoLkuwGEEzy/6TkmNQcOJcHXybmd4FkU\n",
       "sCiUrI7EQZZDcaHsu42dXHVwHyssjF5I6ARIjw37Rs07BUVKLVdzoyXOXmIIvHnQCp73j5A0WU/2\n",
       "tgVME3RNlxdQgUZzePJqk25hQ42kM4bntt2MyucRTEYjrziuY6hxFmD2quAqsGQR8O64mEEQW1JL\n",
       "NTZRNf03ivcWC1RQ/xjbWx/tAAbM8MpReg+fhd2RK/mi52RROdR/HAaUzA3ayRbp2mIJ10S3fEck\n",
       "bP68sOCgD96R4zF72EOfgM3F2abiZr2zyuRo+f/ZRqHiiqxaPf9n9u8DWYYwQaPTsUayhxgFIGML\n",
       "3co6kziqc5WXoRZBCE5Gh7Z821nU0JNImZ/XjU/7OFlslG3aeUdKexNcYdUcMwnd7A608pJ37Wna\n",
       "RMtYWH8hQzaIlJzx2s+pg56qC1u2DvG89jnH0Mn6JNTCsdG9jH426cZTdbtOh24+rwa3jux0SbF5\n",
       "0tdtHyyt0iPvZI7necCCtLkhgvCV3K2pDN/QmXMXQu1EKidxU7ism9MNGdd1P3+KjyeCmPsyhGc7\n",
       "iHpdHdSXdzfK+/qh3jgdkgD2oZNvZ7JWp8GA5IohtsSUwUTBYUDA3K0pap57ojDicbqYdMGz3Z6s\n",
       "3Ke02zOiBjl3qwfcwRhSN06hnSp4xXvWBm4UErSXT9I1YUt+wmrwhyVhvMcGKV+iKJSYeqtIH5nT\n",
       "kq8wLBq0nZn9eHVT29TCvtLbPbV6SgD0vdWX7KlsU8AvrLTOahAxt/tWzx7H/3DSZljDZ2ubz4/U\n",
       "3GKGFHYAjvyTf5nm3pEGyYS1GnwmNMPgOoN3rR2zbM/ryYNM4OnHYZWDPUTCG5B54CwvMKHo87Og\n",
       "psVVU7tRJ9En9sJUm6MucKJ7CG6ungxJCnBgNhW+UwM/cTto8PZFEUlTyF4OTMl1417ig6huvMkn\n",
       "AL7qHOODy6urBZuBdO0t1nzZtqdnCc6tfiDnpVmIDwrmX9hApN/9etc3+ifaoXrCh9/ZAJwCMeZ5\n",
       "NS0HOW8pVYPRCW1C2poaC+3QA8U3kcFNW9/qS0ff9jSXfNq1vhszj6WVkFrj0ADp3v0JcBX1dkDp\n",
       "elNvc2XWyLAg+nKrdgF0AwiHwCRMmu8jrxKyH+YF3Gnhupk303kuon3qTg80o+UVufm5npIttRQb\n",
       "lNkXt5iyo8b60KdzmfaOiV/R3Ueegf2R2c9zsiijXKygMvCadnmW8x3/G4t/czA/wOJHm7LjR74G\n",
       "diRt5IjFDxkLt5PR2ADfsP/8qvZhrNLm0beQO9Ot7zxzbMAZw8taKHNyx0GQISf5Qbx+a1mO4YDZ\n",
       "vjE8fcQPb7R650s8ZwJVn3U1PX/OG7psr0jZ9fFUAuarp1AOe14atkQEAKpmks1Ym9RHTLBXcFGG\n",
       "tfEAFGElG7xsCrI6TsyTa+Lg++RGbIwp20Srynp+8Wq9Xvw4wzKtAxVYK3wdk7ZUNx75devWdDea\n",
       "P6FZpVxdDw6YVJ5vlzK9FqyOETyCPmVgNsxwewCNFgb4Mp0NfyDOFOnXF62UdnQ4BTAucY3F5+KB\n",
       "c0PdUNIO5zU8rObsRSHv+tPUVchVvJD1W22SnWE1ubx8hxX+qRKHNCJ6RmGjh2jCFHjYg6RqXaFo\n",
       "LpjYhvwF12JBWqNM3NBqdjfpEk4HCF5me7g1JGkSnVecRFSx5VKHZi8A8qQSt7sk4ThX/W5ZtAX5\n",
       "jokZpTGz4MhPcRE3ymittM61nFSxu0cR/zl+/+GvHOL7rjf9JCfk2pKw/PwwvKADYEGJ/jjYgGOg\n",
       "NqFPTU+50SorJBZU+frNI1RXWNLYHGgTmYe2ZG7O/iHmPfCN4QPWbnBwWCdy8JhDswcVp8kOOvYg\n",
       "w2ung3Q+kJU6bN5NsIEqt80YrYLHz7tPKLc9Ggp+u1ibwwlFRYy53uBiSTp2+Qdvfmkit+wkOsMo\n",
       "fUUb4ROhDccUFo5LHr1mQIjoBiff+sn0T2gpyIXCgg12fF4SzQFobKk4a0zmYS3afuwHggizM5CL\n",
       "Dg1KHAX5M4Tsa1ayJPvQUrPHJfal78sVRXLQ9kujU1KiLi8Xz8ApDiaKGZlpKFBa1YdDTzOGqYFn\n",
       "/+RUobI047JTWdY71B3b3vASfKtI4Hap9gIED0VPxbQhMMv7mgoMkQKixAfBNO+a1NG/bOK40fpU\n",
       "3lvJnzB6QPsRA/aTMVq8YI6FRk4qtiPZWVIG9TRU6sNM58S1iHPNsJAKkzQIgrxfxKUtmEe88L41\n",
       "bqnSV+6adZyhVBNcnYRxyqpvMmJ3NT8FhkgSo9KZ6kTyBF98yn8Sqsf2kvaHYWRIO59GPWzqt98H\n",
       "FW4uu9nNFBK7UJYTbFZ2QVxpVaMrvqGh5/+AtVmmE0bTHa9wBH+nt2wWLc/OIHYl7GnclHeRfP77\n",
       "6jWSz0w+jHLMMgVcPHCQZH1YD/Az54dFI/JT0ThdM8hSgtimNsUb0UwDtsc+fBqJdLKLfKU/cukW\n",
       "r56omtizNFKJHe9DX6r7wPkmJel+omq5BBwYfHvD3xWjWrL19Lw/LN9cdvWqsbDbYP04dRkwOJSK\n",
       "XBcl1qc78tyshMzZqV2bSPWJCKOtTJtcnrHimI3RoqPGDeew8AJU66EFos3Zn63OwXQhDrQLUa8Z\n",
       "aSd6HTW/i1lGfV8FMw88KT7kTPD2ZU6E7oN316hFKsSs6qZLCCDj75kPDRD3RQbKsF22kzKj9SYN\n",
       "fUlnT4lAgXJCyQHZ6Yd3tpDGoeoOSoR1QbOh+zN5y9d0AtBlXvyD4BfFtBaC6zYsLmlXzYpAYWOX\n",
       "EQATLRrStUz3ciYYPyoALo5CsFAnD0s4YolIa+QKxACGR9EdrG1IkaPWUzfoS6zP1lnNl0bHOZ85\n",
       "Z5usczhRuIxB6ovmVhXTrnYyEkn4L1P0r5PcP/23Jd3DjjbG4Se/WWp8UnwAFH3NHG0+Pyl7WMbq\n",
       "g+KS13m2nAMyd3DmAF8vqZZyjTEczfBCZDmL7A+e8EtBGvJEq25/v1S/xPoXzBVpV/IdM4SE+Zwz\n",
       "2SKgf81U0IteQ7r/Df/EmxAtpIjjUMqyU3U8VIP8Uow13oeRCpo4bS20O+pXZhiXu6rdaHXL5CTA\n",
       "Lc5uywjYXXYCK97fdf2FChNhVcpET/Omfo7DksGAEqOHmWUHPjowiOAYSHSQrG/EsfY9ybXQViPp\n",
       "IEQwQQqmstVK+bTDrMk7dCs6Egazjd45WKSLdlGwxdmeyNQowEyTVIx6WqeFPO8hCZoy0nWjYYK1\n",
       "R4Fq6G2VP6nhOA5h1Vymb4jl9Ha0BFYq6w6u80HOYWYf+7tCGxVwjWE0zQUlXGlJHyzlFhRBL7dp\n",
       "/fut5bBQCyYKr2Nx+EtO/PbNSb3ydriAtfHFFq6+wuABDFfyTh3MVHXI07KIAdBpQg79N754I5JS\n",
       "KG53PIfi1W4vS4s/nml0MyzdByqLeKhSEFC1VkjftpJ+eBYiEETXyCWsL36ubu3kr0U2FIydw+b2\n",
       "XGKbyvsNTNLZyvc4uigoj4He3cWACIb5rScyBZNdjefMwYAfysGWlB7PQ9YTUfDHrPAQqdY1Xlp8\n",
       "4o5z6hQjAV4qyvjkkrZ5towV1KPh0KE1tRkSaqkSp5x6V15oCy+1TgKyyrMSORacX59XjLGdEYIN\n",
       "k9F/6IZ6lZUFntJZY6JzP462RR9ni0cmgQjp8Wf9k4rXsj9hWBlpJOoNIm5vuIUipF0KiPRubS90\n",
       "C9GeLkLAolGIj84qhgh5OObiuzJpfidNnpg6u0jYEVjEgUqr29nm75aLffsejpxolvmKbccfaho/\n",
       "/Dx0S8G3uuK0JcjQOi/JN+sdoeMJFJ3vbLbLsZ2ASQbGIIyb9LLI7muOQ2Bm+A9iskR0tfM3BCyJ\n",
       "1/1LJmR/LoxqAaTtaUVsPsNESgQ+7uVpA60aEEtwey5LcX/YbSeeTLv0vVEqHkTZ53dYe0dP9VPl\n",
       "JHpK7ZHp0Vy9WeiAu46uaj5QCGnJ0GuwPi+AN4F5pgs/CP1G/M9d9//4AODMBfAOQJo/5pAmPiVK\n",
       "/OndNEhV+tqEySdm4V/X6NyIgEFEZU2rKfLq20DlrCRoOMz5T2dlIUA8E9ZUFZQxzaFMb5NUml8y\n",
       "75hR2AE/FggHIkjA0xEYDIU7Vg/hEA1TKopZHCOOEfr2vYoiPqY2Og735O2lABR4wa214SYm/Qcm\n",
       "7j33gWNJUek2dUsyGvmuTkNnSnHHWxZU4Oz7eQBCJGIqk3jT68vdMUVWG1ClWiGy0bmvWScfsWUM\n",
       "7PaIcEDDWPTGqRxb7wpDFm0rj/smtYAMJyssDptVEe5cyas6166vO/x9r0lgBQm+g69xdsQxhSt6\n",
       "JXwrWSNuSCEzwE1heMe9gArWp+8sPMsylqhycsmuNyaApjzdG8voXl3SLdCZVFb5QiGzfih9FjdH\n",
       "ThTK2tKviRnc2wBMIXl3VCQfNve69LoVOCDmQ6l9X3flGmxrKUnrwJvhy6Zc/1nZvAztD+gWMAaR\n",
       "LEXd2+Gsmnc68tpD8QPgRRVqFfHxrGWgGEMLe/9wCOmGvgjvBOOFEqO4b0XOjGtxIaHc94zSlDkU\n",
       "99Hny2Rngq+oxbLyaN1ZHNtfNm7pPGjKXP57msl80WbS24B56wZkU5s2tidYsAOO4HFtNGGICYNV\n",
       "MJieUkvrYPbyZP9LfUy+fWdh7DCB4oJMNySTPh41LQGuKr+EmvigT4w57FkhowlwjX5j0FZOwp6k\n",
       "4tSQy2MyGAlrsHX1DDCP/ByogCPSprglyD1q4y0yc71kO7auv+CeXm6a40Xb/OBoQvTvCO5E1HuV\n",
       "7hmdI2/05kEDacHTmYQQ7pVX94f5bMwI+G6l3r+v5nHqyPJj0Fh2X7Z5j2jond/snQ6KOMKvhanK\n",
       "KdLBtGekT7Au0mkezaLbUljh+2dCT+by56HCW3BbIGVICnQH9lZWcFadcuNpv7zgBWAWWitDvU+z\n",
       "CY7rrHbTHuJLqmazRd/RCVbuQbzsl/d7my+s1Sp3L2guzn3Epeb4zPIau8/L8Mw6UZNhgASuppLY\n",
       "JM2LSBaP3bRh7KgLwMOk1SzhE9xNhe2easDkGHZN2FS6Gj1dVjUxG1RuNg57E9mbFDAsosVkQ3Gf\n",
       "oj5wTZPW/UcLVLqJCW0koxZ/ZLS0hZsfD3YCTNP3LDRyOrxQGIW4asbaQ85gC1BoawtbPVHz3fa3\n",
       "NwOaGaBLwS5ce5xZKx9VBJSmKEOmFraMYotjWUzC6eGxzsOnaczbECn8d36UwEWj01yXBBUEGc+P\n",
       "+vuRkJFKwz8iUuAH7kV27HeAwPTx9rlJLPPuEj4epMzAm+YEzt2Pk5cTZwDpXd02k61v0hAdi7nB\n",
       "N5k4q/Brikk2lK73ZEvqlW08//IeeOxGXqXUaJTw3NdWgoGU7kScxgzYtdmlLC0e4+1MOw2i1Vye\n",
       "IuixbKJZT6BczITGJT8Bq2YiV0E5FiXS7dElajR9lfhUk8ajZUVafpUvGaQJk5AGSlRmdjj/Gk/j\n",
       "yN6rxKkCVwVB+qpeb8XQAAJ5BtMjw6NrRrUjXZNrU7sq1Gls7vqVMWNGKSW+cOmAJHFkzKsU84WW\n",
       "SujUiw7FJoXVJwBrvCydS5CAComI5BRPG9hT3jt+KU4y5xP6lAfc33wARDIff9BkwoJt53VGR1fT\n",
       "U7gn/BhdgMOINSUscSrn6EsgzSch1aqzd1QcHdQiFQTYqBy1F8+Ek4C8PsprhGypNnQAhQ31yzJg\n",
       "6BbnXzChk1EDuItytd9sygdNlxVKeLovv44qOXX6oLFoo5DWz7a4BM/sTbVGGxQGarYXJsNVb2jc\n",
       "Gq0yoouN5x1POyBGU0vt84jiDmc638TpBkrKqkt4Fwk0VJSTaLt4w9lhOYg7wdXBFOwPM5Mx7Kct\n",
       "4Eizr+4VKPTN2Icun0zfmRoU9ExfTr/smpWD+bNdx2/0b2bf3u13fdW06+31mE9yy/+YJV8E2e1F\n",
       "hTRqrk5CMGnYcpZUARzpIf9vgwyaRM+8z7KtJSQG2Iefj2E9j8JO7XsV4rAxRqn47a0R57SB7eL3\n",
       "JE8jHd2qvmUikp8d0Ml14QuxadvLZcKav5zZBOW9eYwTDENlLSyrdatHIwW6gjc74p3zKuEVYm4x\n",
       "2qzCpC3W+HvJepVoqz7cakUzgIygf94D3PtUzAANmQAAA4ptb292AAAAbG12aGQAAAAAAAAAAAAA\n",
       "AAAAAAPoAAAJYAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAA\n",
       "AEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAACtHRyYWsAAABcdGtoZAAAAAMAAAAA\n",
       "AAAAAAAAAAEAAAAAAAAJYAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAA\n",
       "AAAAAAAAAEAAAAACgAAAAeAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAACWAAABgAAAEAAAAA\n",
       "AixtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAACgAAABgAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlk\n",
       "ZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAHXbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAA\n",
       "JGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAABl3N0YmwAAAC3c3RzZAAAAAAAAAAB\n",
       "AAAAp2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAACgAHgAEgAAABIAAAAAAAAAAEAAAAAAAAA\n",
       "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAA1YXZjQwFkABb/4QAYZ2QAFqzZQKA9oQAA\n",
       "AwADAAADABQPFi2WAQAGaOvjyyLA/fj4AAAAABx1dWlka2hA8l8kT8W6OaUbzwMj8wAAAAAAAAAY\n",
       "c3R0cwAAAAAAAAABAAAACAAADAAAAAAUc3RzcwAAAAAAAAABAAAAAQAAAEhjdHRzAAAAAAAAAAcA\n",
       "AAACAAAYAAAAAAEAACQAAAAAAQAADAAAAAABAAA8AAAAAAEAABgAAAAAAQAAAAAAAAABAAAMAAAA\n",
       "ABxzdHNjAAAAAAAAAAEAAAABAAAACAAAAAEAAAA0c3RzegAAAAAAAAAAAAAACAAAI8IAAD3LAAA3\n",
       "wAAAJRMAAC96AAA0mwAAN3YAACbEAAAAFHN0Y28AAAAAAAAAAQAAADAAAABidWR0YQAAAFptZXRh\n",
       "AAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAd\n",
       "ZGF0YQAAAAEAAAAATGF2ZjYwLjE2LjEwMA==\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=300)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsc4WadilvD1"
   },
   "source": [
    "And now wrap it in the Pytorch Datasets class and print one example as sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZXPeOA34lvD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] num_image_token: 256\n",
      "[Dataset] dynamic_image_size: True\n",
      "[Dataset] use_thumbnail: False\n",
      "[Dataset] min_dynamic_patch: 1, max_dynamic_patch: 6\n",
      "Formatting inputs...Skip in lazy mode\n",
      "[Dataset] num_image_token: 256\n",
      "[Dataset] dynamic_image_size: True\n",
      "[Dataset] use_thumbnail: False\n",
      "[Dataset] min_dynamic_patch: 1, max_dynamic_patch: 6\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Train dataset: 150 examples\n",
      "Test dataset: 38 examples\n"
     ]
    }
   ],
   "source": [
    "train_raw_data = [json.dumps(example) for example in dataset[\"train\"]]\n",
    "eval_raw_data = [json.dumps(example) for example in dataset[\"test\"]]\n",
    "\n",
    "tokenizer = getattr(processor, \"tokenizer\", None)\n",
    "if tokenizer is None:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, use_fast=True)\n",
    "\n",
    "train_dataset = VideoQADataset(\n",
    "    template_name='internlm2-chat',\n",
    "    raw_data=train_raw_data,\n",
    "    video_data_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name='mvbench_action_sequence',\n",
    "    num_image_token=256,\n",
    "    image_size=448,\n",
    "    is_train=True,\n",
    "    pad2square=False,\n",
    "    dynamic_image_size=True,\n",
    "    use_thumbnail=False,\n",
    "    min_dynamic_patch=1,\n",
    "    max_dynamic_patch=6,\n",
    "    min_num_frame=8,\n",
    "    max_num_frame=8,\n",
    "    sampling_method='uniform',\n",
    "    normalize_type='imagenet',\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "eval_dataset = VideoQADatasetWithAnswers(\n",
    "    original_data=dataset[\"test\"],\n",
    "    template_name='internlm2-chat',\n",
    "    raw_data=eval_raw_data,\n",
    "    video_data_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name='mvbench_action_sequence',\n",
    "    num_image_token=256,\n",
    "    image_size=448,\n",
    "    is_train=False,\n",
    "    pad2square=False,\n",
    "    dynamic_image_size=True,\n",
    "    use_thumbnail=False,\n",
    "    min_dynamic_patch=1,\n",
    "    max_dynamic_patch=6,\n",
    "    min_num_frame=8,\n",
    "    max_num_frame=8,\n",
    "    sampling_method='uniform',\n",
    "    normalize_type='imagenet',\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Test dataset: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: tokenization mismatch: 1138 vs. 1139. This dataset is mvbench_action_sequence.\n",
      "Keys in sample: dict_keys(['input_ids', 'labels', 'attention_mask', 'pixel_values', 'image_flags'])\n",
      "Input IDs shape: torch.Size([14588])\n",
      "Pixel values shape: torch.Size([4, 3, 448, 448])\n",
      "Labels shape: torch.Size([14588])\n",
      "\n",
      "Original conversation: [{'from': 'human', 'value': 'What happened before the person tidied up the clothes?\\nA. Opened the bag.;\\nB. Took the sandwich.;\\nC. Took the blanket.;\\nD. Sat on the floor.;\\n<video>'}, {'from': 'gpt', 'value': 'C Took the blanket.'}]\n",
      "Video file: D1NT7.mp4\n"
     ]
    }
   ],
   "source": [
    "# Display a sample from the training dataset\n",
    "sample_data = train_dataset[0]\n",
    "\n",
    "print(\"Keys in sample:\", sample_data.keys())\n",
    "print(\"Input IDs shape:\", sample_data['input_ids'].shape)\n",
    "print(\"Pixel values shape:\", sample_data['pixel_values'].shape)\n",
    "print(\"Labels shape:\", sample_data['labels'].shape)\n",
    "\n",
    "raw_example = json.loads(train_dataset.raw_data[0])\n",
    "print(\"\\nOriginal conversation:\", raw_example['conversations'])\n",
    "print(\"Video file:\", raw_example['video'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Th3HTV6A_89"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd4mrynzlvD2"
   },
   "source": [
    "## Load model\n",
    "Next, load your InternVL model from the hub. This is a model with about 1 billion trainable parameters (as it combines a **Qwen2 1B language model** with a relatively low-parameter vision **InternViT encoder**). Do note that we load a model here which already has undergone supervised fine-tuning (SFT) instructions dataset. We can benefit from the fine-tuning that the model already has undergone.\n",
    "\n",
    "## Full fine-tuning, LoRa and Q-LoRa\n",
    "\n",
    "**Select the fine-tuning method.**\n",
    "\n",
    " For reference, fine-tuning a model using the AdamW optimizer (which is often used to optimize neural networks) with mixed precision, you need about 18 times the amount of parameters in GB of GPU RAM. So in this case, we would need 18x1 billion bytes = 18 GB of GPU RAM if we want to update all the parameters of the model. Not so huge right? But using PEFT approach it could be less.\n",
    "\n",
    "Some clever people came up with the LoRa method (LoRa is short for low-rank adapation). It allows to just freeze the existing weights and only train a couple of adapter layers on top of the base model. Hugging Face offers the separate [PEFT library](https://huggingface.co/docs/peft/main/en/index) for easy use of LoRa, along with other Parameter-Efficient Fine-Tuning methods.\n",
    "\n",
    "Moreover, one can not only freeze the existing base model but also quantize it (which means, shrinking down its size). A neural network's parameters are typically saved in either float32 (which means, 32 bits or 4 bytes are used to store each parameter value) or float16 (which means, 16 bits or half a byte - also called half precision). However, with some clever algorithms one can shrink each parameter to just 8 or 4 bits (half a byte!), without significant effect on final performance. Read all about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes.\n",
    "\n",
    "This means that we're going to shrink the size of the base 1B model considerably using 4-bit quantization, and then only train a couple of adapter layers on top using LoRa (in float16). This idea of combining LoRa with quantization is called Q-LoRa and is the most memory friendly version.\n",
    "\n",
    "There exist many forms of quantization, here we leverage the [BitsAndBytes integration](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some problems with attempting of import Qwen3Config:\n",
    "```\n",
    "--> 526 config, kwargs = AutoConfig.from_pretrained(\n",
    "    527     pretrained_model_name_or_path,\n",
    "    528     return_unused_kwargs=True,\n",
    "    529     trust_remote_code=trust_remote_code,\n",
    "    530     code_revision=code_revision,\n",
    "    531     _commit_hash=commit_hash,\n",
    "    532     **hub_kwargs,\n",
    "    533     **kwargs,\n",
    "    534 )\n",
    "    536 # if torch_dtype=auto was passed here, ensure to pass it on\n",
    "    537 if kwargs_orig.get(\"torch_dtype\", None) == \"auto\":\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1035, in AutoConfig.from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n",
    "   1033     if os.path.isdir(pretrained_model_name_or_path):\n",
    "   1034         config_class.register_for_auto_class()\n",
    "-> 1035     return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
    "   1036 elif \"model_type\" in config_dict:\n",
    "   1037     try:\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py:568, in PretrainedConfig.from_pretrained(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\n",
    "    562     if config_dict[\"model_type\"] != cls.model_type:\n",
    "    563         logger.warning(\n",
    "    564             f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n",
    "    565             f\"{cls.model_type}. This is not supported for all configurations of models and can yield errors.\"\n",
    "    566         )\n",
    "--> 568 return cls.from_dict(config_dict, **kwargs)\n",
    "\n",
    "File /opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py:734, in PretrainedConfig.from_dict(cls, config_dict, **kwargs)\n",
    "    731 # We remove it from kwargs so that it does not appear in `return_unused_kwargs`.\n",
    "    732 config_dict[\"attn_implementation\"] = kwargs.pop(\"attn_implementation\", None)\n",
    "--> 734 config = cls(**config_dict)\n",
    "    736 if hasattr(config, \"pruned_heads\"):\n",
    "    737     config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}\n",
    "\n",
    "File ~/.cache/huggingface/modules/transformers_modules/OpenGVLab/InternVL3_5-1B/2f71cf52542334823e48a46ffba0e2bc9add3446/configuration_internvl_chat.py:67, in InternVLChatConfig.__init__(self, vision_config, llm_config, use_backbone_lora, use_llm_lora, select_layer, force_image_size, downsample_ratio, template, dynamic_image_size, use_thumbnail, ps_version, min_dynamic_patch, max_dynamic_patch, **kwargs)\n",
    "     65     self.llm_config = Qwen3MoeConfig(**llm_config)\n",
    "     66 elif architecture == 'Qwen3ForCausalLM':\n",
    "---> 67     from transformers import Qwen3Config\n",
    "     68     self.llm_config = Qwen3Config(**llm_config)\n",
    "     69 else:\n",
    "\n",
    "ImportError: cannot import name 'Qwen3Config' from 'transformers' (/opt/conda/lib/python3.11/site-packages/transformers/__init__.py)\n",
    "```\n",
    "So replace it by Qwen2Config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "\n",
    "if not hasattr(transformers, \"Qwen3Config\"):\n",
    "    try:\n",
    "        from transformers import Qwen2Config as _Base\n",
    "    except ImportError:\n",
    "        from transformers import PretrainedConfig as _Base\n",
    "\n",
    "    class Qwen3Config(_Base):\n",
    "        model_type = \"qwen3\"\n",
    "        def __init__(self, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "    transformers.Qwen3Config = Qwen3Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogically we do it for Qwen3ForCausalLM, Qwen3MoeForCausalLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(transformers, \"Qwen3ForCausalLM\"):\n",
    "    class Qwen3ForCausalLM(PreTrainedModel):\n",
    "        def __init__(self, config): super().__init__(config)\n",
    "    transformers.Qwen3ForCausalLM = Qwen3ForCausalLM\n",
    "\n",
    "if not hasattr(transformers, \"Qwen3MoeForCausalLM\"):\n",
    "    class Qwen3MoeForCausalLM(PreTrainedModel):\n",
    "        def __init__(self, config): super().__init__(config)\n",
    "    transformers.Qwen3MoeForCausalLM = Qwen3MoeForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DQ0nTqbVlvD2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at OpenGVLab/InternVL3_5-1B were not used when initializing InternVLChatModel: ['language_model.model.layers.0.self_attn.k_norm.weight', 'language_model.model.layers.0.self_attn.q_norm.weight', 'language_model.model.layers.1.self_attn.k_norm.weight', 'language_model.model.layers.1.self_attn.q_norm.weight', 'language_model.model.layers.10.self_attn.k_norm.weight', 'language_model.model.layers.10.self_attn.q_norm.weight', 'language_model.model.layers.11.self_attn.k_norm.weight', 'language_model.model.layers.11.self_attn.q_norm.weight', 'language_model.model.layers.12.self_attn.k_norm.weight', 'language_model.model.layers.12.self_attn.q_norm.weight', 'language_model.model.layers.13.self_attn.k_norm.weight', 'language_model.model.layers.13.self_attn.q_norm.weight', 'language_model.model.layers.14.self_attn.k_norm.weight', 'language_model.model.layers.14.self_attn.q_norm.weight', 'language_model.model.layers.15.self_attn.k_norm.weight', 'language_model.model.layers.15.self_attn.q_norm.weight', 'language_model.model.layers.16.self_attn.k_norm.weight', 'language_model.model.layers.16.self_attn.q_norm.weight', 'language_model.model.layers.17.self_attn.k_norm.weight', 'language_model.model.layers.17.self_attn.q_norm.weight', 'language_model.model.layers.18.self_attn.k_norm.weight', 'language_model.model.layers.18.self_attn.q_norm.weight', 'language_model.model.layers.19.self_attn.k_norm.weight', 'language_model.model.layers.19.self_attn.q_norm.weight', 'language_model.model.layers.2.self_attn.k_norm.weight', 'language_model.model.layers.2.self_attn.q_norm.weight', 'language_model.model.layers.20.self_attn.k_norm.weight', 'language_model.model.layers.20.self_attn.q_norm.weight', 'language_model.model.layers.21.self_attn.k_norm.weight', 'language_model.model.layers.21.self_attn.q_norm.weight', 'language_model.model.layers.22.self_attn.k_norm.weight', 'language_model.model.layers.22.self_attn.q_norm.weight', 'language_model.model.layers.23.self_attn.k_norm.weight', 'language_model.model.layers.23.self_attn.q_norm.weight', 'language_model.model.layers.24.self_attn.k_norm.weight', 'language_model.model.layers.24.self_attn.q_norm.weight', 'language_model.model.layers.25.self_attn.k_norm.weight', 'language_model.model.layers.25.self_attn.q_norm.weight', 'language_model.model.layers.26.self_attn.k_norm.weight', 'language_model.model.layers.26.self_attn.q_norm.weight', 'language_model.model.layers.27.self_attn.k_norm.weight', 'language_model.model.layers.27.self_attn.q_norm.weight', 'language_model.model.layers.3.self_attn.k_norm.weight', 'language_model.model.layers.3.self_attn.q_norm.weight', 'language_model.model.layers.4.self_attn.k_norm.weight', 'language_model.model.layers.4.self_attn.q_norm.weight', 'language_model.model.layers.5.self_attn.k_norm.weight', 'language_model.model.layers.5.self_attn.q_norm.weight', 'language_model.model.layers.6.self_attn.k_norm.weight', 'language_model.model.layers.6.self_attn.q_norm.weight', 'language_model.model.layers.7.self_attn.k_norm.weight', 'language_model.model.layers.7.self_attn.q_norm.weight', 'language_model.model.layers.8.self_attn.k_norm.weight', 'language_model.model.layers.8.self_attn.q_norm.weight', 'language_model.model.layers.9.self_attn.k_norm.weight', 'language_model.model.layers.9.self_attn.q_norm.weight']\n",
      "- This IS expected if you are initializing InternVLChatModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing InternVLChatModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of InternVLChatModel were not initialized from the model checkpoint at OpenGVLab/InternVL3_5-1B and are newly initialized: ['language_model.model.layers.0.self_attn.k_proj.bias', 'language_model.model.layers.0.self_attn.q_proj.bias', 'language_model.model.layers.0.self_attn.v_proj.bias', 'language_model.model.layers.1.self_attn.k_proj.bias', 'language_model.model.layers.1.self_attn.q_proj.bias', 'language_model.model.layers.1.self_attn.v_proj.bias', 'language_model.model.layers.10.self_attn.k_proj.bias', 'language_model.model.layers.10.self_attn.q_proj.bias', 'language_model.model.layers.10.self_attn.v_proj.bias', 'language_model.model.layers.11.self_attn.k_proj.bias', 'language_model.model.layers.11.self_attn.q_proj.bias', 'language_model.model.layers.11.self_attn.v_proj.bias', 'language_model.model.layers.12.self_attn.k_proj.bias', 'language_model.model.layers.12.self_attn.q_proj.bias', 'language_model.model.layers.12.self_attn.v_proj.bias', 'language_model.model.layers.13.self_attn.k_proj.bias', 'language_model.model.layers.13.self_attn.q_proj.bias', 'language_model.model.layers.13.self_attn.v_proj.bias', 'language_model.model.layers.14.self_attn.k_proj.bias', 'language_model.model.layers.14.self_attn.q_proj.bias', 'language_model.model.layers.14.self_attn.v_proj.bias', 'language_model.model.layers.15.self_attn.k_proj.bias', 'language_model.model.layers.15.self_attn.q_proj.bias', 'language_model.model.layers.15.self_attn.v_proj.bias', 'language_model.model.layers.16.self_attn.k_proj.bias', 'language_model.model.layers.16.self_attn.q_proj.bias', 'language_model.model.layers.16.self_attn.v_proj.bias', 'language_model.model.layers.17.self_attn.k_proj.bias', 'language_model.model.layers.17.self_attn.q_proj.bias', 'language_model.model.layers.17.self_attn.v_proj.bias', 'language_model.model.layers.18.self_attn.k_proj.bias', 'language_model.model.layers.18.self_attn.q_proj.bias', 'language_model.model.layers.18.self_attn.v_proj.bias', 'language_model.model.layers.19.self_attn.k_proj.bias', 'language_model.model.layers.19.self_attn.q_proj.bias', 'language_model.model.layers.19.self_attn.v_proj.bias', 'language_model.model.layers.2.self_attn.k_proj.bias', 'language_model.model.layers.2.self_attn.q_proj.bias', 'language_model.model.layers.2.self_attn.v_proj.bias', 'language_model.model.layers.20.self_attn.k_proj.bias', 'language_model.model.layers.20.self_attn.q_proj.bias', 'language_model.model.layers.20.self_attn.v_proj.bias', 'language_model.model.layers.21.self_attn.k_proj.bias', 'language_model.model.layers.21.self_attn.q_proj.bias', 'language_model.model.layers.21.self_attn.v_proj.bias', 'language_model.model.layers.22.self_attn.k_proj.bias', 'language_model.model.layers.22.self_attn.q_proj.bias', 'language_model.model.layers.22.self_attn.v_proj.bias', 'language_model.model.layers.23.self_attn.k_proj.bias', 'language_model.model.layers.23.self_attn.q_proj.bias', 'language_model.model.layers.23.self_attn.v_proj.bias', 'language_model.model.layers.24.self_attn.k_proj.bias', 'language_model.model.layers.24.self_attn.q_proj.bias', 'language_model.model.layers.24.self_attn.v_proj.bias', 'language_model.model.layers.25.self_attn.k_proj.bias', 'language_model.model.layers.25.self_attn.q_proj.bias', 'language_model.model.layers.25.self_attn.v_proj.bias', 'language_model.model.layers.26.self_attn.k_proj.bias', 'language_model.model.layers.26.self_attn.q_proj.bias', 'language_model.model.layers.26.self_attn.v_proj.bias', 'language_model.model.layers.27.self_attn.k_proj.bias', 'language_model.model.layers.27.self_attn.q_proj.bias', 'language_model.model.layers.27.self_attn.v_proj.bias', 'language_model.model.layers.3.self_attn.k_proj.bias', 'language_model.model.layers.3.self_attn.q_proj.bias', 'language_model.model.layers.3.self_attn.v_proj.bias', 'language_model.model.layers.4.self_attn.k_proj.bias', 'language_model.model.layers.4.self_attn.q_proj.bias', 'language_model.model.layers.4.self_attn.v_proj.bias', 'language_model.model.layers.5.self_attn.k_proj.bias', 'language_model.model.layers.5.self_attn.q_proj.bias', 'language_model.model.layers.5.self_attn.v_proj.bias', 'language_model.model.layers.6.self_attn.k_proj.bias', 'language_model.model.layers.6.self_attn.q_proj.bias', 'language_model.model.layers.6.self_attn.v_proj.bias', 'language_model.model.layers.7.self_attn.k_proj.bias', 'language_model.model.layers.7.self_attn.q_proj.bias', 'language_model.model.layers.7.self_attn.v_proj.bias', 'language_model.model.layers.8.self_attn.k_proj.bias', 'language_model.model.layers.8.self_attn.q_proj.bias', 'language_model.model.layers.8.self_attn.v_proj.bias', 'language_model.model.layers.9.self_attn.k_proj.bias', 'language_model.model.layers.9.self_attn.q_proj.bias', 'language_model.model.layers.9.self_attn.v_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with QLoRA (4-bit quantization)\n",
      "Model type: <class 'transformers_modules.OpenGVLab.InternVL3_5-1B.2f71cf52542334823e48a46ffba0e2bc9add3446.modeling_internvl_chat.InternVLChatModel'>\n",
      "Model device: cuda:0\n",
      "Total parameters: 687,130,624\n"
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# QLoRA: model uses 4-bit quantization, which helps in reducing memory usage while maintaining performance.\n",
    "# Standard LoRA:  model is loaded with standard LoRA adaptations.\n",
    "# Full Fine-Tuning: no memory optimization are done. In that case Flash Attention is used to speed up training, if hardware supports it.\n",
    "\n",
    "# We use QLoRA as it's the most memory-efficient approach\n",
    "# This allows fine-tuning on consumer GPUs while maintaining good performance\n",
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "cfg = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "if hasattr(cfg, \"llm_config\"):\n",
    "    llm_arch = getattr(cfg.llm_config, \"architectures\", [\"\"])[0]\n",
    "    if llm_arch == \"Qwen3ForCausalLM\":\n",
    "        cfg.llm_config.architectures[0] = \"Qwen2ForCausalLM\"\n",
    "\n",
    "if USE_QLORA:\n",
    "    # Configure 4-bit quantization using bitsandbytes\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,  # Nested quantization for additional memory savings\n",
    "        bnb_4bit_quant_type=\"nf4\",  # NormalFloat 4-bit quantization\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # Compute in bfloat16 for better performance\n",
    "    )\n",
    "    \n",
    "    # Load model with 4-bit quantization\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        config=cfg,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"cuda:0\"\n",
    "    )\n",
    "    print(\"Model loaded with QLoRA (4-bit quantization)\")\n",
    "    \n",
    "elif USE_LORA:\n",
    "    # Load model in bfloat16 without quantization\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        use_flash_attn=False\n",
    "    )\n",
    "    print(\"Model loaded for standard LoRA\")\n",
    "    \n",
    "else:\n",
    "    # Full fine-tuning - load model without any optimizations\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"  # Use Flash Attention if supported\n",
    "    )\n",
    "    print(\"Model loaded for full fine-tuning\")\n",
    "\n",
    "# Print model info\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYDW50LslvD2",
    "outputId": "400cf245-42be-42d6-83f1-23211f77cb3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,716,288 || all params: 1,069,664,256 || trainable%: 0.8149\n"
     ]
    }
   ],
   "source": [
    "def find_all_linear_names(model):\n",
    "    # Only for LoRA ot QLoRA\n",
    "\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "    \n",
    "\n",
    "target_modules = find_all_linear_names(model.language_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "if USE_QLORA:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuv04h-DlvD2"
   },
   "source": [
    "## Define PyTorch Lightning Module for Video-LLaVA\n",
    "To streamline the training and evaluation of the Video-InternVL model, you can use [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html), which abstracts away much of the boilerplate code and provides a structured framework for model training. In this section, you need to define the InternVLModelPLModule, a custom PyTorch Lightning module that encapsulates the model, training loop, validation loop, and optimizer configuration.\n",
    "\n",
    "### InternVLModelPLModule Class\n",
    "\n",
    "The InternVLModelPLModule class inherits from LightningModule and includes methods for training, validation, and optimizer configuration. This setup ensures a clean and efficient training process.\n",
    "\n",
    "Basically, PyTorch Lightning will take care of all device placements (.to(device)) for us, as well as the backward pass, putting the model in training mode, etc.\n",
    "\n",
    "Notice the difference between a training step and an evaluation step:\n",
    "\n",
    "- a training step only consists of a forward pass, in which we compute the cross-entropy loss between the model's next token predictions and the ground truth (in parallel for all tokens, this technique is known as \"teacher forcing\"). The backward pass is handled by PyTorch Lightning.\n",
    "- an evaluation step consists of making the model autoregressively complete the prompt using the generate() method. After that, you compute an evaluation metric between the predicted sequences and the ground truth ones. This allows you to see how the model is improving over the course of training. The metric we use here is accuracy of answering the question.\n",
    "\n",
    "Besides that, you define the optimizer to use (AdamW is a good default choice) and the data loaders, which use the collate functions defined above to batch together items of the PyTorch datasets. Do note that AdamW is a pretty heavy optimizer in terms of memory requirements, but as we're training with QLoRa we only need to store optimizer states for the adapter layers. For full fine-tuning, one could take a look at more memory friendly optimizers such as 8-bit Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InternVLModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        \n",
    "        self.lr = config.get(\"lr\", 1e-4)\n",
    "        self.batch_size = config.get(\"batch_size\", 1)\n",
    "        self.max_epochs = config.get(\"max_epochs\", 2)\n",
    "        self.warmup_steps = config.get(\"warmup_steps\", 50)\n",
    "        \n",
    "        self.validation_outputs = []\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step: forward pass and loss computation\n",
    "        \"\"\"\n",
    "        # FIX: Updated to handle image_flags\n",
    "        input_ids, attention_mask, pixel_values_videos, labels, image_flags = batch\n",
    "        \n",
    "        # Reshape pixel_values for model [batch*frames, C, H, W]\n",
    "        if pixel_values_videos.ndim == 5:\n",
    "            batch_size, num_frames, c, h, w = pixel_values_videos.shape\n",
    "            pixel_values_videos = pixel_values_videos.reshape(batch_size * num_frames, c, h, w)\n",
    "            image_flags = image_flags.reshape(batch_size * num_frames)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values_videos,\n",
    "            labels=labels,\n",
    "            image_flags=image_flags  # FIX: Pass image_flags to model\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        \"\"\"\n",
    "        Validation step: generate answers and compute accuracy\n",
    "        \"\"\"\n",
    "        if batch is None:\n",
    "            return\n",
    "            \n",
    "        # FIX: Updated to handle image_flags\n",
    "        input_ids, attention_mask, pixel_values_videos, answer_choices, image_flags = batch\n",
    "        \n",
    "        # Reshape pixel_values for model\n",
    "        if pixel_values_videos.ndim == 5:\n",
    "            batch_size, num_frames, c, h, w = pixel_values_videos.shape\n",
    "            pixel_values_videos = pixel_values_videos.reshape(batch_size * num_frames, c, h, w)\n",
    "            image_flags = image_flags.reshape(batch_size * num_frames)\n",
    "        \n",
    "        # Generate answers\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values_videos,\n",
    "            image_flags=image_flags,  # FIX: Pass image_flags to generate\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id if hasattr(self.processor.tokenizer, 'pad_token_id') else self.processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Decode predictions\n",
    "        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Extract only generated part (after prompt)\n",
    "        prompts = self.processor.batch_decode(input_ids, skip_special_tokens=True)\n",
    "        generated_answers = []\n",
    "        \n",
    "        for gen_text, prompt in zip(generated_texts, prompts):\n",
    "            if gen_text.startswith(prompt):\n",
    "                answer = gen_text[len(prompt):].strip()\n",
    "            else:\n",
    "                answer = gen_text.strip()\n",
    "            generated_answers.append(answer)\n",
    "        \n",
    "        # Extract predicted choices (A, B, C, D)\n",
    "        predicted_choices = []\n",
    "        for answer in generated_answers:\n",
    "            choice = None\n",
    "            # Look for first choice letter in answer\n",
    "            for char in answer:\n",
    "                if char.upper() in ['A', 'B', 'C', 'D']:\n",
    "                    choice = char.upper()\n",
    "                    break\n",
    "            predicted_choices.append(choice if choice else \"\")\n",
    "        \n",
    "        # Compute accuracy\n",
    "        correct = sum(1 for pred, gt in zip(predicted_choices, answer_choices) if pred == gt)\n",
    "        total = len(answer_choices)\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        # Store for epoch metrics\n",
    "        self.validation_outputs.append({\n",
    "            'correct': correct,\n",
    "            'total': total,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "        \n",
    "        self.log(\"val_accuracy\", accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Compute and log epoch metrics\n",
    "        \"\"\"\n",
    "        if len(self.validation_outputs) > 0:\n",
    "            total_correct = sum(x['correct'] for x in self.validation_outputs)\n",
    "            total_samples = sum(x['total'] for x in self.validation_outputs)\n",
    "            epoch_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "            \n",
    "            self.log(\"val_epoch_accuracy\", epoch_accuracy, prog_bar=True, logger=True)\n",
    "            \n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Validation Epoch Results:\")\n",
    "            print(f\"Accuracy: {epoch_accuracy:.4f} ({total_correct}/{total_samples})\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Clear for next epoch\n",
    "            self.validation_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimizer and scheduler\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.lr,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Linear warmup scheduler\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < self.warmup_steps:\n",
    "                return float(current_step) / float(max(1, self.warmup_steps))\n",
    "            return 1.0\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            train_dataset, \n",
    "            collate_fn=simple_train_collate_fn,\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=2\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            eval_dataset, \n",
    "            collate_fn=simple_eval_collate_fn,\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Lightning module\n",
    "import lightning as L\n",
    "import torch\n",
    "\n",
    "class SimpleInternVLPLModule(L.LightningModule):\n",
    "    \"\"\"Simple Lightning module for InternVL fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, pixel_values, labels, image_flags = batch\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=image_flags,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, pixel_values, labels, image_flags = batch\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=image_flags,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "# Create model module\n",
    "simple_model_module = SimpleInternVLPLModule(model, learning_rate=1e-4)\n",
    "print(\"Simple Lightning module created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning module adapted from Yurii's notebook\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class InternVLModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, tokenizer, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.model.train()\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = config.get('batch_size', 1)\n",
    "        self.learning_rate = config.get('learning_rate', 1e-4)\n",
    "        self.warmup_steps = config.get('warmup_steps', 50)\n",
    "        self.max_epochs = config.get('max_epochs', 10)\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters(ignore=['model', 'tokenizer'])\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.model.train()\n",
    "        # Extract inputs\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        image_flags = batch['image_flags']\n",
    "        \n",
    "        # Forward pass\n",
    "        # Direct forward pass without PEFT wrapper issues\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = self.model.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                labels=labels,\n",
    "                image_flags=image_flags,\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        self.model.eval()\n",
    "        # For validation, we'll compute both loss and generation accuracy\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        image_flags = batch['image_flags']\n",
    "        answer_choices = batch.get('answer_choices', [])\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss = None\n",
    "        if labels is not None:\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                    outputs = self.model.base_model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        pixel_values=pixel_values,\n",
    "                        labels=labels,\n",
    "                        image_flags=image_flags,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                val_loss = outputs.loss\n",
    "                self.log('val_loss', val_loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "        # Generate responses for accuracy calculation, add generation prompt\n",
    "        response_ids = []\n",
    "        for input_id in input_ids:\n",
    "            decoded_text = self.tokenizer.decode(input_id, skip_special_tokens=False).strip()\n",
    "            # For InternVL format, we need to handle the chat template differently\n",
    "            if '[/INST]' in decoded_text:\n",
    "                response = decoded_text.split('[/INST]')[0] + '[/INST]'\n",
    "            else:\n",
    "                response = decoded_text + ' '  # Fallback\n",
    "            \n",
    "            response_id = self.tokenizer(\n",
    "                response,\n",
    "                return_tensors='pt',\n",
    "                padding='max_length',\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=False,\n",
    "            ).input_ids\n",
    "\n",
    "            response_ids.append(response_id)\n",
    "\n",
    "        response_ids = torch.cat(response_ids).to(self.model.device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                generated_ids = self.model.base_model.generate(\n",
    "                    input_ids=response_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    image_flags=image_flags,\n",
    "                    max_new_tokens=10,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "        print(f\"\\n=== Epoch {self.current_epoch}, Batch {batch_idx} ===\")\n",
    "        for i in range(min(len(generated_ids), 2)):  # First 2 samples\n",
    "            input_text = self.tokenizer.decode(response_ids[i], skip_special_tokens=True).strip()\n",
    "            generated_text = self.tokenizer.decode(generated_ids[i], skip_special_tokens=True).strip()\n",
    "            expected_answer = answer_choices[i] if i < len(answer_choices) else ''\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"  Expected: '{expected_answer}'\")\n",
    "            print(f\"  Input: '{input_text}'\")\n",
    "            print(f\"  Generated: '{generated_text}'\")\n",
    "            print(f\"  Match: {generated_text.startswith(expected_answer) if expected_answer else 'N/A'}\")\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = self._calculate_accuracy(generated_ids, answer_choices, response_ids)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_accuracy': accuracy}\n",
    "    \n",
    "    def _calculate_accuracy(self, generated_ids, answer_choices, response_ids):\n",
    "        \"\"\"Calculate accuracy by comparing generated text with answer choices\"\"\"\n",
    "        if not answer_choices:\n",
    "            print('############# There is no answers #########')\n",
    "            return 0.0\n",
    "        \n",
    "        correct = 0\n",
    "        total = len(generated_ids)\n",
    "        \n",
    "        for i in range(total):\n",
    "            generated_tokens = generated_ids[i]\n",
    "            # Decode generated text\n",
    "            generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Get expected answer (first character: A, B, C, or D)\n",
    "            expected_answer = answer_choices[i] if i < len(answer_choices) else ''\n",
    "            \n",
    "            # Check if generated text starts with the expected answer\n",
    "            if expected_answer and generated_text.startswith(expected_answer):\n",
    "                correct += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer and learning rate scheduler\"\"\"\n",
    "        # Separate parameters for different weight decay\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.config.get('weight_decay', 0.01),\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        # Calculate total training steps\n",
    "        train_loader = self.train_dataloader()\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        total_steps = steps_per_epoch * self.max_epochs\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            train_dataset, \n",
    "            collate_fn=train_collate_fn,\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=4\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            eval_dataset, \n",
    "            collate_fn=eval_collate_fn,\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "# Create model module with Yurii's configuration\n",
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": 1,\n",
    "    \"max_epochs\": 1,\n",
    "    \"warmup_steps\": 50,\n",
    "    \"weight_decay\": 0.01,\n",
    "}\n",
    "\n",
    "model_module = InternVLModelPLModule(config, tokenizer, model)\n",
    "\n",
    "print(\"Lightning module adapted from Yurii's notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-EncDAllvD2"
   },
   "source": [
    "## Define callbacks\n",
    "Optionally, Lightning allows to define so-called [callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html), which are arbitrary pieces of code that can be executed during training.\n",
    "\n",
    "You'd better use the EarlyStopping callback of Lightning, which will automatically stop training once the evaluation metric (edit distance in our case) doesn't improve after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2KRo-rclvD2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"InternVL3.5_finetuning_qlora\")\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "# FIX: Monitor val_epoch_accuracy instead of val_accuracy\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_epoch_accuracy\",\n",
    "                                    patience=3, verbose=False, mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-ylKSrMlvD2"
   },
   "source": [
    "## Train!\n",
    " Trainer class supports many more flags. See the [docs](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Test the complete pipeline before training\n",
    "print(\"Testing complete pipeline...\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\n1. Testing data loading...\")\n",
    "try:\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"✅ Train sample loaded successfully\")\n",
    "    print(f\"   Keys: {list(sample.keys())}\")\n",
    "    print(f\"   Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"   Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "    print(f\"   Image flags shape: {sample['image_flags'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading train sample: {e}\")\n",
    "\n",
    "# Test collate functions  \n",
    "print(\"\\n2. Testing collate functions...\")\n",
    "try:\n",
    "    # Create small batches - get multiple samples to test batching\n",
    "    train_samples = [train_dataset[0], train_dataset[1]]\n",
    "    train_batch = simple_train_collate_fn(train_samples)\n",
    "    print(f\"✅ Train collate function works\")\n",
    "    print(f\"   Batch shapes: input_ids={train_batch[0].shape}, attention_mask={train_batch[1].shape}\")\n",
    "    print(f\"   Pixel values shape: {train_batch[2].shape}, labels shape: {train_batch[3].shape}\")\n",
    "    print(f\"   Image flags shape: {train_batch[4].shape}\")\n",
    "    \n",
    "    eval_samples = [eval_dataset[0], eval_dataset[1]]\n",
    "    eval_batch = simple_eval_collate_fn(eval_samples)\n",
    "    print(f\"✅ Eval collate function works\")\n",
    "    print(f\"   Answer choices: {len(eval_batch[3])} answers\")\n",
    "    print(f\"   Image flags shape: {eval_batch[4].shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in collate functions: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test model forward pass\n",
    "print(\"\\n3. Testing model forward pass...\")\n",
    "try:\n",
    "    # Move batch to device\n",
    "    device = next(model.parameters()).device\n",
    "    input_ids, attention_mask, pixel_values, labels, image_flags = train_batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    image_flags = image_flags.to(device)\n",
    "    \n",
    "    # Reshape pixel values and image_flags for model\n",
    "    if pixel_values.ndim == 5:\n",
    "        batch_size, num_frames, c, h, w = pixel_values.shape\n",
    "        pixel_values = pixel_values.reshape(batch_size * num_frames, c, h, w).to(device)\n",
    "        image_flags = image_flags.reshape(batch_size * num_frames).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels,\n",
    "            image_flags=image_flags\n",
    "        )\n",
    "    \n",
    "    print(f\"✅ Model forward pass works\")\n",
    "    print(f\"   Loss: {outputs.loss.item():.4f}\")\n",
    "    print(f\"   Logits shape: {outputs.logits.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in model forward pass: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test model generation\n",
    "print(\"\\n4. Testing model generation...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Use smaller batch for generation test\n",
    "        gen_input_ids = input_ids[:1]  # Use just first sample\n",
    "        gen_attention_mask = attention_mask[:1]\n",
    "        \n",
    "        # Calculate frames for first video\n",
    "        first_video_frames = train_samples[0]['pixel_values'].shape[0]\n",
    "        gen_pixel_values = pixel_values[:first_video_frames]  # First video only\n",
    "        gen_image_flags = image_flags[:first_video_frames]\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            input_ids=gen_input_ids,\n",
    "            attention_mask=gen_attention_mask,\n",
    "            pixel_values=gen_pixel_values,\n",
    "            image_flags=gen_image_flags,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    generated_text = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"✅ Model generation works\")\n",
    "    print(f\"   Generated text length: {len(generated_text)} chars\")\n",
    "    print(f\"   Generated: {generated_text[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in model generation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test Lightning module\n",
    "print(\"\\n5. Testing Lightning module...\")\n",
    "try:\n",
    "    # Test training step\n",
    "    loss = model_module.training_step(train_batch, 0)\n",
    "    print(f\"✅ Lightning training step works\")\n",
    "    print(f\"   Training loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # Test validation step\n",
    "    val_accuracy = model_module.validation_step(eval_batch, 0)\n",
    "    print(f\"✅ Lightning validation step works\")\n",
    "    print(f\"   Validation accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in Lightning module: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PIPELINE TEST COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"If all tests passed ✅, you can proceed with training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liZhW7NhlvD3"
   },
   "outputs": [],
   "source": [
    "# Test the pipeline\n",
    "print(\"Testing pipeline...\")\n",
    "try:\n",
    "    # Test dataset\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample keys: {sample.keys()}\")\n",
    "    print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "    print(f\"Image flags shape: {sample['image_flags'].shape}\")\n",
    "    \n",
    "    # Test collate function\n",
    "    train_samples = [train_dataset[0], train_dataset[1]]\n",
    "    batch = simple_train_collate_fn(train_samples)\n",
    "    \n",
    "    input_ids, attention_mask, pixel_values, labels, image_flags = batch\n",
    "    print(f\"\\nBatch shapes:\")\n",
    "    print(f\"  input_ids: {input_ids.shape}\")\n",
    "    print(f\"  attention_mask: {attention_mask.shape}\")\n",
    "    print(f\"  pixel_values: {pixel_values.shape}\")\n",
    "    print(f\"  labels: {labels.shape}\")\n",
    "    print(f\"  image_flags: {image_flags.shape}\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=image_flags,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n✅ Forward pass successful!\")\n",
    "    print(f\"   Loss: {outputs.loss:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1XGM6QflvD3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the pipeline with new template structure\n",
    "print(\"Testing pipeline with template structure...\")\n",
    "try:\n",
    "    # Test dataset\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample keys: {list(sample.keys())}\")\n",
    "    print(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"Pixel values shape: {sample['pixel_values'].shape}\")\n",
    "    print(f\"Image flags shape: {sample['image_flags'].shape}\")\n",
    "    print(f\"Answer choice: {sample['answer_choice']}\")\n",
    "    \n",
    "    # Test collate function\n",
    "    train_samples = [train_dataset[0]]\n",
    "    batch = train_collate_fn(train_samples)\n",
    "    \n",
    "    print(f\"\\nBatch format: dict\")\n",
    "    print(f\"Batch keys: {list(batch.keys())}\")\n",
    "    print(f\"  input_ids: {batch['input_ids'].shape}\")\n",
    "    print(f\"  attention_mask: {batch['attention_mask'].shape}\")\n",
    "    print(f\"  pixel_values: {batch['pixel_values'].shape}\")\n",
    "    print(f\"  image_flags: {batch['image_flags'].shape}\")\n",
    "    print(f\"  labels: {batch['labels'].shape}\")\n",
    "    print(f\"  answer_choices: {batch['answer_choices']}\")\n",
    "    \n",
    "    # Test forward pass with Lightning module\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            pixel_values=batch['pixel_values'],\n",
    "            image_flags=batch['image_flags'],\n",
    "            labels=batch['labels']\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n✅ Forward pass successful!\")\n",
    "    print(f\"   Loss: {outputs.loss:.4f}\")\n",
    "    \n",
    "    # Test Lightning module training step\n",
    "    model_module.train()\n",
    "    loss = model_module.training_step(batch, 0)\n",
    "    print(f\"✅ Lightning training step successful!\")\n",
    "    print(f\"   Training loss: {loss:.4f}\")\n",
    "    \n",
    "    print(\"\\n✅ All tests passed! Template structure working correctly.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbEdmdrn_K_l"
   },
   "outputs": [],
   "source": [
    "# DataLoaders are now defined inside the Lightning module\n",
    "# The module will automatically create train_dataloader() and val_dataloader()\n",
    "print(\"DataLoaders are now handled internally by the Lightning module\")\n",
    "print(f\"Training configuration: {config}\")\n",
    "\n",
    "# Test the adapted module\n",
    "print(\"\\nTesting adapted Lightning module...\")\n",
    "try:\n",
    "    # Test training step\n",
    "    train_batch = next(iter(DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=1)))\n",
    "    loss = model_module.training_step(train_batch, 0)\n",
    "    print(f\"✅ Training step successful: {loss:.4f}\")\n",
    "    \n",
    "    # Test validation step\n",
    "    eval_batch = next(iter(DataLoader(eval_dataset, collate_fn=eval_collate_fn, batch_size=1)))\n",
    "    val_results = model_module.validation_step(eval_batch, 0)\n",
    "    print(f\"✅ Validation step successful: loss={val_results.get('val_loss', 'N/A')}, accuracy={val_results.get('val_accuracy', 'N/A'):.3f}\")\n",
    "    \n",
    "    print(\"✅ Lightning module adapted successfully from Yurii's notebook!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQPiR5IV89qs"
   },
   "source": [
    "# Training with adapted Lightning module (Yurii's version)\n",
    "print(\"Starting training with Yurii's Lightning module...\")\n",
    "\n",
    "try:\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config[\"max_epochs\"],\n",
    "        accelerator=\"auto\",\n",
    "        precision=\"16-mixed\",\n",
    "        enable_checkpointing=False,\n",
    "        logger=False,\n",
    "        gradient_clip_val=1.0,\n",
    "        accumulate_grad_batches=1\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model_module)\n",
    "    \n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    \n",
    "    # Test final model\n",
    "    model_module.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(DataLoader(eval_dataset, collate_fn=eval_collate_fn, batch_size=1)))\n",
    "        \n",
    "        # Extract components for manual generation test\n",
    "        input_ids = test_batch['input_ids']\n",
    "        pixel_values = test_batch['pixel_values']\n",
    "        image_flags = test_batch['image_flags']\n",
    "        \n",
    "        # Prepare prompt for generation\n",
    "        response_ids = []\n",
    "        for input_id in input_ids:\n",
    "            decoded_text = tokenizer.decode(input_id, skip_special_tokens=False).strip()\n",
    "            if '[/INST]' in decoded_text:\n",
    "                response = decoded_text.split('[/INST]')[0] + '[/INST]'\n",
    "            else:\n",
    "                response = decoded_text + ' '\n",
    "            \n",
    "            response_id = tokenizer(\n",
    "                response,\n",
    "                return_tensors='pt'\n",
    "            ).input_ids\n",
    "            response_ids.append(response_id)\n",
    "        \n",
    "        response_ids = torch.cat(response_ids).to(model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        generated_ids = model_module.model.base_model.generate(\n",
    "            input_ids=response_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=image_flags,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        expected_answer = test_batch.get('answer_choices', [''])[0]\n",
    "        \n",
    "        print(f\"\\nFinal test:\")\n",
    "        print(f\"  Expected: '{expected_answer}'\")\n",
    "        print(f\"  Generated: '{generated_text}'\")\n",
    "        print(f\"  Match: {generated_text.startswith(expected_answer) if expected_answer else 'N/A'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jc0JHzIY89qs"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "# Option 1: Load from HuggingFace Hub (if you pushed it)\n",
    "# processor_inference = AutoProcessor.from_pretrained(REPO_ID, trust_remote_code=True)\n",
    "# model_inference = AutoModelForCausalLM.from_pretrained(REPO_ID, trust_remote_code=True, device_map=\"auto\")\n",
    "\n",
    "# Option 2: Load from local checkpoint\n",
    "# First load the base model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_inference = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# If using PEFT/LoRA, load the adapter weights\n",
    "# model_inference = PeftModel.from_pretrained(model_inference, \"path/to/checkpoint\")\n",
    "\n",
    "processor_inference = processor  # Use the same processor\n",
    "\n",
    "print(\"Model loaded for inference\")\n",
    "print(f\"Model device: {model_inference.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Lqxte3U89qs"
   },
   "source": [
    "See one example from the validation set here and plot 8 frames to see what is happening in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNw1jE0n89qw"
   },
   "outputs": [],
   "source": [
    "# Set up Trainer with custom DataLoaders\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "print(\"🚀 Setting up Lightning Trainer...\")\n",
    "\n",
    "# Set up Wandb logger (if API key is provided)\n",
    "if WANDB_API_KEY:\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"internvl-mvbench-finetune\",\n",
    "        name=f\"internvl3.5-1b-qlora-mvbench\",\n",
    "        log_model=True\n",
    "    )\n",
    "    print(\"✅ Wandb logger initialized\")\n",
    "else:\n",
    "    wandb_logger = None\n",
    "    print(\"⚠️ Wandb API key not provided, using console logging only\")\n",
    "\n",
    "# Set up callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints\",\n",
    "    filename=\"internvl-mvbench-{epoch:02d}-{val_accuracy:.3f}\",\n",
    "    save_top_k=2,\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_last=True,\n",
    "    every_n_epochs=1\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "class WandbModelUpload(L.Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    precision=config[\"precision\"],\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    accumulate_grad_batches=2,  # Effective batch size = 2\n",
    "    gradient_clip_val=1.0,\n",
    "    val_check_interval=0.5,  # Validate twice per epoch\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[checkpoint_callback, lr_monitor],\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    deterministic=True,\n",
    "    benchmark=False\n",
    ")\n",
    "\n",
    "# Set up data for trainer\n",
    "trainer.model = model_module\n",
    "trainer.train_dataloader = train_dataloader\n",
    "trainer.val_dataloaders = val_dataloader\n",
    "\n",
    "print(\"✅ Lightning Trainer configured successfully\")\n",
    "print(f\"  Max epochs: {config['max_epochs']}\")\n",
    "print(f\"  Precision: {config['precision']}\")\n",
    "print(f\"  Effective batch size: {config['batch_size'] * 2} (with accumulation)\")\n",
    "print(f\"  Validation check interval: 0.5 epoch\")\n",
    "print(f\"  Checkpoint directory: ./checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWWHU1xB89qx"
   },
   "source": [
    "Next you need to prepare the video for the model, along with the text prompt we used during training. You need to apply the chat template to make sure the format is respected.\n",
    "\n",
    "Notice that this is exactly the same as what you did for the evaluation data collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBWCEdiT89qx"
   },
   "outputs": [],
   "source": [
    "# Prepare the input for inference\n",
    "video_frames = [Image.fromarray(frame) for frame in clip]\n",
    "\n",
    "# Format the conversation for inference (question only, without answer)\n",
    "conversation = [sample['conversations'][0]]  # Only the human question\n",
    "prompt_text = processor_inference.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"Prompt:\\n{prompt_text}\\n\")\n",
    "\n",
    "# Process the input\n",
    "inputs = processor_inference(\n",
    "    text=[prompt_text],\n",
    "    videos=[video_frames],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model_inference.device)\n",
    "\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Pixel values shape: {inputs.get('pixel_values_videos', torch.tensor([])).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline\n",
    "try:\n",
    "    train_samples = [train_dataset[0], train_dataset[1]]\n",
    "    test_batch = simple_train_collate_fn(train_samples)\n",
    "    \n",
    "    input_ids, attention_mask, pixel_values_videos, labels, image_flags = test_batch\n",
    "    \n",
    "    print(f\"Batch created:\")\n",
    "    print(f\"  input_ids: {input_ids.shape}\")\n",
    "    print(f\"  pixel_values: {pixel_values_videos.shape} (dtype: {pixel_values_videos.dtype})\")\n",
    "    print(f\"  image_flags: {image_flags.shape}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values_videos,\n",
    "            image_flags=image_flags,\n",
    "            labels=labels\n",
    "        )\n",
    "    \n",
    "    print(f\"Forward pass successful: loss {outputs.loss:.4f}\")\n",
    "    \n",
    "    eval_samples = [eval_dataset[0]]\n",
    "    eval_batch = simple_eval_collate_fn(eval_samples)\n",
    "    gen_input_ids, gen_attention_mask, gen_pixel_values, _, gen_image_flags = eval_batch\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=gen_input_ids,\n",
    "            attention_mask=gen_attention_mask,\n",
    "            pixel_values=gen_pixel_values,\n",
    "            image_flags=gen_image_flags,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    generated_text = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Generation works: {generated_text[:50]}...\")\n",
    "    print(\"Pipeline test completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"🔧 Testing complete pipeline with FIXED PLModule and autocast...\")\n",
    "\n",
    "try:\n",
    "    # Create test batch using our FIXED collators\n",
    "    print(\"Creating test batch...\")\n",
    "    train_samples = [train_dataset[0], train_dataset[1]]\n",
    "    test_batch = simple_train_collate_fn_fixed(train_samples)\n",
    "    \n",
    "    input_ids, attention_mask, pixel_values_videos, labels, image_flags = test_batch\n",
    "    \n",
    "    print(f\"✅ Batch created successfully:\")\n",
    "    print(f\"  input_ids: {input_ids.shape} (device: {input_ids.device})\")\n",
    "    print(f\"  attention_mask: {attention_mask.shape} (device: {attention_mask.device})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a summary table with results\n",
    "results = {\n",
    "    \"Task\": [\"Action Sequence\"],\n",
    "    \"Model\": [\"InternVL3.5-1B\"],\n",
    "    \"Fine-tuning Method\": [\"QLoRA (4-bit + LoRA)\"],\n",
    "    \"Train Dataset Size\": [len(train_dataset)],\n",
    "    \"Test Dataset Size\": [len(eval_dataset)],\n",
    "    \"Metric\": [\"Accuracy\"],\n",
    "    \"Test Accuracy\": [\"[TO BE FILLED AFTER TRAINING]\"],\n",
    "    \"Training Epochs\": [config[\"max_epochs\"]],\n",
    "    \"Learning Rate\": [config[\"lr\"]],\n",
    "    \"Batch Size\": [config[\"batch_size\"]],\n",
    "    \"LoRA Rank\": [16],\n",
    "    \"LoRA Alpha\": [32],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# After training, you can update the test accuracy by running evaluation on the full test set\n",
    "# Example: test_accuracy = trainer.validate(model_module)\n",
    "# Then update: results[\"Test Accuracy\"] = [f\"{test_accuracy:.4f}\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFSVXwN889qx"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"Starting InternVL3.5-1B fine-tuning...\")\n",
    "\n",
    "try:\n",
    "    trainer.fit(\n",
    "        model=model_module,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed successfully!\")\n",
    "    \n",
    "    # Test trained model\n",
    "    model_module.eval()\n",
    "    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        test_batch = next(iter(val_dataloader))\n",
    "        input_ids, attention_mask, pixel_values, answer_choices, image_flags = test_batch\n",
    "        \n",
    "        generated_ids = model_module.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=image_flags,\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        generated_text = model_module.processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        print(f\"Generated: {generated_text[:100]}...\")\n",
    "        print(f\"Expected: {answer_choices[0]}\")\n",
    "    \n",
    "    print(\"Fine-tuning completed! Model ready for use.\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted by user\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c80b263628d4c37b129278c8cb80fc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e40a2be14574c9cab05c6a2066506c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9e922c6bf14487599e94963b8afd747",
      "max": 188,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7dd46cf21684684b12361c82c5e01ca",
      "value": 188
     }
    },
    "24b4a2b91c7e484bacb5a543a60dff44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c53a11f8b4d48bab7bb07005e09dab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_606c508016d34f1bb86d21b2c4e3cfc3",
      "placeholder": "​",
      "style": "IPY_MODEL_8c9c50dc1265410e82248451e1fdd156",
      "value": "Map: 100%"
     }
    },
    "606c508016d34f1bb86d21b2c4e3cfc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6997d1b4ec2e40268bbfe56ff5ad1ddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c13ed62e28e44468633fdfa010a07cb",
      "placeholder": "​",
      "style": "IPY_MODEL_24b4a2b91c7e484bacb5a543a60dff44",
      "value": " 188/188 [00:00&lt;00:00, 1990.77 examples/s]"
     }
    },
    "7c13ed62e28e44468633fdfa010a07cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c9c50dc1265410e82248451e1fdd156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c44886c523194321a45ac116a99d7e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c53a11f8b4d48bab7bb07005e09dab2",
       "IPY_MODEL_1e40a2be14574c9cab05c6a2066506c9",
       "IPY_MODEL_6997d1b4ec2e40268bbfe56ff5ad1ddc"
      ],
      "layout": "IPY_MODEL_0c80b263628d4c37b129278c8cb80fc2"
     }
    },
    "e7dd46cf21684684b12361c82c5e01ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9e922c6bf14487599e94963b8afd747": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
