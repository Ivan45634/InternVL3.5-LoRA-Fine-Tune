{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1: Fine-tuning InternVL3.5-1B on MVBench\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates fine-tuning of the InternVL3.5-1B multimodal model on a video understanding task from the MVBench benchmark.\n",
    "\n",
    "## What this notebook covers:\n",
    "1. ✅ Dataset loading and preprocessing (MVBench - Action Sequence task)\n",
    "2. ✅ Train/test split (80/20)\n",
    "3. ✅ Model loading with QLoRA (4-bit quantization + LoRA adapters)\n",
    "4. ✅ Training and evaluation loops using PyTorch Lightning\n",
    "5. ✅ Accuracy metric for multiple-choice questions\n",
    "6. ✅ Logging with TensorBoard and checkpointing\n",
    "7. ✅ Inference pipeline for video question answering\n",
    "\n",
    "## Requirements:\n",
    "- GPU with at least 16GB VRAM (A100 recommended, but RTX 4090 works)\n",
    "- ~1TB storage for MVBench videos\n",
    "- Python packages: transformers, peft, bitsandbytes, pytorch-lightning, datasets, decord, av\n",
    "\n",
    "## Before running:\n",
    "1. Update API keys and tokens in Cell 4 (WANDB_API_KEY, REPO_ID, HF token)\n",
    "2. Ensure sufficient disk space for video downloads\n",
    "3. Consider applying the processor patch if you encounter video processing errors (see note below)\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we start, make sure you have the following:\n",
    "\n",
    "- Access to a GPU (preferably A100 since videos require high sequence lengths).\n",
    "- Familiarity with Hugging Face’s Transformers library.\n",
    "- Pre-install necessary packages by running the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-T8dupbwlvDx",
    "outputId": "3ef6018b-fa1e-4a5f-8961-97e65ef40c12"
   },
   "outputs": [],
   "source": [
    "# !pip install -U -q pip setuptools wheel\n",
    "# !pip install --upgrade --no-deps -q numpy==2.0.0 huggingface-hub==0.35.3\n",
    "# !pip install -U -q huggingface-hub>=0.23.0 pyarrow>=21.0.0\n",
    "# !pip install -U -q transformers==4.45.0 accelerate bitsandbytes peft datasets==4.1.0\n",
    "# !pip install -q av decord\n",
    "# !pip install -q lightning\n",
    "# !pip install -q wandb\n",
    "# !pip install -q timm deepspeed flash_attn\n",
    "# !pip install -q nltk\n",
    "# !pip install -q opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q transformers accelerate bitsandbytes peft datasets\n",
    "!pip install -q av decord\n",
    "!pip install -q lightning\n",
    "!pip install -q pyarrow==21.0.0\n",
    "!pip install -q wandb\n",
    "!pip install -q timm deepspeed flash_attn\n",
    "!pip install -U -q tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Choices and Justification\n",
    "\n",
    "### Task Selection\n",
    "**Selected Task:** Action Sequence from MVBench\n",
    "- This task involves understanding temporal sequences of actions in videos\n",
    "- It is a multiple-choice task, making it suitable for accuracy-based evaluation\n",
    "- The dataset has 188 valid samples after filtering missing videos\n",
    "\n",
    "### Fine-tuning Method: QLoRA (Quantized Low-Rank Adaptation)\n",
    "\n",
    "**Why QLoRA?**\n",
    "\n",
    "1. **Memory Efficiency**: \n",
    "   - 4-bit quantization reduces model memory footprint by ~75%\n",
    "   - Enables fine-tuning on consumer GPUs (e.g., single A100 or even RTX 4090)\n",
    "   - Only adapter parameters are trained, not the full model\n",
    "\n",
    "2. **Performance**:\n",
    "   - Research shows QLoRA achieves near-identical performance to full fine-tuning\n",
    "   - NormalFloat 4-bit (NF4) quantization preserves model quality\n",
    "   - LoRA rank of 16 provides good balance between capacity and efficiency\n",
    "\n",
    "3. **Practical Considerations**:\n",
    "   - Faster iteration and experimentation\n",
    "   - Lower computational costs\n",
    "   - Easier to deploy (smaller checkpoint files)\n",
    "\n",
    "**Configuration:**\n",
    "- Quantization: 4-bit NF4 with double quantization\n",
    "- LoRA rank: 16\n",
    "- LoRA alpha: 32\n",
    "- Target modules: All linear layers in the language model (excluding vision encoder and projector)\n",
    "- Compute dtype: bfloat16\n",
    "\n",
    "### Metric: Accuracy\n",
    "- Multiple-choice questions have 4 options (A, B, C, D)\n",
    "- Accuracy is the standard metric for such tasks\n",
    "- Easy to interpret and compare with baselines\n",
    "\n",
    "### Training Configuration\n",
    "- Optimizer: AdamW with weight decay (0.01)\n",
    "- Learning rate: 1e-4 with linear warmup (50 steps)\n",
    "- Batch size: 1 with gradient accumulation (8 steps)\n",
    "- Effective batch size: 8\n",
    "- Mixed precision: bfloat16\n",
    "- Max epochs: 2 (to prevent overfitting on small dataset)\n",
    "- Early stopping: patience of 3 epochs based on validation accuracy\n",
    "\n",
    "### Data Processing\n",
    "- Videos: 8 frames uniformly sampled from each video\n",
    "- Resolution: 448x448 (dynamic tiling)\n",
    "- Train/Test split: 80/20 (stratified)\n",
    "- Data augmentation: None (to maintain temporal consistency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTG2AJZT89qk"
   },
   "source": [
    "## Fine-tune InternVL 1B. on MMBench dataset\n",
    "\n",
    "In this notebook, you need to fine-tune the [InternVL](https://huggingface.co/OpenGVLab/InternVL3_5-1B) model on [MVBench](https://huggingface.co/datasets/OpenGVLab/MVBench) dataset which is comprised of various video-related tasks. Note that MMBench is quite small and is not made for tuning. So firstly you need to split it into training/testing parts.\n",
    "\n",
    "The goal for the model in this notebook is to answer given multiple choice questions based on the video. The questions can be realetd to temporal aspects of the video, pose prediction and so on.\n",
    "Sources:\n",
    "\n",
    "* InternVL [documentation](https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html)\n",
    "* InternVL [checkpoint on the hub](https://huggingface.co/OpenGVLab/InternVL2-1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szf17AKL89qm"
   },
   "source": [
    "## Define variables\n",
    "\n",
    "We'll first set some variables useful througout this notebook and doo all the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJtnWc3b89qn",
    "outputId": "ecdb83b2-e427-4546-9e69-4c733a8300b5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import av\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import bisect\n",
    "import shutil\n",
    "import traceback\n",
    "import numpy as np\n",
    "from nltk import edit_distance\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from typing import Dict, Any, List, Union, Tuple\n",
    "\n",
    "from transformers import (AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer,\n",
    "                          HfArgumentParser, Trainer, TrainingArguments,\n",
    "                          set_seed, AutoProcessor)\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from PIL import Image, ImageFile, PngImagePlugin, UnidentifiedImageError\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping, Callback\n",
    "\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 4096\n",
    "MODEL_ID = \"OpenGVLab/InternVL3_5-1B\"\n",
    "REPO_ID = \"Ivan1008/internvl3.5-finetune\"\n",
    "\n",
    "# os.environ[\"WANDB_API_KEY\"] = getpass(\"Input wandb api key: \")\n",
    "# os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "\n",
    "# access_token = getpass(\"Input hugging face access token: \")\n",
    "# login(access_token)\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8JeFyKyoT4X",
    "outputId": "1a8da900-90e2-4587-c169-9e71f9c66d85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'InternVL'...\n",
      "remote: Enumerating objects: 3748, done.\u001b[K\n",
      "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
      "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
      "remote: Total 3748 (delta 32), reused 25 (delta 25), pack-reused 3688 (from 3)\u001b[K\n",
      "Receiving objects: 100% (3748/3748), 39.63 MiB | 31.05 MiB/s, done.\n",
      "Resolving deltas: 100% (2288/2288), done.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/OpenGVLab/InternVL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "si7CcgJAoc7S",
    "outputId": "191cf088-b4e2-48f4-fd12-93c9c8033669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\tzero_stage1_config.json\n",
      "eval\t\tzero_stage2_config.json\n",
      "evaluate.sh\tzero_stage3_config.json\n",
      "examples\tzero_stage3_config_100b.json\n",
      "internvl\tzero_stage3_config_100b_1e7_offload.json\n",
      "pyproject.toml\tzero_stage3_config_100b_1e8.json\n",
      "shell\t\tzero_stage3_config_34b.json\n",
      "tools\t\tzero_stage3_config_70b.json\n"
     ]
    }
   ],
   "source": [
    "! ls InternVL/internvl_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uXmhQGtEoT9h"
   },
   "outputs": [],
   "source": [
    "sys.path.append('./InternVL/internvl_chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bhtzSpMpoT-7"
   },
   "outputs": [],
   "source": [
    "from internvl.dist_utils import init_dist\n",
    "# from internvl.model.internlm2.modeling_internlm2 import InternLM2ForCausalLM\n",
    "from internvl.model.internvl_chat import (InternVisionConfig,\n",
    "                                          InternVisionModel,\n",
    "                                          InternVLChatConfig,\n",
    "                                          InternVLChatModel)\n",
    "# from internvl.patch import (concat_pad_data_collator,\n",
    "#                             replace_llama_rmsnorm_with_fused_rmsnorm,\n",
    "#                             replace_train_sampler)\n",
    "from internvl.train.constants import (BOX_END_TOKEN, BOX_START_TOKEN,\n",
    "                                      IMG_CONTEXT_TOKEN, IMG_END_TOKEN,\n",
    "                                      IMG_START_TOKEN, QUAD_END_TOKEN,\n",
    "                                      QUAD_START_TOKEN, REF_END_TOKEN,\n",
    "                                      IMAGENET_MEAN, IMAGENET_STD,\n",
    "                                      REF_START_TOKEN)\n",
    "from internvl.train.dataset import (ConcatDataset, read_frames_decord,\n",
    "                                    WeightedConcatDataset, build_transform,\n",
    "                                    dynamic_preprocess, preprocess,\n",
    "                                    preprocess_internlm, preprocess_mpt, preprocess_internvl2_5,\n",
    "                                    preprocess_phi3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM0HKJyKEwlp"
   },
   "source": [
    "# MVBench benchmark\n",
    "\n",
    "[MVBench on HF Datasets](https://huggingface.co/datasets/OpenGVLab/MVBench)\n",
    "\n",
    "<!-- ![MVbench1.png](https://huggingface.co/datasets/OpenGVLab/MVBench/resolve/main/assert/generation.png) -->\n",
    "\n",
    "It consists of the 20 temporal task examples as follows.\n",
    "\n",
    "![MVbench-structure.png](https://huggingface.co/datasets/OpenGVLab/MVBench/resolve/main/assert/task_example.png)\n",
    "\n",
    "\n",
    "Here we have a nice viewer for each task:\n",
    "\n",
    "[Dataset viewer](https://huggingface.co/datasets/OpenGVLab/MVBench/viewer/action_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNZQ3imilvDz"
   },
   "source": [
    "We will start by downloading and processing the dataset. Even though MVBench is a small dataset, it still requires **around 1000B to store the videos**, so make sure you have enough free space.\n",
    "\n",
    "First, we will use this mapping to get the datasets because each one is a separate subset in its own folder. Then we need a few helper functions to download videos and process them to fit the model's format (8 frames each video)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gIwJovvg9YpI"
   },
   "outputs": [],
   "source": [
    "data_list = {\n",
    "    \"Action Sequence\": (\"action_sequence.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Prediction\": (\"action_prediction.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Action Antonym\": (\"action_antonym.json\", \"ssv2_video/\", \"video\", False),\n",
    "    \"Fine-grained Action\": (\"fine_grained_action.json\", \"Moments_in_Time_Raw/videos/\", \"video\", False),\n",
    "    \"Unexpected Action\": (\"unexpected_action.json\", \"FunQA_test/test/\", \"video\", False),\n",
    "    \"Object Existence\": (\"object_existence.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"Object Interaction\": (\"object_interaction.json\", \"star/Charades_v1_480/\", \"video\", True), # has start & end\n",
    "    \"Object Shuffle\": (\"object_shuffle.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Moving Direction\": (\"moving_direction.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"Action Localization\": (\"action_localization.json\", \"sta/sta_video/\", \"video\", True),  # has start & end\n",
    "    \"Scene Transition\": (\"scene_transition.json\", \"scene_qa/video/\", \"video\", False),\n",
    "    \"Action Count\": (\"action_count.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Moving Count\": (\"moving_count.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"Moving Attribute\": (\"moving_attribute.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "    \"State Change\": (\"state_change.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Fine-grained Pose\": (\"fine_grained_pose.json\", \"nturgbd/\", \"video\", False),\n",
    "    \"Character Order\": (\"character_order.json\", \"perception/videos/\", \"video\", False),\n",
    "    \"Egocentric Navigation\": (\"egocentric_navigation.json\", \"vlnqa/\", \"video\", False),\n",
    "    \"Episodic Reasoning\": (\"episodic_reasoning.json\", \"tvqa/frames_fps3_hq/\", \"frame\", True),  # has start & end, read frame\n",
    "    \"Counterfactual Inference\": (\"counterfactual_inference.json\", \"clevrer/video_validation/\", \"video\", False),\n",
    "}\n",
    "\n",
    "data_dir = \"dataset\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yVdfSXMGlvDz"
   },
   "outputs": [],
   "source": [
    "def read_video_pyav(video_path, start, end, n_frames=8):\n",
    "    \"\"\"\n",
    "    Reads a video for given start-end timestamps interval\n",
    "    and uniformly samples 8 frames of it\n",
    "    \"\"\"\n",
    "    container = av.open(video_path)\n",
    "    video = container.streams.get(0)[0]\n",
    "\n",
    "    av_timestamps = [\n",
    "        int(packet.pts * video.time_base) for packet in container.demux(video) if packet.pts is not None\n",
    "    ]\n",
    "\n",
    "    av_timestamps.sort()\n",
    "    start_id = bisect.bisect_left(av_timestamps, start)\n",
    "    end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "    # in case it is a very short video, lets take a longer duration and sample\n",
    "    if end_id  - start_id < 10:\n",
    "        end_id += 10\n",
    "        start_id -= 10\n",
    "\n",
    "    end_id = min(len(av_timestamps) - 1, end_id)\n",
    "    start_id = max(1, start_id)\n",
    "\n",
    "    # We sample n_frames frames for tuning following the original paper\n",
    "    # But we can increase the number of frames for longer videos and check out if it helps performance\n",
    "    # Change the below \"n_frames\" to any number of frames you want, and note that more frames -> more computational resources needed\n",
    "    indices = np.linspace(start_id, end_id, n_frames).astype(int)\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_id:\n",
    "            break\n",
    "        if i >= start_id and i in indices:\n",
    "            frames.append(frame)\n",
    "    assert len(frames) == n_frames, f\"Got {len(frames)} frames but should be {n_frames}. Check the indices: {indices};, start_id: {start_id}, end_id: {end_id}. Len of video is {len(av_timestamps)} frames.\"\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "X9-8O207lvD0"
   },
   "outputs": [],
   "source": [
    "def collate_read_video(example, path):\n",
    "    # Some datasets have a start-end interval, so we try to get it if exists.\n",
    "    # Otherwise just set a very large end timestamp\n",
    "    video_path = f'{path}/{example[\"video\"]}'\n",
    "    source = av.open(video_path)\n",
    "    video = source.streams.get(0)[0]\n",
    "    duration_seconds = source.duration / av.time_base\n",
    "    example['end'] = min(example['end'], duration_seconds)\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9jUicyg46_7K"
   },
   "outputs": [],
   "source": [
    "# Download the videos from datasets repo and unzip.\n",
    "# Make sure you have enough free space before downloading and unzipping\n",
    "\n",
    "# videos = snapshot_download(repo_id=\"OpenGVLab/MVBench\", allow_patterns=\"*\", repo_type=\"dataset\")\n",
    "# for zip_file in os.listdir(f\"{videos}/video\"):\n",
    "#     if zip_file.endswith(\".zip\"):\n",
    "#         shutil.unpack_archive(f\"{videos}/video/{zip_file}\", f\"{videos}/videos_unzipped/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrZnpKaflNQG",
    "outputId": "3d52c667-dbbd-49bc-a4e6-9fa11860b8c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Action Sequence\n",
      "Annotation file: action_sequence.json\n",
      "Videos are stored in: star/Charades_v1_480/\n",
      "Videos are represented as: video\n",
      "Videos are have start/end timestamps: True\n",
      "star.zip\n"
     ]
    }
   ],
   "source": [
    "# Or download only selected task with appropriate files\n",
    "# https://huggingface.co/docs/huggingface_hub/v0.24.7/en/package_reference/file_download#huggingface_hub.hf_hub_download\n",
    "\n",
    "TASK_NAME = \"Action Sequence\"\n",
    "annotation_fn, video_dir, video_type, has_clip = data_list.get(TASK_NAME)\n",
    "print(f\"Task: {TASK_NAME}\")\n",
    "print(f\"Annotation file: {annotation_fn}\")\n",
    "print(f\"Videos are stored in: {video_dir}\")\n",
    "print(f\"Videos are represented as: {video_type}\")\n",
    "print(f\"Videos are have start/end timestamps: {has_clip}\")\n",
    "\n",
    "annotation_fn_local = hf_hub_download(repo_id=\"OpenGVLab/MVBench\",\n",
    "                                    filename='json/' + annotation_fn,\n",
    "                                    repo_type=\"dataset\",\n",
    "                                    local_dir=data_dir)\n",
    "videos_zip = hf_hub_download(repo_id=\"OpenGVLab/MVBench\",\n",
    "                            filename='video/' + video_dir.split(\"/\")[0] + \".zip\",\n",
    "                            repo_type=\"dataset\",\n",
    "                            local_dir=data_dir)\n",
    "\n",
    "# Unzip\n",
    "for zip_file in os.listdir(f\"{data_dir}/video\"):\n",
    "    if zip_file.endswith(\".zip\"):\n",
    "        print(zip_file)\n",
    "        shutil.unpack_archive(f\"{data_dir}/video/{zip_file}\", f\"{data_dir}/video/videos_unzipped/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThTV0DbDA9RP"
   },
   "source": [
    "Make a following data structure:\n",
    "\n",
    "```\n",
    "dataset/\n",
    "    /json\n",
    "        task_name.json\n",
    "    /video\n",
    "        /task_name_prefix (optional)\n",
    "            /task_name\n",
    "                video_0.mp4\n",
    "                video_1.mp4\n",
    "                video_2.mp4\n",
    "                ...\n",
    "                video_100.mp4\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMNQND-ilvD0",
    "outputId": "ac1c78fc-6fcf-4e4c-b82c-76f53dcfc1eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['video', 'question', 'candidates', 'answer', 'start', 'end'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"json\", data_files=annotation_fn_local, split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSWorVK8B9ap",
    "outputId": "c0c865e8-9399-4d27-b562-8b9efbf08990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video `EDXBD.mp4` does not exists!\n",
      "Video `K47J5.mp4` does not exists!\n",
      "Video `9MNZ5.mp4` does not exists!\n",
      "Video `QXT9W.mp4` does not exists!\n",
      "Video `ABHC6.mp4` does not exists!\n",
      "Video `ALXUC.mp4` does not exists!\n",
      "Video `BAUQE.mp4` does not exists!\n",
      "Video `PHH6B.mp4` does not exists!\n",
      "Video `MNC10.mp4` does not exists!\n",
      "Video `W7CR5.mp4` does not exists!\n",
      "Video `Q8UJ8.mp4` does not exists!\n",
      "Video `X9WTR.mp4` does not exists!\n"
     ]
    }
   ],
   "source": [
    "# Some tasks in MVBench are missing video files - keep it in mind!\n",
    "has_missing = False\n",
    "for sample in ds:\n",
    "    if not os.path.exists(f\"{data_dir}/video/videos_unzipped/{video_dir}/{sample['video']}\"):\n",
    "        print(f\"Video `{sample['video']}` does not exists!\")\n",
    "        has_missing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1stzqVuCtRF",
    "outputId": "c11388de-691f-4816-98e7-07b8a2ecdfa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length = 200\n",
      "Dataset length = 188\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset length = {len(ds)}\")\n",
    "if has_missing:\n",
    "    ds = ds.filter(lambda x: os.path.exists(f\"{data_dir}/video/videos_unzipped/{video_dir}/{x['video']}\"))\n",
    "\n",
    "print(f\"Dataset length = {len(ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c44886c523194321a45ac116a99d7e09",
      "4c53a11f8b4d48bab7bb07005e09dab2",
      "1e40a2be14574c9cab05c6a2066506c9",
      "6997d1b4ec2e40268bbfe56ff5ad1ddc",
      "0c80b263628d4c37b129278c8cb80fc2",
      "606c508016d34f1bb86d21b2c4e3cfc3",
      "8c9c50dc1265410e82248451e1fdd156",
      "e9e922c6bf14487599e94963b8afd747",
      "e7dd46cf21684684b12361c82c5e01ca",
      "7c13ed62e28e44468633fdfa010a07cb",
      "24b4a2b91c7e484bacb5a543a60dff44"
     ]
    },
    "id": "eySn7X6zlbn3",
    "outputId": "0c291bd5-d7c5-4337-ef61-99d9399d3cad"
   },
   "outputs": [],
   "source": [
    "# Load videos and split them into frames\n",
    "ds = ds.map(collate_read_video,\n",
    "            batched=False,\n",
    "            fn_kwargs={\"path\": f\"{data_dir}/video/videos_unzipped/{video_dir}\"})\n",
    "\n",
    "# Make conversation\n",
    "\n",
    "def make_conversation(sample):\n",
    "    id2choice = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "    question, candidates = sample[\"question\"], sample[\"candidates\"]\n",
    "    answer_i = candidates.index(sample[\"answer\"])\n",
    "    answer_choice = id2choice[answer_i]\n",
    "\n",
    "    mult_choice = \"\\n\"\n",
    "    for i, choice in enumerate(candidates):\n",
    "        mult_choice += f\"{id2choice[i]}. {choice};\\n\"\n",
    "\n",
    "    conversations = [\n",
    "         {'from': 'human', 'value': question + mult_choice + '<video>'},\n",
    "         {'from': 'gpt', 'value': answer_choice + \" \" + sample[\"answer\"]}\n",
    "    ]\n",
    "    sample['conversations'] = conversations\n",
    "    return sample\n",
    "\n",
    "ds = ds.map(make_conversation,\n",
    "            batched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6JsAJjJx1C4"
   },
   "source": [
    "\n",
    "\n",
    "```json\n",
    "{'id': 578004, 'video': '013901_013950/1025449886.mp4',\n",
    "'conversations':\n",
    "[{'from': 'human', 'value': 'Render a clear and concise summary of the video below.\\n<video>'},\n",
    "{'from': 'gpt', 'value': 'Aerial; drone flight around dangerous eroded steep stony slopes of cabo da roca; granite boulders, sea cliffs along the high coast; long seashore with lighthouse, overlooking the promontory, portugal'}]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GRTrOYp6lvD0",
    "outputId": "e0f038f5-bef8-4695-95a3-4efa5e636e1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternVLProcessor:\n",
       "- image_processor: GotOcr2ImageProcessor {\n",
       "  \"crop_size\": null,\n",
       "  \"crop_to_patches\": false,\n",
       "  \"data_format\": \"channels_first\",\n",
       "  \"default_to_square\": true,\n",
       "  \"device\": null,\n",
       "  \"do_center_crop\": null,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"GotOcr2ImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"input_data_format\": null,\n",
       "  \"max_patches\": 12,\n",
       "  \"min_patches\": 1,\n",
       "  \"processor_class\": \"InternVLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"return_tensors\": null,\n",
       "  \"size\": {\n",
       "    \"height\": 448,\n",
       "    \"width\": 448\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: Qwen2Tokenizer(name_or_path='OpenGVLab/InternVL3_5-1B-HF', vocab_size=151643, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>'], 'context_image_token': '<IMG_CONTEXT>', 'end_image_token': '</img>', 'start_image_token': '<img>', 'video_token': '<video>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151669: AddedToken(\"<img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151670: AddedToken(\"</img>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151671: AddedToken(\"<IMG_CONTEXT>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151672: AddedToken(\"<quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151673: AddedToken(\"</quad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151674: AddedToken(\"<ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151675: AddedToken(\"</ref>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151676: AddedToken(\"<box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151677: AddedToken(\"</box>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151678: AddedToken(\"<video>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")\n",
       "- video_processor: InternVLVideoProcessor {\n",
       "  \"crop_size\": null,\n",
       "  \"data_format\": \"channels_first\",\n",
       "  \"default_to_square\": true,\n",
       "  \"device\": null,\n",
       "  \"do_center_crop\": null,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_pad\": null,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"do_sample_frames\": false,\n",
       "  \"fps\": null,\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"initial_shift\": true,\n",
       "  \"input_data_format\": null,\n",
       "  \"num_frames\": null,\n",
       "  \"pad_size\": null,\n",
       "  \"processor_class\": \"InternVLProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"return_metadata\": false,\n",
       "  \"size\": {\n",
       "    \"height\": 384,\n",
       "    \"width\": 384\n",
       "  },\n",
       "  \"size_divisor\": null,\n",
       "  \"video_metadata\": null,\n",
       "  \"video_processor_type\": \"InternVLVideoProcessor\"\n",
       "}\n",
       "\n",
       "\n",
       "{\n",
       "  \"image_seq_length\": 256,\n",
       "  \"processor_class\": \"InternVLProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model's processor\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID + \"-HF\", trust_remote_code=True, use_fast=False, model_max_length=MAX_LENGTH)\n",
    "processor.padding_side = \"right\"\n",
    "\n",
    "processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtxYp7h2lvD0"
   },
   "source": [
    "## Custom Dataset Class\n",
    "\n",
    "In the next step, you'll need **to define a custom dataset** class and the necessary functions to prepare our data for fine-tuning model. The VideoQADataset class extends the [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) class to facilitate loading and processing \"MMBench\". This class will handle the conversion of dataset samples into the format required for training and evaluation by preparing a prompt and making array from videos.\n",
    "\n",
    "Next, you need **to define collate functions** to handle the batching of data during training and evaluation. These functions ensure that the input data is properly formatted and padded.\n",
    "\n",
    "Here use the processor to turn the (video, target token sequence) into the format that the model expects (which is pixel_values, input_ids etc.). Use a dynamic padding of the batches: each batch contains ground truth sequences of varying lengths.\n",
    "\n",
    "Also you can limit the length of the text tokens (input_ids) to a max length due to memory constraints, feel free to expand if your target token sequences are longer (I'd recommend plotting the average token length of your dataset to determine the optimal value).\n",
    "\n",
    "The formatting of the input_ids is super important: you need to respect a so-called [chat template](https://huggingface.co/docs/transformers/main/en/chat_templating).\n",
    "\n",
    "Labels are created for the model by simply copying the inputs to the LLM (input_ids), but with padding tokens replaced by the ignore index of the loss function. This ensures that the model doesn't need to learn to predict padding tokens (used to batch examples together).\n",
    "\n",
    "Why are the labels a copy of the model inputs, you may ask? The model will internally shift the labels one position to the right so that the model will learn to predict the next token. This can be seen here.\n",
    "\n",
    "The collate function for evaluation is different, since there you only need to feed the prompt to the model, as we'll use the `generate()` method to autoregressively generate a completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "\n",
    "class VideoQADataset(Dataset):\n",
    "    \"\"\"Dataset for Video QA fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        template_name,\n",
    "        raw_data,\n",
    "        video_data_dir,\n",
    "        tokenizer,\n",
    "        ds_name,\n",
    "        num_image_token,\n",
    "        image_size=224,\n",
    "        is_train=True,\n",
    "        pad2square=False,\n",
    "        dynamic_image_size=False,\n",
    "        use_thumbnail=False,\n",
    "        min_dynamic_patch=1,\n",
    "        max_dynamic_patch=6,\n",
    "        min_num_frame=8,  # for video data\n",
    "        max_num_frame=8,  # for video data\n",
    "        sampling_method='rand',  # for video data\n",
    "        repeat_time=1,\n",
    "        normalize_type='imagenet',\n",
    "        random_seed=0,\n",
    "    ):\n",
    "        super(VideoQADataset, self).__init__()\n",
    "        self.ds_name = ds_name\n",
    "        self.tokenizer = tokenizer\n",
    "        self.template_name = template_name\n",
    "        self.num_image_token = num_image_token\n",
    "        print(f'[Dataset] num_image_token: {num_image_token}')\n",
    "        print(f'[Dataset] dynamic_image_size: {dynamic_image_size}')\n",
    "        print(f'[Dataset] use_thumbnail: {use_thumbnail}')\n",
    "        print(f'[Dataset] min_dynamic_patch: {min_dynamic_patch}, max_dynamic_patch: {max_dynamic_patch}')\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        self.pad2square = pad2square\n",
    "        self.max_num_frame = max_num_frame\n",
    "        self.min_num_frame = min_num_frame\n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "        print('Formatting inputs...Skip in lazy mode')\n",
    "\n",
    "        self.raw_data = raw_data.shuffle(seed=random_seed) if hasattr(raw_data, 'shuffle') else raw_data\n",
    "\n",
    "        gc.collect()\n",
    "        self.root = video_data_dir\n",
    "        self.cached_data_dict = {}\n",
    "        self.dynamic_image_size = dynamic_image_size\n",
    "        self.use_thumbnail = use_thumbnail\n",
    "        self.min_dynamic_patch = min_dynamic_patch\n",
    "        self.max_dynamic_patch = max_dynamic_patch\n",
    "        self.normalize_type = normalize_type\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def get_preprocess_function(self):\n",
    "        # Select the appropriate preprocessing function based on the template name\n",
    "        return preprocess_internvl2_5\n",
    "        \n",
    "    def get_transform(self):\n",
    "        # Build transformation function\n",
    "        transform = build_transform(is_train=self.is_train, input_size=self.image_size,\n",
    "                                    pad2square=self.pad2square, normalize_type=self.normalize_type)\n",
    "        return transform\n",
    "\n",
    "    def video_get_item(self, data_item):\n",
    "        # Build transformation function\n",
    "        transform = self.get_transform()\n",
    "\n",
    "        # Ensure the first conversation contains a video placeholder\n",
    "        if '<video>' not in data_item['conversations'][0]['value']:\n",
    "            data_item['conversations'][0]['value'] = '<video>\\n' + data_item['conversations'][0]['value']\n",
    "\n",
    "        # Get the video file path\n",
    "        video_file = data_item['video']\n",
    "        video_path = os.path.join(self.root, video_file)\n",
    "        \n",
    "        # Use read_video_pyav instead of read_frames_decord\n",
    "        # image_list = read_video_pyav(video_path,\n",
    "        #                              start=data_item.get('start', 0),\n",
    "        #                              end=data_item.get('end', 1000000),\n",
    "        #                              n_frames=self.max_num_frame)\n",
    "        image_list = read_frames_decord(video_path,\n",
    "                                        num_frames=self.max_num_frame,\n",
    "                                        min_num_frames = self.min_num_frame,\n",
    "                                        sample=self.sampling_method,\n",
    "                                        client=None,\n",
    "                                        clip=(data_item.get('start', None),\n",
    "                                              data_item.get('end', None)))\n",
    "        \n",
    "        # image_list = [Image.fromarray(frame) for frame in image_list]\n",
    "        \n",
    "        # Generate special tokens for each video frame\n",
    "        special_tokens = '\\n'.join(['Frame{}: <image>'.format(i + 1) for i in range(len(image_list))])\n",
    "        data_item['conversations'][0]['value'] = data_item['conversations'][0]['value'].replace(\n",
    "            '<video>', special_tokens)\n",
    "\n",
    "        # Transform each frame image and stack them into a tensor\n",
    "        pixel_values = [transform(image) for image in image_list]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches = pixel_values.size(0)\n",
    "\n",
    "        # Select the appropriate preprocessing function based on the template name\n",
    "        preprocess_function = self.get_preprocess_function()\n",
    "\n",
    "        # Preprocess the conversations and generate the return dictionary\n",
    "        num_image_tokens = [self.num_image_token] * num_patches\n",
    "\n",
    "        if self.is_train:\n",
    "            # For training, use full conversation (human + assistant)\n",
    "            sources = [deepcopy(data_item['conversations'])]\n",
    "        else:\n",
    "            # For evaluation, use only human part (prompt)\n",
    "            sources = [deepcopy(data_item['conversations'])]\n",
    "        \n",
    "        ret = preprocess_function(template_name=self.template_name, \n",
    "                                  sources=sources,\n",
    "                                  tokenizer=self.tokenizer,\n",
    "                                  num_image_token_list=num_image_tokens,\n",
    "                                  text_only=False,\n",
    "                                  group_by_length=False,\n",
    "                                  ds_name=self.ds_name, \n",
    "                                  num_image=num_patches\n",
    "        )\n",
    "\n",
    "        conv = data_item['conversations']\n",
    "        \n",
    "        # Extract the correct answer choice from the GPT response\n",
    "        answer_choice = ''\n",
    "        if len(conv) > 1 and 'value' in conv[1]:\n",
    "            gpt_response = conv[1]['value']\n",
    "            # Extract just the choice letter (A, B, C, D) - usually first character\n",
    "            answer_choice = gpt_response.strip()[0]  # Get first character (A, B, C, or D)\n",
    "            \n",
    "\n",
    "        # Create the final return dictionary\n",
    "        ret = dict(\n",
    "            input_ids=ret['input_ids'][0],\n",
    "            labels=ret['labels'][0],\n",
    "            attention_mask=ret['attention_mask'][0],\n",
    "            pixel_values=pixel_values,\n",
    "            image_flags=torch.tensor([1] * num_patches, dtype=torch.long),\n",
    "            answer_choice=answer_choice\n",
    "        )\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def pure_text_get_item(self, data_item):\n",
    "        \"\"\"Handle text-only examples\"\"\"\n",
    "        # Simple text processing\n",
    "        sources = [deepcopy(data_item['conversations'])]\n",
    "        \n",
    "        ret = preprocess_internvl2_5(template_name=self.template_name, \n",
    "                                    sources=sources,\n",
    "                                    tokenizer=self.tokenizer,\n",
    "                                    num_image_token_list=[0],\n",
    "                                    text_only=True,\n",
    "                                    group_by_length=False,\n",
    "                                    ds_name=self.ds_name, \n",
    "                                    num_image=0)\n",
    "        \n",
    "        conv = data_item['conversations']\n",
    "        answer_choice = ''\n",
    "        if len(conv) > 1 and 'value' in conv[1]:\n",
    "            answer_choice = conv[1]['value'].strip()[0]\n",
    "\n",
    "        return dict(\n",
    "            input_ids=ret['input_ids'][0],\n",
    "            labels=ret['labels'][0],\n",
    "            attention_mask=ret['attention_mask'][0],\n",
    "            pixel_values=torch.zeros(1, 3, self.image_size, self.image_size),\n",
    "            image_flags=torch.tensor([0], dtype=torch.long),\n",
    "            answer_choice=answer_choice\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        i = i % len(self.raw_data)\n",
    "        while True:\n",
    "            try:\n",
    "                data_item = self.raw_data[i]\n",
    "                if 'image' in data_item and len(data_item['image']) != 0:\n",
    "                    # For simplicity, handle images as text in this version\n",
    "                    ret = self.pure_text_get_item(data_item)\n",
    "                elif 'video' in data_item and data_item['video'] is not None and data_item['video'] != '':\n",
    "                    ret = self.video_get_item(data_item)\n",
    "                else:\n",
    "                    ret = self.pure_text_get_item(data_item)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e, self.ds_name, flush=True)\n",
    "                if not isinstance(e, UnidentifiedImageError):\n",
    "                    traceback.print_exc()\n",
    "                data_item = self.raw_data[i]\n",
    "                if 'image' in data_item:\n",
    "                    if type(data_item['image']) == list:\n",
    "                        images = [self.root + item for item in data_item['image']]\n",
    "                        print(f'Failed to load image: {images}, the dataset is: {self.ds_name}')\n",
    "                    else:\n",
    "                        if data_item['image'].startswith('s3://'):\n",
    "                            data_path = self.root + data_item['image']\n",
    "                        else:\n",
    "                            data_path = os.path.join(self.root, data_item['image'])\n",
    "                        print(f'Failed to load image: {data_path}, the dataset is: {self.ds_name}')\n",
    "                elif 'video' in data_item:\n",
    "                    data_path = os.path.join(self.root, data_item['video'])\n",
    "                    print(f'Failed to load video: {data_path}, the dataset is: {self.ds_name}')\n",
    "                i = random.randint(0, len(self.raw_data) - 1)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function for training that batches examples together.\n",
    "    Handles padding for text and stacks video frames.\n",
    "    \"\"\"\n",
    "    # Extract all components from examples\n",
    "    input_ids = [example['input_ids'] for example in examples]\n",
    "    attention_mask = [example['attention_mask'] for example in examples]\n",
    "    pixel_values = [example['pixel_values'] for example in examples]\n",
    "    image_flags = [example['image_flags'] for example in examples]\n",
    "    labels = [example['labels'] for example in examples]\n",
    "    answer_choices = [example['answer_choice'] for example in examples]\n",
    "\n",
    "    # Pad input_ids, attention_mask, and labels to the same length\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    padded_labels = []\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        # Pad input_ids\n",
    "        pad_length = max_length - len(input_ids[i])\n",
    "        padded_input_ids.append(\n",
    "            torch.cat([\n",
    "                input_ids[i],\n",
    "                torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Pad attention_mask\n",
    "        padded_attention_mask.append(\n",
    "            torch.cat([\n",
    "                attention_mask[i],\n",
    "                torch.zeros(pad_length, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Pad labels (using -100 for ignore index)\n",
    "        padded_labels.append(\n",
    "            torch.cat([\n",
    "                labels[i],\n",
    "                torch.full((pad_length,), -100, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    # Stack all tensors\n",
    "    input_ids_batch = torch.stack(padded_input_ids)\n",
    "    attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    labels_batch = torch.stack(padded_labels)\n",
    "    \n",
    "    # Handle pixel_values - FLATTEN the video frames\n",
    "    # Instead of padding frames, we'll process each frame as a separate image\n",
    "    flattened_pixel_values = []\n",
    "    flattened_image_flags = []\n",
    "    \n",
    "    for i, (pv, flags) in enumerate(zip(pixel_values, image_flags)):\n",
    "        # pv shape: (num_frames, channels, height, width)\n",
    "        # We need to flatten this to (num_frames * batch_size, channels, height, width)\n",
    "        num_frames = pv.size(0)\n",
    "        for frame_idx in range(num_frames):\n",
    "            flattened_pixel_values.append(pv[frame_idx])\n",
    "            # Note: image_flags is not defined in this scope - you need to extract it from examples\n",
    "            flattened_image_flags.append(flags[frame_idx])\n",
    "    \n",
    "    # Stack flattened pixel values\n",
    "    if flattened_pixel_values:\n",
    "        pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "        image_flags_batch = torch.stack(flattened_image_flags)\n",
    "    else:\n",
    "        pixel_values_batch = torch.tensor([])\n",
    "        image_flags_batch = torch.tensor([])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_batch,\n",
    "        'attention_mask': attention_mask_batch,\n",
    "        'pixel_values': pixel_values_batch,\n",
    "        'labels': labels_batch,\n",
    "        'image_flags': image_flags_batch,\n",
    "        'answer_choices': answer_choices\n",
    "    }\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    \"\"\"\n",
    "    Collate function for evaluation that batches examples together.\n",
    "    Similar to training but doesn't need labels for generation.\n",
    "    \"\"\"\n",
    "    # Extract all components from examples\n",
    "    input_ids = [example['input_ids'] for example in examples]\n",
    "    attention_mask = [example['attention_mask'] for example in examples]\n",
    "    pixel_values = [example['pixel_values'] for example in examples]\n",
    "    answer_choices = [example['answer_choice'] for example in examples]\n",
    "\n",
    "    # Pad input_ids and attention_mask to the same length\n",
    "    max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "    padded_input_ids = []\n",
    "    padded_attention_mask = []\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        # Pad input_ids\n",
    "        pad_length = max_length - len(input_ids[i])\n",
    "        padded_input_ids.append(\n",
    "            torch.cat([\n",
    "                input_ids[i],\n",
    "                torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Pad attention_mask\n",
    "        padded_attention_mask.append(\n",
    "            torch.cat([\n",
    "                attention_mask[i],\n",
    "                torch.zeros(pad_length, dtype=torch.long)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    # Stack all tensors\n",
    "    input_ids_batch = torch.stack(padded_input_ids)\n",
    "    attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    \n",
    "    # Handle pixel_values - FLATTEN the video frames\n",
    "    flattened_pixel_values = []\n",
    "    \n",
    "    for i, pv in enumerate(pixel_values):\n",
    "        # pv shape: (num_frames, channels, height, width)\n",
    "        num_frames = pv.size(0)\n",
    "        for frame_idx in range(num_frames):\n",
    "            flattened_pixel_values.append(pv[frame_idx])\n",
    "    \n",
    "    # Stack flattened pixel values\n",
    "    if flattened_pixel_values:\n",
    "        pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "    else:\n",
    "        pixel_values_batch = torch.tensor([])\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_batch,\n",
    "        'attention_mask': attention_mask_batch,\n",
    "        'pixel_values': pixel_values_batch,\n",
    "        'answer_choices': answer_choices\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_collate_fn(examples):\n",
    "#     \"\"\"\n",
    "#     Collate function for training that batches examples together.\n",
    "#     Handles padding for text and stacks video frames.\n",
    "#     \"\"\"\n",
    "#     # Extract all components from examples\n",
    "#     input_ids = [example['input_ids'] for example in examples]\n",
    "#     attention_mask = [example['attention_mask'] for example in examples]\n",
    "#     pixel_values = [example['pixel_values'] for example in examples]\n",
    "#     image_flags = [example['image_flags'] for example in examples]\n",
    "#     labels = [example['labels'] for example in examples]\n",
    "#     answer_choices = [example['answer_choice'] for example in examples]\n",
    "\n",
    "#     # Pad input_ids, attention_mask, and labels to the same length\n",
    "#     max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "#     padded_input_ids = []\n",
    "#     padded_attention_mask = []\n",
    "#     padded_labels = []\n",
    "    \n",
    "#     for i in range(len(input_ids)):\n",
    "#         # Pad input_ids\n",
    "#         pad_length = max_length - len(input_ids[i])\n",
    "#         padded_input_ids.append(\n",
    "#             torch.cat([\n",
    "#                 input_ids[i],\n",
    "#                 torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "#             ])\n",
    "#         )\n",
    "        \n",
    "#         # Pad attention_mask\n",
    "#         padded_attention_mask.append(\n",
    "#             torch.cat([\n",
    "#                 attention_mask[i],\n",
    "#                 torch.zeros(pad_length, dtype=torch.long)\n",
    "#             ])\n",
    "#         )\n",
    "        \n",
    "#         # Pad labels (using -100 for ignore index)\n",
    "#         padded_labels.append(\n",
    "#             torch.cat([\n",
    "#                 labels[i],\n",
    "#                 torch.full((pad_length,), -100, dtype=torch.long)\n",
    "#             ])\n",
    "#         )\n",
    "\n",
    "#     # Stack all tensors\n",
    "#     input_ids_batch = torch.stack(padded_input_ids)\n",
    "#     attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "#     labels_batch = torch.stack(padded_labels)\n",
    "    \n",
    "#     # Handle pixel_values - FLATTEN the video frames\n",
    "#     # Instead of padding frames, we'll process each frame as a separate image\n",
    "#     flattened_pixel_values = []\n",
    "#     flattened_image_flags = []\n",
    "    \n",
    "#     for i, (pv, flags) in enumerate(zip(pixel_values, image_flags)):\n",
    "#         # pv shape: (num_frames, channels, height, width)\n",
    "#         # We need to flatten this to (num_frames * batch_size, channels, height, width)\n",
    "#         num_frames = pv.size(0)\n",
    "#         for frame_idx in range(num_frames):\n",
    "#             flattened_pixel_values.append(pv[frame_idx])\n",
    "#             # Note: image_flags is not defined in this scope - you need to extract it from examples\n",
    "#             flattened_image_flags.append(flags[frame_idx])\n",
    "    \n",
    "#     # Stack flattened pixel values\n",
    "#     if flattened_pixel_values:\n",
    "#         pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "#         image_flags_batch = torch.stack(flattened_image_flags)\n",
    "#     else:\n",
    "#         pixel_values_batch = torch.tensor([])\n",
    "#         image_flags_batch = torch.tensor([])\n",
    "    \n",
    "#     return {\n",
    "#         'input_ids': input_ids_batch,\n",
    "#         'attention_mask': attention_mask_batch,\n",
    "#         'pixel_values': pixel_values_batch,\n",
    "#         'labels': labels_batch,\n",
    "#         'image_flags': image_flags_batch,\n",
    "#         'answer_choices': answer_choices\n",
    "#     }\n",
    "\n",
    "\n",
    "# def eval_collate_fn(examples):\n",
    "#     \"\"\"\n",
    "#     Collate function for evaluation that batches examples together.\n",
    "#     Similar to training but doesn't need labels for generation.\n",
    "#     \"\"\"\n",
    "#     # Extract all components from examples\n",
    "#     input_ids = [example['input_ids'] for example in examples]\n",
    "#     attention_mask = [example['attention_mask'] for example in examples]\n",
    "#     pixel_values = [example['pixel_values'] for example in examples]\n",
    "#     answer_choices = [example['answer_choice'] for example in examples]\n",
    "\n",
    "#     # Pad input_ids and attention_mask to the same length\n",
    "#     max_length = max(len(ids) for ids in input_ids)\n",
    "    \n",
    "#     padded_input_ids = []\n",
    "#     padded_attention_mask = []\n",
    "    \n",
    "    # for i in range(len(input_ids)):\n",
    "    #     # Pad input_ids\n",
    "    #     pad_length = max_length - len(input_ids[i])\n",
    "    #     padded_input_ids.append(\n",
    "    #         torch.cat([\n",
    "    #             input_ids[i],\n",
    "    #             torch.full((pad_length,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "    #         ])\n",
    "    #     )\n",
    "        \n",
    "    #     # Pad attention_mask\n",
    "    #     padded_attention_mask.append(\n",
    "    #         torch.cat([\n",
    "    #             attention_mask[i],\n",
    "    #             torch.zeros(pad_length, dtype=torch.long)\n",
    "    #         ])\n",
    "    #     )\n",
    "\n",
    "    # # Stack all tensors\n",
    "    # input_ids_batch = torch.stack(padded_input_ids)\n",
    "    # attention_mask_batch = torch.stack(padded_attention_mask)\n",
    "    \n",
    "    # # Handle pixel_values - FLATTEN the video frames\n",
    "    # flattened_pixel_values = []\n",
    "    \n",
    "    # for i, pv in enumerate(pixel_values):\n",
    "    #     # pv shape: (num_frames, channels, height, width)\n",
    "    #     num_frames = pv.size(0)\n",
    "    #     for frame_idx in range(num_frames):\n",
    "    #         flattened_pixel_values.append(pv[frame_idx])\n",
    "    \n",
    "    # # Stack flattened pixel values\n",
    "    # if flattened_pixel_values:\n",
    "    #     pixel_values_batch = torch.stack(flattened_pixel_values)\n",
    "    # else:\n",
    "    #     pixel_values_batch = torch.tensor([])\n",
    "    \n",
    "    # return {\n",
    "    #     'input_ids': input_ids_batch,\n",
    "    #     'attention_mask': attention_mask_batch,\n",
    "    #     'pixel_values': pixel_values_batch,\n",
    "    #     'answer_choices': answer_choices\n",
    "    # }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sH2oWArlvD1"
   },
   "source": [
    "## Shuffling and Splitting the Dataset\n",
    "You need to shuffle dataset, and then split it into training and test sets. This ensures that our model is trained on a diverse and representative sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "DenEK2IqlvD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 150\n",
      "Test dataset size: 38\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and split dataset into 80% train and 20% test\n",
    "ds = ds.shuffle(seed=42)\n",
    "split_idx = int(len(ds) * 0.8)\n",
    "\n",
    "dataset = {\n",
    "    'train': ds.select(range(split_idx)),\n",
    "    'test': ds.select(range(split_idx, len(ds)))\n",
    "}\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "X_DDgpOulvD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video shape: (8, 270, 480, 3)\n",
      "Question: What happened after the person took the phone/camera?\n",
      "Answer: Closed the window.\n",
      "Candidates: ['Took the book.', 'Put down the pillow.', 'Closed the window.', 'Took the food.']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "example = dataset['train'][0]\n",
    "# Read the video for visualization\n",
    "video_path = f\"{data_dir}/video/videos_unzipped/{video_dir}/{example['video']}\"\n",
    "clip = read_video_pyav(video_path, example.get('start', 1), example.get('end', 1e+10), n_frames=8)\n",
    "\n",
    "# np array with shape (frames, height, width, channels)\n",
    "video = np.array(clip)\n",
    "\n",
    "print(f\"Video shape: {video.shape}\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Answer: {example['answer']}\")\n",
    "print(f\"Candidates: {example['candidates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "aLtCWizwlvD1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQABWWhtZGF0AAACrgYF//+q\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2NCByMzEwOCAzMWUxOWY5IC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMyAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTE1\n",
       "IGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50\n",
       "ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi\n",
       "X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w\n",
       "PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0zIHNjZW5lY3V0PTQwIGludHJhX3Jl\n",
       "ZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAu\n",
       "NjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAABU\n",
       "MGWIhAAR//73iB8yy2n5OtdyEeetLq3x9SoaO1hHRRsrYAAAAwAAAwAGuS1k1DeP2njU0ktwAAAJ\n",
       "QAE/BsBDQ4cqXBCyXIABhcN/jf/qY+Ilw+Rbi8M1tj75t5j6a50QK0GMYrbHo8MX+8miea8CrYRm\n",
       "eCZmiznyLvX7QGDMys4GcDEaZURluGmnW3ekfftjXju9jVLvNMY4YZVBIDvK6wXDMJDP1YE+p9Aw\n",
       "PIcXi3jwZc3MjJTwKadSphpHz1r6pxd4ZjK2PPIVSOEBR7yTgajkaBIg7pOt33d4cNiAcUVNIBJ9\n",
       "OukLCL95aQMnLP4EoZfJnA01PPG54+xoyeE9zZMhiR5/ygPrhTNK6iuEZeGV7g4NinJHGe3p4C4v\n",
       "IMe8KfmHGeuNW2xOVyKkAjMCQKscQhFav/Xi9iGcoUiHo3XvM8cTjdslEhPGH6cplBmEykjMTxMa\n",
       "L2JOhtTxDNB5botx480yp/gazsCN+Fxnfvew1PP+FjJruIHbSnKflZUXARm4MxpeJrVuAF2oiE1w\n",
       "JaKVdXUPiQy91QY3knnYXU+ofrgA78wWr/BipukxGbrdQhQeIcMfyssDsJ9wGWEViUa5ohqKszmy\n",
       "aYrVH98imPfB2NCsX14LKKPeL+xd6sXeZfp7SUdNyE3Y/8cLnZRolMQAF3iqOLdkGl0lwi2v3lkR\n",
       "cS457zaiF1owhzk6yVG/RNky+fEBiNaFHkq/TgnUu2Noav3KOdhQe7WZPuoxF0FIg+kRPEdkQKOJ\n",
       "2eG5VWc/vW1GG+8cCtc6erWRTqQWg6HnobIc7lOzOsQ6v9LCcXKSVuNIBTqA5fA4JtaJRPJAHJWF\n",
       "IYtfdPs6Ab2ImvzTc5+V487K6LYmgs33w6S1OX1Y0A0ipxQw2h92JZTtz2A0RTaVonIRm1vyEsnU\n",
       "rDCaHg/91G/CBo/rO7K6fY1ZFc4BbpW7ZqfZbVl3zFQ8Nr63JGIdvdVkD5sTSfwcwyfyAMlKtoql\n",
       "Yy3WFcxAfKP9LheeTnDLli/LjErkR5rFPgDvc2LsAeFzPl5u/ucAtygMnkYkDIApo7YvvjFQl5fH\n",
       "EIDTykpkQ4WE9UR39eec4wz/0n8X+z6ciYdsoUegTDsJYR6STifDjVi+MMtCuGegNEqLkTNpThfz\n",
       "aS3dFVjyauqEtuib9vMW4lNyVlBLaEBkhRMIm1GSJje8X7XihFCabVfsIbLsT8uAunGekAjaxk5l\n",
       "oK/W5hMOLApe+iNXEEgbVHdcuu3fXUwpYzuvHm3LgYWs7hYckshZ1XFPnrx01s3dyB+JYlPwc/lm\n",
       "hv+cCLV1CadFaIhnOLHZd490ZFTfhSyCy6D95Yb+Kvfh0rNia6SlOVShfECnAOsXgJ8PmDmr4oxW\n",
       "BLY6mIiku0sFP63wgbZMs3CA5hbJu7g6PfY7Cn3ah9yfWAO2d40gDlLv4m1mN4QVFRBQB8KWOJV2\n",
       "7g6NBrzEBFDUa8F77Jq0XsSLTnHINkDKszXj77U3GCWCLViHiNxMUO6GCtt7ymuiZM+5ZKNXtZWR\n",
       "nmVaNlJ0QPOWkv7/2TOEUsOnKcdwuuiRGaWKutJ5qUujjT7s3/0/YMWCvjq2ddEP7uVHgjsu6+2T\n",
       "zXgBAqHDI+WNyNB9d9CA/iHrS5Qwxni2Ix42V3Q6Qybx+sG2Lb1DNS4FKVAyWC7HucP19Ub3XX7g\n",
       "5cE4OB5N70o1yKRczyxCHnAJnMkMt/UcdTO9yED5cE2kMuJ7XBTad4ptQcUsZkx/1/uFnnms6VO0\n",
       "OVYXtEtjLDmoYkI9E9C/KYHmFlVxV3+6/CLPizspXbklt97vUtNaM3gQlyXOMiFwYQrULvjgHWAd\n",
       "4USHKuQ/qUoOGS/SNL9VpSBY3t7JV0Nc6VSWAfZUzvJzgPROv9htuOPf2vmlOiGupgwfzt0CVmbO\n",
       "/V+uQ/CfkByN3BYswS3N336tacBdMbr/l2zLnkG1bx80QSmx6pLx+HkZWfQTMkhVf/Mau8pH3OGQ\n",
       "5XuNkZSUZL66sa4k/G6YJY7Ek4UaV4VInhkaY8fVr2WWCgmbjs1BW4/FDGnTFHK8T+QxLqwtrYkv\n",
       "5fKqPmMYoFiETUYC9uru0qzY+Z/OYXRAmUvfc+CiWgQck2CRJyecfvehCGupn9ALZ9fkCjxNA3h4\n",
       "xXEuzynITIWpAZWZUhf/REisZbZ0ZB2LuuIb4AtjNzE27rHFQYewaoH6DoWScbFeGg+ETalagr0t\n",
       "a/G39s0boefp6jTxlIcUjI+E+6IRDdCmx7K0zeWGQAmsB+vZm9aRChZ5iAaPUQH2VI7fkTiuEsqy\n",
       "bpUg8NkH+RHEbPBoyo5E9bYyJ38G7OYXBpM8ZLnjM2vrCbAP6+yq3Rcvbt18spqspYUF2ouo+YX0\n",
       "klzDSC8pBgFsABL9E8D7z1RMyfU36TY1kZjQBWYOcL6vWXHqvjcUL9UP7nZQYN+5gB1OZumCC808\n",
       "WanB9Onzp7MOfWIE0wJQzeQ4XUsp7PLXEEjF938z0WHMFEhs6o4xhb00ZRU7yYIyUmwhXCFV/d7I\n",
       "ofLxi62krQ9WCpM1OCSlb2olv8p965fR/Mp7ig9fIbUk6uGlXfC4gp1GmhuQKMgMlsjhLTfzkvK1\n",
       "9oMmHWjhOi4MMoqbZxA2SUbN9IcSc9vp5iY1hQtfNsJWaF4c9CzD8lJJ60ahlvHX9C3A9CkBHRlx\n",
       "I2K8dRYQrIApgpTJ/1Ozb4XCi4vE4cqXL4c1++e2U4W0KTir8osvzAU4xWkPWF0ROsKKRxEvom42\n",
       "7uGx5ZJiM+fnynn0A3VWFk8tA+8gF96Y6VieEwBQnCTT3TVcbO//aCmXi7Fq0gdnFmyU1iP2Ul1Y\n",
       "7Ngb6YntOsPh109A0R3v23jd5SMtGY8zhcsX/B5vDln4aadqD5tpSnOdzHjjZ7YT/P2YBMZpSYZB\n",
       "78rOuWumT4A1NQ+dnNsZGDNPQPWTP3ZcNzoklI/cbnpiXRadub9iZgOy+7XsANuwQ/QkR6avbivR\n",
       "ZheGie9F1NyzW0BpI60R9Wg7lo5bd8jgNbQqrnGZ9/9HCPF9d8D7Ui2CzO7Qg11G15kge3Le+RFS\n",
       "/4cbuxYjr5k7riXfn5JU3Mr9LONvT4Jm8/UHL80JBv5yEy4YasR4UKNt+QiCgZ1yvYEXVemo2eSF\n",
       "19RoVRdx2vNg+7pYduCuSJR0Qw59RqDM5OmKXMIUfGbAW+k9n/o2bIFrBqiyWfTnD+4Sx8oHauew\n",
       "vEMElCrEmQEfkKTj7k7VppzBK2qt4nWzIx4Inl02ur7Dq5eJNu67MkmtbVKN2vPWGeyOg0Qo1rBb\n",
       "Bq97EpkMlWMt+Qd5ccKJ58jBLEDoFX/IcGUAICorsEev20ooYu8btSMZCGDWy0/E+kn5SSwebJKU\n",
       "ZFEmLtx/zjvFDazVQobzu5m9qflwnk7n/i5sSUhDH4aujl57Gc7iX3zs3OzVdTGRhhnAxdBDmspS\n",
       "99QyS9F4hzcSuPd16IacPaG0wUZJLhoyqABw4FOwViW77V+qPb3Hb1Rz3nzwRHAMdVpnzfI7XmvO\n",
       "3ft4GWQwtd7C+XOEWvaktLscg6YnOH0sKNwtSqHlCezW+ct54UlfkFwpr+q2gGJeVCSH6EB3Qqoj\n",
       "OlZA3mqgEjR5O/QKL9qOMNB540GCKK3gdX9wpbSmU0tdPJxZzSfyM0tMPbWdHoja3oBsrO5O4juG\n",
       "l7otE+HR7ZyXnVK0Ph9JXbH1MZg3lC5pVKmJq2jRqRZVyirjc73zF0vO27yC0W2IrP2xpWKKW6tb\n",
       "IZw6n8CfA8Fa1Kt40TsHhFBiPWbHcR6Y9P8y15eN+bnshNnJJS465U40hO7ef9rPIK7f/S8PpJ/G\n",
       "0a0OTUKBxYKIFiCjbgcuCzc6rK9DG5dFsD/2ouM95a6qYHNOLA2aqH3gcnHRvkDjpxH1xwyPajj8\n",
       "DbmqhugKGslqBovcVDlTwCBpX74D1X6Na3vjHc8I/f1BlCtwpKIznPTOi2D+6mWYYcH/lI/3GL5r\n",
       "ZFNkW+1sMpB8Kx6M2czt5dROx7JzZMWWKnGPrCGY68b/MEz61nOIm7a7Fm+Pe3MsYhJnm1d6kgqr\n",
       "qNre0xfBdJHVaosnClLYRSO6rxPI2wL3UGTF1IgoEeUgughLSDWHKXxKYTCapRP76mEOldfeBpN5\n",
       "R0ziu0tDOusqswDU6u35/uq6OKAXWgIN1WYApsNHkbyfL4JM8GvrbAoStw+0YxeZ5zgmIzAl8BM0\n",
       "SO4qnLMaRgo5OOi+XWHaPN/6f9gtNdT9sG0IvsvUkWTz0CBT6NCA7IG+P7vEPwJfUTlLy6xgHIwt\n",
       "yxEeUK+aKAFUCZKmWewxtzljqgc0wZ0qYORuSoTCeIQ91TVVTiR13dLQmVefbZqxHfOp+/Ed6Ak7\n",
       "lAa9ZG7shZ/fCJ3sSkAxTbq6vtuCvsag7WGeELnk2GfkeNRiz2fDTv+EcB1/hozjF0PpXpI1vqDQ\n",
       "fJS/chaNva9f1DRpcwVFuvL9fm2nimz1SaGYHgUzXCNJSH3lRmxS+MjFQXo4d2bzBe2+91jO60d0\n",
       "RqExycxh6XXsRyTnQUuoHqH8wUhJAzYEsVIm8A5NrmqtlakuPrBjE/V8hGkH8++lov0l4D/ZoYEH\n",
       "qa5gmz6dPkFFfY2uLvKWnyYp8huNmEqNInbWL6h9C6e7DE2K3MnpGCvTF2unF/wp93UVW5HS/vCK\n",
       "ageXmkZof45Xpz5lJkt0hEcGrl9pdScVh+Jxxj3EgRUoOc4IBXFdV2iZjme9Obnd0Cz16Sdfc0Wg\n",
       "RFGO2b/vaGZlHNLJTn3AXp8+oANzmcAqa+0i2g33wXfl95bMyBKss4bCHAG5Wwo4lY04eo3gd6dy\n",
       "5j08zrcTbd1iR0e/BRSttmdXXPfr/Z/fom+NFFDtptzPVLRxzW8ECMUHWbNnJ+BggCHdIhUlblNq\n",
       "z6bnvag4GkpODehaSshtcl51GBv+98XnSb/C/x0RJkKU7SbU8SYnHVrK8f8GQ4BhH3Ej7oFg09GV\n",
       "wIQVTVG0a8TncKF3CRZT9rkSTESCpuZTvldSXhZdSF3ZOSE+/ECQfXLn8u9vyyy2eWvCl4z9NlU2\n",
       "R1ZEEs2mlSUHilZhMRbeV/elDWsR4Jkdb1N1/jl+Xv2rHA1hm0oy6/BtMxPFOnIhiv68PLnbW/b4\n",
       "Y/+dfIf/OxhJmAw6Z3hLoqmzem0EQCw8CrmFIyyUqfpAaRkPYh9KS2dP3sZWYIVeplQ//MT6e0mV\n",
       "xwP4O50XSrYqEFqJBoWr0+C2ygXtmszIlyfRh3FYQqgqt3K3iWIkuKIx9458YiNjuIdMhN6YRLCk\n",
       "pfLHt410yVe7sLqgLd46cOvFOuxYdXO5WJ+umnpdcOb69ZmHDlwIyoLs7zzrUKwolZA8/+b1ftrW\n",
       "Y7fkiwJkdb/TLx9EVsQptNIYWMBrexRys4FkioxaNKmHyP4fDenr22NkSh7XrDPEshGjUsTcMC10\n",
       "yOQeZVIdzDGkOiXZc6gJm6Sp0Bw20XjmIr3H3LwWZkDYMX/L91MOxwWp0Ag/UwTaRWM6ctUH04c4\n",
       "GeCdjR8R2XEcWBx30VVHNc9ilJb7t5Z80aIDEhB3Yi3VHR8a0BpoHquaTzoAdPzJ03zfp17VhGKA\n",
       "5VABWc4vIfNJc0QEU4zO2KpEJdWSm5k5idrnti+RnA6QJ6OGqabjF0dgbl81iexB+K5/R42O2tEJ\n",
       "urnt3YgRoayvAVExIROdxdrKCWd6B4v+HTuO77WbeUKHg/m+PLEIRej0p60gftOJJzwAL/BNYg67\n",
       "/Zp4xjZZzO4pp+qADfQqiAeaiCFtC/IVzh/UWHCRiHOiW8jFAqu9ko+n8FX+3TqDCWIa/G10rVSW\n",
       "45i70XukpPCfobrvSGR5/rDApJxINKI49uunZzfH6wgpRx8eFGOA8LgO0VCJZ3kVMRD9niVfPw/6\n",
       "su2zrlEfI+UPi2PNMsYCyErhujs5JASRdDG6TzDzbBBjL4JyRB8VgSe0Aku4o69iP+3ZLZ3X4sUc\n",
       "HlhuKgVX0akT1yDYR2WDI7f8tUVRIfxk9vZz/WiUxMfbCNKhVN3Dx4LYV7noB0wEMzlOBKgRCQnQ\n",
       "IpuIcHzGiC86UPHQlsQ+3vh+boojpYSkGZtbMKdPmmusZzu8FmmIWpT7k15QAkSyC5LRJ9BPu5V4\n",
       "YNptduSPWA11idRRSyVGORkOdhiq5aaF3snKT+cuPrtTKze0xdp+gxttmLrq76rm3Zod1TKMPEuD\n",
       "9Zidiv5vOTJ8sUwu3sp0IXJSK2gDCu9P2yB0346+xpdDGXPGCNhhzwFLowBXMDYzXTFhGZqynPc+\n",
       "vhiVsVR0tOw5MpGo/tfznn7lwQ14BuXT09jWh5mw11K+boZIGzgJgJ9JQlfKDXyMe+o0s1MN0PKm\n",
       "KvkOXW7pWkhsk3fQy81eLXHPFzsRX4J8ifr2rsGa7zxTrou/on4+knTgHVwdsk4xOXPpx93JoGfG\n",
       "xz7YEJWW8g3a7F7/IBpZYlZhvW8qIbvLILWJ7DGHa+aUfbwKsy9wFDfr5ENyMcmhLc20GxRDK2ev\n",
       "Ux+gAJXRCUEnqx0J8PQ8AeYsaxhra0VqBQ21WIK3hYdeZaK57pyBhuwZOWRsKnoyS3mcFHLI9BWd\n",
       "5LzKVotE94n15tNc7gxBuzLqlgSTI2ih7ogfjp4X9b6jpxoFjqv3BrF90Q7zgJuFO/v7k11PtikM\n",
       "9Db8GEx2ledpu3gP2RPxIOUne5nq4+1WY1cwKYgWjlWWxAOXuBe+BG/drf1RByYoUvMpeLDJMnai\n",
       "pgsdh2W4L+R/O+T0S7AxQ9owhZyaEZsbJbsIXZi03AK2IIWxIObT+iwHNld+rLNl1sJAP+AFB4pk\n",
       "vh9oJ+u+FqaqXRUfQ1zMGDwF6WLHKSE5HsvSZLidbJC7ykokWvUE4Iu7xDwIcgFPHkrzmIZ1HpTP\n",
       "Gyio5YfjYR2JvtBmx3zFDmFU3wqH22Z0DMignaZMS9yANandsBpPs/SQeVtzhSWHM8Slug5wUGSb\n",
       "3US5YBApjerlXDH0F5T4UOILbagY4tdHK8UVdZjhIGXo7VI63HfDIbj44bvFKl+3dE67gQDkUe2f\n",
       "n1dZLXqyZFhJyJkZQakcKr35GZMU0JNzwQWYGSVCXXa25+OjVXPy+G0HDIuE9+Tdsj4nsLnKlEXF\n",
       "OaqbawQZb2wo+26709dQG8MuNY9/4arpEZGYWKS5SQwz4LIRddjwmGMxeTb4UefIzVoJj0ThKaJF\n",
       "QUMe2B3fyUtUjIgc4qWttXj+dNxvHlHOhl3uwia+aFClM67GNxEBa9kh8IKrB8AtYM9q7hWy4/Ez\n",
       "RSoUMdw/t3xpXguQIVCNWLAT1KkgBmBNWkHNEuEQgEGERoYZgckKI0L8rV1EoGvbhggllkfp2wnp\n",
       "NFPrnnQm0YtqvEnY5jNAFFS3dnc83kTWnmoO0ASt+FJaJTf3bkEEzc6LHnpzKsKktC5xqrxWILOi\n",
       "NXiOOkiieBZciOZ+krcHhVr8OaTStxriwPg20b8r0q4wbGGF3UN6T866aKsSHVHtFly6/5jPaGF2\n",
       "DH06NUGAA0JFrldOsP+GSFUUZ0bTexcEOkMxSV441Ca3xsoX7CM1dIc23i0IXYCNyFYszQm8bjnq\n",
       "lXMrWfzPJQBkM+QYWms2RAg1GB1tBCR7BdDo3RWyRAvKv1lmOJNw6zdRFOmAoxA/URVLA+Itez3y\n",
       "zrvTp3XsR0gEg68Q5AUkmCz3SR1+V6X2e/xsBB7TUJOu8QyaxuaC3KpiF5dEMHpom3A28wnoDuCG\n",
       "SDQ2fvX6LxXhLus5hJqZr4/WK6+2bIXyw1PHYeWNppgyqT7OPyk2scROyUH0/e+ClW97QPwRy5jz\n",
       "NEgubFmTewl6ICga9yj7arReADCNj/2wlOI3gLDs8FahbcToB6OuSWHg7u01OMmIvtCfUW6a8XSk\n",
       "gO7Zo8+cAkcHNnba2wd3tqtg5aOyd1s7Mx6gTwskOofmWLLRf/oKfsZyGanKasJ+Q24wjDnLT4Pi\n",
       "tuNWjKoIbStSvH4oE4znuuXG5BldbFtK3vyCd+Ak/4kJA58p0fcwEXtOV5kAIVJlYxcg2ajnF4nL\n",
       "c8fnpZJiR8Og2nTicHwjZvXHn/UjcqgHh0JmMQyyLO7NEBYpw38atwf9USTqx1bg7D+s2B3s3zXT\n",
       "4xD6Okl9Vc286/hZp38aUmS5LhD9pUuVEZoCJeEg5zD3v2eCSZ7IyZhwkqqV1S6a8QEWXUhs+ftZ\n",
       "mYyjVRP0/HL7gOualAVvL0SxLW3VbSvrhuowNopSlVx8cWWQExdUpGveGYIHTuYtJZ4/JGbLLCIJ\n",
       "L71PxfVbP+XVMO1RMilGCjKfd6iBEstb92jfpCAtMnk9h9WevnKZWN09ShfQZrI9QpiG3pjeIhsY\n",
       "I0KVTUNYGaHRKHb0FUoKQLIVySHom/QW0sQa13oN3MNReens6l1inNT2szswarkFYKbGEFFGTkWd\n",
       "VvX2ZBEoS+GKUAAJ/fmq3hromVVWatYXsbV0ktyX4qBSa9sBqIVfzcjRCab7pFVFKOORQK4jpNTE\n",
       "sKgTgdjfVHvaL7bLKmnXrd29DOFIaufYZLZ99FNA3hA6UMOqRgUD0r1aU9bs5iyoW54NFiAQXojh\n",
       "XTEM5dbpZRavOz0FLDXApjD6pjjKgDkulLiAOQvX2+hQY60Wn3psRUwe3/Jqvr6ut/qBNiXuJ3Tf\n",
       "kumfPZOTlWwE/qRk4sLeu+epV9h0IOz6Z2eYFHtDBQKjaWZZBccNLJoocp3rHTDOP/tPCJ00G18u\n",
       "C5kyxibb+j7aXcZlGlw96BbhJ0SVx75du/yBOhJC/028EvEdOGMz34FOJiJcz7nWdAdJMMlGaYY7\n",
       "QXkS2qtBZ/W24tZN8ZibNAl/kPf7X8unxveQUmFhxSRwq6MKaIPH9C0OPJ262AJoHRTqAvFeB5ox\n",
       "XSdUI+KC6aeKfB2hzn5K/7vpUNCZYGf5rWq0kBdLUrcy9SSJHdSDuUthSB/4y0+Jt16XKTK7h6JF\n",
       "uiivAbWhTLTUBnJuN+xuYpWO8KjvKUV7Jjz+Vwkwee72mDDg0jTKP6gHMAgOGH0O6lcPWSG6e8LJ\n",
       "obhDhUEkGAjpT/Y90O+5hCnJz9W9GecHubE1qO9ta6nd6p6qdj6oY9U9+ScKMtw9KMTowDVYBRt3\n",
       "2jMhOVORqoN4RFp5Mzzjwv/icP3QEMXbBeDCujVlwTIqnby+z2IlyrldracZlKNaB68VtEQZ2d1s\n",
       "/4+MD9SNoyUwKwg53lkdNJGTThNCLRAqQgS7N3n2o55hyHLZVIxAh5/8Gyl2oDPVuIMCs1zqpNU9\n",
       "+ZLoARW7NmvI0QEpa8vZV+DUUFhlpns8oZep97h/TFfYdNFWJQauMjPYcxCZSol1T4swCjmz3ExV\n",
       "0Ijto0lbOKU+pN0ncHskb/3n9tyI+hZJYZl7UKATQDyfzs/Uy7tEHAlNT78+ZxY+pyYe9CBKAO6D\n",
       "xpcnNtF+JKmeC9xoPYJT/GhQvyUvGbYbvg+p4zZJ9s5NJw3oDri3moPNLj9KUlyxdo5PPqblFcSo\n",
       "ZEfUg4pbrGDch6ZVjzAUxflStVtoqmnYZWu7rzqntiXFc0I/WRX4QEhGd+NHXJ2rnUFY9KXQ3MUI\n",
       "oY6CA4Htg+pb9F3viJKbQmHIZ3FAfAh9BpB86vT8gXJ9dXB6M/E7VuP8Nt7sP7URlGjnPGdt9jy4\n",
       "XKBDiL4CE+ZiwCv+Ha71vZdzMpiEq87rRUAo2g6BZnx3N2Ohr8ihZLsC/68L8jJ4TUYSxTDa8FNu\n",
       "FyhxqBWdcbCgKLTeP4Wpcxr0CpmfdG7NKPJzkcQcpVK+GU8CbrKbs3gRXJdKbDKtCfXAUfsY+L74\n",
       "at1ZyvBKx2CxoPxI834CXukM2si9KoUTZ8/EedkdhgrERKMaRHz5O61RCdY6Ar7Y/xPNx33EWKFs\n",
       "cnuD1vVtMhxUyQGYxiqvKmtgzgEFmFTQWt4CXTE5Mti+LQtK2nm/Fe3Y0NyzbyswRmnYZpuD9OPx\n",
       "7bnAEvsMFcstrnEYAd3vAgGCRg3uqKJRoi4GFfOMhVpcX81z7U68cuW9euOJqFu8N8HQAxxJieqb\n",
       "USnor2f1GwFyBVtXkiBJlS3ZqWX4jui1T9gnkc5d+2VOcMlUGjoXffwpuGLO3ykco9Y7qhnUJg90\n",
       "Cgoj35x7mPYRWeSE9sF3QSZmNS6xH+vJxpi1GWCmetugZu2wtkxUsVodU2X+7VaE7Ezxk+quIuyl\n",
       "AyP/MqV/Gac99/wjr7lxCOpzOIal8V1laAD+3ioqXdcpjt2wcI2XJZcc/4E2kf6Ssrd16jCTM3tX\n",
       "z0OmC3Y4cP3W3SKktKXFoRjLKrsL1iJiMnwwuGSDSbHWNEObuYjYUpziKXkCO9ZIk1Tg3NRXvqpA\n",
       "xcgj82r9+tUXA/uJtUDlmStxzbJAjoJnT1uoPzwAd6LNzhz69j3zF2NsaSp4NYf2Yi6FTGw/ukaA\n",
       "RhqFxcKRA9ujoOH3T5A3239BCEIUZa8TXHI9B3IpEaZGZS6L0Wbgwf0IWVyAGydd614AeUFEihTe\n",
       "Bn3/QnyX5CPtY3G+UQIqi7Vb0qk3SWDOjtjtmOPuBF74sYk3EoOMiDfuqJa/Im9T51E8dahjCUax\n",
       "nbqIiOIdWDZNypjaKFgLIS1g9r3D1Or72ZYHgGNgP9YN8eQ2nITnV+PGDXn4wje124a+98dJNUdv\n",
       "scvSMbXJ+R2AcEvNDrNcQXhDtBYTZKG+crSA6D8cxxjTUQoAawlkRJK5CP4wIilBActFlDzYjTLD\n",
       "a0xrQC2HixQ/Kift818swTl9gdwGbwnuMo2OgcIwl1mabEbTUYbwsaVZFCnfl4lwrULba/bLSZL6\n",
       "M5enNfKqIBUgeySQW8ohR1lutzUATW3MhFJdKLXShRdpQLxjBplcSuFnihuAVAuMPffl0B9pPcax\n",
       "kzrm7EQNB8Nwz0b9GZwzvEk5v0+vcdNeI9LTrvAh49a22vn17O3B9fsVhSRbGl4JK2K/PvrEzTdI\n",
       "9DmMhcEKJfwnpcfwBClcu7XqSBsImCZHuws2Pl1iBkscSR/4xW1PPPKeijxr9Hbbmh019qhWZJ65\n",
       "YnvhXihTMHmBZ4HcPBbCWqP+M/KJUx6ZGmbO5sIcjfBrIOmIm3juQaZZxq7NAs7D83+AMrksUeH0\n",
       "EUwh3ZA8ly0CcM+vmIR9w7cuA29jMRoQrRZeavuBHBdVwrCogntVnKKdnkyeX4pTec1GftF+JW5X\n",
       "LNho1TVZrPgIO3tyrnKVt9sZ1mLR6LI7goi94httcwpnWvPhcbnGRdLnW1/Xc2PcRAKZ7sGwmDJ5\n",
       "lo5mqobHFd2GvcTkmcgF/nm084th8Y+83a8ovms314KOFfhLyk3xyFec+E89NPgi26aUUvDk2GbI\n",
       "556v9YQuo3HhfxxYh8VcB5k9USSO0//P7n/H+p7y2MUbk8lh2G/kuZkA9pf1CCXw5xVQOIDSD3WV\n",
       "aOcAJOJktYabnXr8AR+5M5uPobZnFxst2owKQ0ndSIq9Ox6aZu1twFBpeW0svGt4FvoRyTzaptf1\n",
       "vjkv/noVS/2VzS1eSBocgr077A3IVVb9/I/6GKyzYN9B6l/8cnMpj+PH4h8U+H9wHyU8E0zRKAl6\n",
       "yPTvpg5PWAHDjTI/nIMwbgkTg5bFD66ZzOvO28tVuxF0CYpNJj5vsH3H4BlFapWMZONX1pfhGuDG\n",
       "/VFTuLbbmezOSc5QzjjPJNjd8p/hagyBPzYBptER0hqk5+H3ZAc5hZ4YhIXnBz2/NUEduzkRnvC5\n",
       "nteP4EAxVoMoUlgCSOkdrrY56Nmi+QXvPvEtkSh2H5o5iLpd9PRhEgW5JsuXDAkXEEP77VkqfS4f\n",
       "RctFwslvwi1OOtN2kvm3lGRAWU0t8U7CErhrO24/vp+KHeOuBw17gfUqV/mG3ASlgoGf2yvljd4h\n",
       "YvFmDk1xPkSeZhhhWfWMk6hKiiWQEM78aw5hE7EZErH/tGoczt5oaI5t7aXycW6IDNYf8Rw4hPW7\n",
       "aNRwqoLYCOzbQ3Lb3L0RAeSjLakBME3s7M4hVZrXo8f5JAMybydHsstgxMjMAt2flATsxpM2i5uz\n",
       "HSBmeGOsNYdxZpEZl1S1zr6Iedh99kBh/t/RIMFtvJJMsiRq0P2eUT1zkxgaP1UocWwOvMYZ3c5G\n",
       "VwjudpUVfA8VB8sb5u0VnUkzXJAwjcRyynmrPcIizvE74r+xqq6/SaP0pxV7bzmWVMpqrfc+9blm\n",
       "iv0Tq1yP4DNceeMqPy00Wy3ofktrlWyyVPDGxMZ10tuRiTebVNU8ZsgJFnLdSO02EI6GoZ0tuFfX\n",
       "R5RwWu8nMS/IyNGg8UPiCpjGheuozXVcrVIarzuarDrLYZaRj0+yQlFuLrYBDbhaLogUoYOWQACD\n",
       "gb1Ws44L+6fm6nsb41lbzjc/rYjO3apXIT2zg9RkewiXYpcF3horHe+xQyFX3NZpfTzeUBCiUR8v\n",
       "xlvjJvpaljmTleuGLesEU1L+uC4LDombhwptjZRKeaUcXhOxoFrfcyuz3s/UfhFmhW/k1AGg9oz9\n",
       "FG/ogeucaFLp9ihDp/8WIRpo6Vr5x4Q+gEXXkHdombRZCR+uMcYukDE8NBIG0yUAURvmA1/jHBi6\n",
       "RjdkyygPwBE9pR+ziWQu0f4JMzuaClXB4kR08hcdqAzSgvczhTRG36GNfSMZg6ilzGt43OsVNWXS\n",
       "O6qAo0BAETzL1+hEv6HfexI4Te+lXo39KiXHMTFxxeggbuL0pnYSzWuRFnHiC5IaiGxKZ/XXUj+O\n",
       "MBHz5U7Yk2fkNOUpKD6QsgL8HRvEYQ2NtmZ7eOtUQ9eKbOlmuIdcV7fIeYA/G5e81x63gYWf2xpR\n",
       "EuU3YMCHuaoG9tLrXA3mU9XWyAeyyLx/4vsDONjUWmVWPD5VoGjTqDvl4FYMxCiM1XU0MedW8kff\n",
       "y3FscF428Yyta6Hdi+MRQn3w3WnRFL15RUirm/wfHeU4ZFvtgRjHoW02p4vA4pOm67D0cBwoqRd/\n",
       "AGOEORkpBYAKyELWKcJrXPhV+st6x+0S0h4BKyx4K3F9vHzmLzfe2VbkoTyalEkQtdwrADRqO2JF\n",
       "aFF1cb/MF7LGojpAy8xXs8QhaFrXIsC2HtZwOBGDwjWO86U43NWYSii3YeyIsTqzXH4hZb14MmEw\n",
       "5nMVEiU/xGUqXo/eelwKFJpQcYnOjLLJYly6BnnhUX/IW/bHZaN7VaRzgttGp8B4UBU396sUm8uh\n",
       "NCBF4/zK2HV7LWEVaspJN9fcsuM+IWnhfmXF/MkAQgxULz5iDh8fRoz2OOs5yRdxMiOctL94nswv\n",
       "kwI1K9Hodtui42UGERPchV+GwxZ0p/QYIar/8JyFxsyieKl+qcIou+QcOuzceQrPNgA/ZLKu+cIf\n",
       "Ok21i3UhVtXQ766pylX8NO8XzwyYjTDGGLcSTpP1yrNOXhOaIwEZlTMeq6TrJXzNIWBxGDAKdqwx\n",
       "ka8kBwq5O6mgKyq7JhGFw/lLdL2nGZqSrPtYAIEHVkSlik+UHCgicYkfxburto22h5FJngknwCj2\n",
       "5K4Ai3Otn6Z2yHkB/y5LCLt9esv3ROManstmrwPQluEiHEyc0o8iBqfibr08JHUBiGmZ/KdUPnvo\n",
       "lGaqh8yz4AE9WTLTmYh81Fj84+cXmI+sH/qdPueLLADY8Z06/3GvYVIpGOMx/EmQbsugAv/OBDtZ\n",
       "j4quR0PSOyOrhYY2ke5DuvNfPb3j+km34s3rr6sDxMaSOjLFhHDfT7ow+THJTNQRRCTFaPxCe0jQ\n",
       "Gf/nt838WXsJvkzkeP/Iabk4O4tkzDuEm2K0RjfQDxitsDZ9w/d0/xerPHW6LdT/j4c+jNl5++GO\n",
       "mv3b2kA/8jmfq1OnKDfaNl+fxDNdNEGF3YAp8A4lNly/sfdy31NunK+fcr5u7Zr8J6xl1EHj2RfW\n",
       "JTtzlQ43M39UkvpXgwm9gaB6RFBdRmp6hXwEFHGNGYOEbEZSzOj6ObdJ82k28FElk1HHq+EXpiBd\n",
       "4fchnYPd2j2xmrXtLVXNcEqeqBh8/zC7GghOhmmYjKVxhFjSJDUS0w/gR7gHlFuZkNlSjJtUnN9I\n",
       "/vpT6CT5hcihTTRx4+Zb0XaivWo+O8g/89m9YVQuBkTZTMqR1YscQhrKAd9ZW28tdqjo2ZYXnYFA\n",
       "5AyO9kyByl0/mGp7YT+HgKAD7U92TXMoMiiuSlwZDX9ciZftkyFKHfaP1zvS6CIdI81T4qpnYYWP\n",
       "/OoYDUlkJvFsyrU4zh6vvnE7cs93qPWPNho/pzne/Zv7Vn991l/hU5tdyLOLTR6JHkYo9Q2IZDG9\n",
       "2bhwgwfDclH+b7v/FIwmxVL0As++9AINlfpPhvmACC1Fvo3/BlJalQUNgM0oTpQ7/KvVhmOGc07G\n",
       "qC2XZ9Zh1TY13W4GAXt7KISeR/kEQeBSRZqMiSpXHDMozefUNrgj2uEvluiS51ut1EvlObfnlBdG\n",
       "JamFtHTAkGW4Cshwq78vdkRVXrOL378nEUzAnLqFv7NoD9o2aEKQfuZsYU71DAqeKF4Pk5PyKnkg\n",
       "gB5yMlFyvmAA7BIjv8dC79iVeUbTYnMYjkWWjTsmkgJBr5ODaZJ1K6yt0nFgneJxahwdgoPlko9T\n",
       "evCmPXjQMT4Dlk9+T5TcyKQf0ifh7QYCBJqzgapLHMQ00sZBFzbhDqynuX8SMjAph2CJVuqQq1pE\n",
       "jTQwbXJs0TneN9gVnll0NZ38g92X9LGpwckwtAp9VvjE6exj/JNHvR0ZnYBOabY7nm1GdEMQ6Jo8\n",
       "Fvf4rRmpSWXBBxVaJJVnOGLQbiwYfwttECIF37cpeMdctW2p5xBJG+uw+V/ZVU3PYDEx0KXLCOiH\n",
       "OJqonkvAEQy5Qvhntg6+sFP7zeh6jZ0PQaDxVpBIQIiltn+5LC/Z5AaqKnImuFgoAbiCYlzu6pK4\n",
       "wKoYrKYyeM1ZANxCjNueLbbgaWQlYkAaSOIbfWCcGu6xk0MB6DzKGBNrbrcGYG+2Ho3V6HgNTqXd\n",
       "ohZ77SQKEUqn3FHjRWDR12DwSsJMmehViTpDCUD+S/5XZkZmEvux3SPaJ3pNHq5o8PBojtS7wu0W\n",
       "eYsoycYOvFQZzZ6UfbhyBXbJtXfpwvDKRKDEx7HB4x1hYkafslcHJddp10S98+EoUfEnKEHZShs2\n",
       "4oq5Fe/b0/DZuP9aLHEnSywkXfwBP8Aax08+bVBKw6vrVwW4S0L2sNGeOPX2XZ3zkCpiHIhCwe5+\n",
       "Mk07txbthj3HikKcbTUMcbJsAuYbmdXaPGqyE9MPcQvefqiyPGg8lo6Hu8sCtcrcsS1kWPc7+zjD\n",
       "YerEjPAtJjmImJMDUZll9iXacr88+nGPNJlDtQD1Ni6mtxEK21f2RUmA4rLVLjnFC2XS3OVagvPn\n",
       "gXgKOon93O+Y6RXv0ZtFlrPPp9Ad6GdkawjaCZkuVaDHDTgFey8x9HZZ6D1YDyJkT87E2u4CEWBH\n",
       "DVzPPYe5bbWqGmAhneNt86QY9ViVdL+XgHtWU2td5vf/9CkFeP+HNabEb20zKGbWjO1Q3mVDq8Tx\n",
       "db8YJlq8uF5jfVCeIqGqzNMIdlUQu4e6h46jO/3TrMr36w8lEBEzsrOpPKjftdV7Ctir4qWbIW1M\n",
       "y+02g+Mzk9VcfGA5aCJiG2nBFfrJ/0IIJ7/BMLZEDsBGn2USB7K8XjwbO2DuqwCrHDDsTpREpfsd\n",
       "ujsqvPxmtPeZ4GC5ZvXZDbMDU9oponQEOwdtJBf8PdqXI0F+kY/Zls7xBSJkYzRuTlRLKwMm+TMj\n",
       "wrcw/bkYliBjGTkdwxCygsuuSigLX1S1n4n8HQFW4i7yytuLWaafXYPev7OIVfyFEdeXMmirqhIY\n",
       "2ziN97LnR2+VPiuCnnG8uW/WAfIJniIum4OcWIzCau+D58bwU/IL/SWWnOSPtqWDme4ZinMpZYCL\n",
       "bpXg3xhAUdOX8h3+IZoVMSxFHT8qSDz5xVPUuaqUdfceMcmrxiwsxkzmI/SZ8OvQayF2P4hjzoy+\n",
       "2oePnfzsG8u36lCEmaNhpqfrYJ8cM51UpSe54yjNv1pFlkvQACy4AjB8mPmlkU7ktiuDWhEEAQog\n",
       "Rf+rlm2+4soLMDYCLGR13DIyyOzgf6jXg3Bk2u+iUJYEZAOUIU4o+vIwa0iPMviMIHAwTLy1UHoI\n",
       "2IGLSOfYn//1QR8DRkZ8J5FTFVns6BZ525cx70gGAUzZaNOoJyL7u+HYHYxGEWtml/ftLCvKH57c\n",
       "rxOfoeHHv/4OwVFNBMnWOqxz8kT9/O1lFjLnsZDE5gXd6/ARCPCizQ4rd3E1O7CrNDK6jl4WEPiX\n",
       "bvSfs+Q+wCGujpnPp1Q7vJMh6UvaObzu5tJulMakSj1ihiZ/rSRQqjyZTIqld38pGpWd27hEIlfK\n",
       "9aYxErKlptPTflvC8KDiQqGcxaXB62RWLzXEHDcOjJeMn9/qHE20t3hT+2S+diMY26YDVeqTnGdr\n",
       "Ou54E4YqXyH3E417+zvuv9evOj3JCJPkGfElYRHXJmXMKIxADxlSFJeVr4TWmGfiWATZfq/hizOi\n",
       "CRHjYjoeoyr/ZOGfCjbFy4AQ68pCvxiihd0Yi+rGgNLcJESiZnwUWIVwcklglJsWEfbg0SlJWqB0\n",
       "/fWYOgylJgXHmxDdZZf9NR90bmOw1ZIq7EH4yr3h7VlSLddO4cCjDgO707fdtGHRGLmajWB0rlru\n",
       "Wg+Qo1OxCOZxeGiC2oMDB3Ppo8G9ukza3jO/8TlYeZCjABdDxAh2MMn8qawuRMXkYaVNfwZUCCAr\n",
       "pbOAQiQljWirwVWKpxV5SrH4BuO1jSK5iC2uQiqYQI243IiVNTSfTy5Zsd/zRa/+Mp9tX0fntoW8\n",
       "nf9MSDuk+jBEi/WSBkpNglr+IBCAp6m4/w/0+x481360nEcHReisNOWgUAnVn06OdtD0IEpgyGlz\n",
       "sK7QRRRfCScG68qbVKr8kTIqEvW9A7DSUwKUh52Olo68rKZRY4M+AeF8UcM0WEX/Ahhcp1iSwNkg\n",
       "D5dMWdwXx0qoSvKg35snlrKLnw2vrce6d24GyJINC8n7PrfM1fEHa933DgfikFO6Tk7ldEi70vbl\n",
       "l+arY8KOYCHzK9V8rutaSKQENW9r9wVGyuKx8t/lsl6Zla20eS0FkQpbQTasS4ug5olu74bamaKl\n",
       "Yds0c0iEg/HnsaA8ARPtNpF+iqQBJSHbiEEoNsX2F7E3I5ceVlKMlrCg5pXlHW43vuf9uW0GUF2+\n",
       "q6Y0nCj7i84NxyxC4N7+4+flcOx5ZbkbjnG9wh3ve4M9ueIondgE3q2HFcu9wtXkZve98LK9MELs\n",
       "dsIQn7rzizozgQ0TtF9OMTF9Yrws/Zb3kDNagV6nHb0bI8s5HdFgxgDNEfWzDZHu8ZJ7HgQ45faS\n",
       "mXQ27fg9ypV71dlh3q9tjpozYdCEco7o7x8EAiBvxT4sX1BJNptgZkj9q2rarunX7KOYpKmXsoWY\n",
       "HX4IgLqxnZSPaJkfeStFP1qtea8Lq9RTBOjNHPziIhglyArXn/jUPfBziaI0N/R/xKy58tx+XpjF\n",
       "Li+TKCo2i919BNRaaZvVniIRQp0u52ovBJlKkeqOaJfAaRhXY/t2nmXR8bRPAqqpQZT6iu325kch\n",
       "oWZGXwywWDAqEz+v6RSIrMxX3wtfAfRsSKumkMy03fMbkCpJFZHTaD7zRLkZpo8xDPy/w2iy913c\n",
       "+m7VslpTlPL39XCUTbSh53Rfb32xllmkbchRRcfdKNe4r9+KU8o25bizmOUnUqa3UJ0TqooizRgU\n",
       "SaiP3eXaT648BwndbH0gKpQDHQ61d85buxqiXZqRmIw9LUsS9JYtC6lwr4Eb7kBLr3jN0D/bWK+y\n",
       "6JegtnodWljKBHfnQKIKuVIO0RxrTPVaXrS+1nkllRuG0oghN6RIVzrZxjnc8D5d8RxK7oS2EdOW\n",
       "snhq4IYoGpS51agMFWcc5tzp4ki3qpzga6tmEyNX+sL39kJOmS7GkX5RmsUKkXlHWv2lPtpByAHn\n",
       "ZfJikoGAzVxYwQsoeKANqDB0G3eJGF2bNZ4qARacCvCfpIrp3nEirkbfLhFBNFbFjy5XqFxEQdDe\n",
       "nycahrdb8cwf4xSEEtEpGT2y+gRwcSXEgOjAeAT/C/ZQp2sZJ8MN/VaX4hpa0ITj84wQTmd1k/0Y\n",
       "F0UfVU+dMaKiq6N89esruJCNFOeUCBhd7XiRtR022G6j5xwTD1MVXgY5ynn+bhXX4W1LV7MK8m5Y\n",
       "2oROHdlf1dFho3YG8j6LM/orUL/AED91rBnwqtBBocXlP3zAeNXFVFWhNx3TlY04KwsM+Wvq1cno\n",
       "/AopnISRb6DMUbUdBt0ONlFHgFEqyGfnujCjB9Zy0KS5Ee9TPcNK16FqO+6Uf5A2Bx7sJlpuJuVb\n",
       "u9Tm2tXPEBqcr5KRzMhu0P/fi7b6tHBpt57JzeHGy23cgmUUd9uPqw8NqIWBEs5url6xlnGwTu0f\n",
       "YpDmN3p7M2/vwEs/Ct9HNF01sAUTMuGKgJ1pUlbidQOivKwvN93McuJYJQEFrqlEgx4ZT7EDToNH\n",
       "eaLeIh+YPi7ObNAV0ro7NbZ5OIWkjS/OL+yiZbeN+ARxSVJ92907Zz6DJvRP+bffeym1h1i22JTS\n",
       "8etpuJESDqrJUT5vTjoS+HWnkDHShV+yhALENbZWIsiYJa94vFb8Q1NSY0ODrHMlp5xJNCm71VxY\n",
       "CnrKl1Ul/nsbqGAe9kHcXx8gKCz8ZHTt0dZQXEngQHqIRzQYaq/mIsnWuzydUCGqdeFoWL8rP+4M\n",
       "zHq1r8u2NDXbVvyMtINeFe7qaffAZ2+AvucKk4pnSU56gfOblddEMaexmC2xEk91jIjIcQvAmwoI\n",
       "m9P9eX5HoKzdCcDrMkY0V9WE6Fy/fCyuB2wx0uSTHEYFovrFzABkdt9B2A6UVSIFmSX8bHVi+1gY\n",
       "dmhp+SaLRxhUgnHKr30kxz7o7GgajQow0B4xMdGQmf3r3fZlpMnLwY10Q2cMroVlUo4bwMgLpGFN\n",
       "Kc2fu9OP8eJEYUODeL6U9XlRh4CNZy05bJvjacvijj4nld7iu4PFqlPcXesXj4cfvk7Sl3IJAr7E\n",
       "LEvE69ZjBfS7BU/1pLdcdzONJidjbGTod3s8w55xxiD8pSG+LDJ3vQGp9vn+bV/TwixcGeO1b2ER\n",
       "tCJiH4Xvnij2lY7SqH5/qnr7MPCJfHh31YfjDQQ2l4ZB1eMh8jYbKpByM/0UBA8BG0YdJ+XKJS39\n",
       "ts3f+9dzABR6INf4fAHaPMuMgv4WjENPN0TgH9t0dI7G9SkZ9NJRfCXr11l7fWyMEGuR3FD00M7y\n",
       "/azaPxz2U5rGrDZndwgMUp3wiUKf/uER44lkOktk2Mkne2nRWYinPNm7TKXO8HUw4KZHqQcXn6Mg\n",
       "/LdaSbK+SObCdvEmOQnLKoGsq/FtfYQpyWNylaEe1m6u4VyRfTJV+H7eIJSRSpVFm+cIngstB3XP\n",
       "qiYjt3bwQdnk6DdqynaLIbLo0234seAtXM4e3M1PkgXqI+Z7dsIfSmyEKk4Fx9VSa7HS2OlEsApU\n",
       "E2vCsvpvFrjOP6O/DB4b1DBGx4OC0NGhztRHPHqdZQY32NXUi4EzsD86uNSXQm/Kl3eZ0O8CwYAc\n",
       "OwnGZxpjkl1dR/nIy1kp5avsQtG6gvtLlZMTHQpU1v/Ga3spJarM1kYbO1xu9KxKqbUlD7aKnfQi\n",
       "3t+FScaK3Wl1b/w8zroj6yarD9rUQlm5Tg3nKgXUkctdZGlSPikYW5zUL6AfLYUP3UgtIbVw7qDv\n",
       "6HJXgzyxo2BR3gVJZbzW59/mHrERzwhpbuDwr/02sg61Z7WRhVI4BsXGmz9vlrUm202j4aJp6jf6\n",
       "EVkajsDc39xu+WyXPjG1CEaRZMMdhpFWnG2yTh9msN8AdK2pELqTsBzdKNciD2nG6eFQ7PErY42j\n",
       "fgRB3tpuO5lKyLoc92oGdQAgg6XH0kqJ34l1o3DqVwiyE3DYpa4kGNb8zMY1d7pCz17qgogs4KdX\n",
       "AX+HXg8Ficaks3ZIJE5eLRrQ3zJYtJOsyVH7HyhhuqaT/kjifj1XeOdDY+PJFh3zWGTypd8mgU6Z\n",
       "fJ2Wnw5l/NnGx2vMCAtNYhDlsarzfrsd5CEpBUl7fsTIwPJI+sT8v7HcpAH6g6T/nvt6kD6I5+Ca\n",
       "X7bt21cwtxhLHSjcl6QRsw25rmN/zrDy/Lxqjs9xXFzsPGwGDrWt91Cm1kgXxUVib0AvQCykr2vn\n",
       "PfuacClg2De5lOKTyUxvFubEqk3xnVZHv5X6izn4H3Sub+sZytSDhdgURq/Fy+rjksp9YdREMGdk\n",
       "bZuyzBdJaglQPjPys3rMGLtzwlrILBigrszyRihfH4/VQRBb+C0cRGJ/CY+c5182/GqfSPg68n6r\n",
       "sOkHdv3ymXXS5FtoikaosWWhG3s9sDeX6Qpf9V3ISVD40bnVEvPIsWmWslObIfZuQdME3PPYakar\n",
       "tDclsTsl7L/6ysujJeM3Cm91ovNLRRIebbJTw8hpj6FKwz/7dVrg9M97JGaA3Au2g23n/PW9ds6n\n",
       "dEeenlCFT+4EiEQvw3zDlF//XRXG0avqaQeRG5hHSOPugIvlxTT5w4I2BqipWZpkFgB3360vqJ9C\n",
       "RD3lshYPXumrNP6q3rKLKEvJupudPqxFFwcXplLHMx7jS2P3Dt7p9TI9e3hyWKiqEDyKlngVZIkc\n",
       "XuOYjieQmGKZsHP7I1AB8rZ20oZ0Q9U2+dBdlupLbnc7eU14/E8iJgf3ysL8DQyiMeQyZoiVODe9\n",
       "KyApAxFDEQbNXM7WKmgCDESkAlywQe1I63yxOGkt3yjX2AGY86+f/qsShExNGdAahPza50Kdc/3J\n",
       "p+JWRfrSw1G5AuS2gzektx1C2MNDhIgE2EyuQmzOM7qE2I8DNB9wBJTsbe1HZgu+MolzG6TbbWDS\n",
       "OcgS3HJ68loahrtpxzIvPjecZzW8X1r/U5VUAtA8dMf1W84ODBOkW3RSYcT2EndLrjM5x+dQJiHb\n",
       "I+hpWPxcCz6jWfOSoU2PV3Yb/InXGRqntZ4SRs7e7SSI5XYcKRzl01Zn6iLLFsoQGJ+m6UhdcvSz\n",
       "QYSnqk0HQRyqz1eqKpbbmfg8n0dVhAIjcmJi505an/xgEocAKaGhA6T45G0+i81Rn1fmdl+xaQOe\n",
       "W5UJ7kmTwO3d+rRcsFVKfm0wwT6k2C9XDYnJjmjBzesFYKaWH/AeJ/GMhYW8BNXg/U3eJHMofEas\n",
       "j6pj59t9NG2DJfjiKT9zfxmKbmMXR5V6epO/BPagQXK7NA2IelNt1vNBRglVCZB2gq+oex8zwy81\n",
       "PWU6Q6y9NGZ8TOOqGayTa5krTHqmAlE6XufxidKHKnNIpoyqD656Wz+ed49N+b3t2Ng8JlvU7nY/\n",
       "xD2PDVz45JLWQkF5C/WEPD75snl7hvG+dnOkSq96Y2ASOyImBhD+Ps/tQEXSf7gUHnnfbgFkv3Yk\n",
       "DhRFmoQnO+gSCdSXZv109RE3O4WNNDiP24hWfkwHw1+Y/Mi0VuO3Pmb1Wfx5yYIibhv7XYuGhVy0\n",
       "4Vc76c0tu3IbD/1DuI8XBFTdThVbRCjkfK0D9LGuml+o+xC2YUhVmDG10W5OzjmvqQ8SBkM3eppE\n",
       "lnPazvapFeamUiwHht75mbHUW2KcZlvv9INy9084khrjTBLUVInZ+P/1/PckQxZLCImGcFNVgr6h\n",
       "RUJtY6vGlbO5Z7gjkzIUN/t5u6ZA3qHqgl3pEXTVJuT1obz3u7GovpEreF8QstL9F94KzTbd8CVv\n",
       "GS+zs3dXere165au2cirabOKO95R+YHSR5v28j2BcG6/iXjjIHaIgyEayElAPXEfWTrzciFxmHUw\n",
       "nmxRC9anBOUORWKZfnu5fbnaaP7yz5/6rWgfAxxJ+lOXSXOZEzzxO8SKspS/UDYWxsWBiRimIG1S\n",
       "oEzvJxNeT3dRrtHn/xaxn3TjzjOlrf7SFurN0mPIiZonBI4N3+QQiNsLaAR6f1SHAEcRsg3KBypg\n",
       "sdfkky325BGss2ad1wfrDVxBWXny+nAHhKIbcAFh+jR2XmJzP+rZwq2gIuJcjjWXNGugSp8xcKzJ\n",
       "Fp3qJ+5cgBvAYN6HYywvq5jMmL6ZPaZd5yMS+AqQqAzXmEz7/4RBP7dqKkvHNmL+EA6RU0jlCo7G\n",
       "eSBrVYXVKMsjyHApc6PgkG0FL6jMep0grZ9Rv76hU1GwkfYaFR20IgbFegEPD4jw7PCnHSJnZosi\n",
       "J8AySPcEvSzfztfNBxt4fHINUW4+qxRM66ssrAYwJwPUC1Fft35wrdDsFTb8P+sjTluv+BRq7cOW\n",
       "Stda+1d85tIlH8qOE3fWb/JIcOR+CAr+HlxjM6jaKxNmnALHCstKuWqwg8M7n6UNgFf01II3k6Lv\n",
       "kC4cifEU4WSStti7wH1k17QJldHh11SlGUdMZwQkt5g856roZB8U7DgCXSx1t+KsP/WvVtpOE/Xx\n",
       "SGhO8QMlLgcLkiXUb5DhH8M2xVZ2+y/k7NOsEf4eNs2TSglL6AJZFaeS+BqvD2AHprJZPbiNrlh6\n",
       "rpYJgH1DUW9uxMc7sTQInLi7Ql5v5rI1i9dT+wQlUth31y4iKStIo4/80cozJk3HRb4LmXExG31J\n",
       "/1w+4aN5jGUyzSM7seODxWsKjI5JgN3NBELtEFMWRvKUpnR8qrQJkb52cj3k07bCT2xBefLes8b5\n",
       "rPxsGOqEmWXWgf/DRqw06De4A3GcFNVoHuRZPXypWaI9I/xMPwUr5CQI5dKTVeZMbGMfAS9f6aG3\n",
       "+gYGcfDyLNlHsWxGw1udls14SO1EVo2EBXaZsIDMeEQ9laR2TuCaWoii/W23FSX7+9tsPpYtE3g1\n",
       "A5ypdhKe/g9aMK0qsn7dkuYlvHor+qBk2ewm6NM8qvJ+LajVbV/kjbIePrlORW9ZoTdizIwn8O16\n",
       "JDRhk73v8O8O4AmPNuFxm2kZa5iER/LgsFltFkybozVRem3fqqdfV//zGqqcDZMlgAEY+S2EhmYu\n",
       "oRgR5NDY2vMJ/lb5gMfHDdDa42Ox8PhXajXJQwMAMYS+dKbVdkJMrtEKY3SJ8duIC3xjzyELbD5g\n",
       "s04SHNm7y1IlvFC28lIoh99z1QJUWVtXR1QvdAZnRkRkGACrd1hslBConZ2f7GqxSXf/GfJOgdSS\n",
       "nMoW1b2SwIER6WrWV3ifG5EnQUIQfQoqWSrQO9ti42KVWnf/o1hvoW6y0uvWDZ8QiIZTdKSXsDiw\n",
       "Avj6Rib73lWnyovPwKb+HwTF/h57EDvDyInpVoY3E0AswmHscXzqimivYbuqPqnn2xfMBgVmTwgv\n",
       "/DRnKSl+v0DUuIpRU7k/rZddifcjnBLZzC0hzb4sQ1dBr9v+BsKtgG2Rd5P3qiVDpcDPMlf/zLWM\n",
       "4abIBOpt5mLkBtT5J0g0M6ifVM2Z62bKzSKr9dpy4GBT0mnwTLjNCUQERdPsNqa8ProIJvD4rNAR\n",
       "6JFRbZfclxwQ0Qb/4jbNalL/bML2KSVtffBq/OOA6bwbSj2gMXGi2v6NwKThK4vcZk7bFeA4vVg3\n",
       "3mgQqxxY1bnXWsWyk36PctDfI1frpogKvDptQRJUgzfUOF1YwfjSinydKoDvuco913filRizmVrV\n",
       "T8uj/Hp4UhtF/FI0rFJPKdk/Q1t6SoTj/mASyh+YvEJdS6CAGcaA5YvA9nkLC13mgl2ORypUuS8i\n",
       "wRUkDCnEthM58qccAy0+PJEOoX+tcsNxIWF/AGdwV4Zamj3IOJ9ETqlkzf/M7wVU4CIZS7TJv2ar\n",
       "k3BoIg4XHLJjIWRyDl7efJnPsnveR7rztJjSPNRtq/lkGymF8EuJ3ZmU4Z3NnwglzrYSCo93Wk/Q\n",
       "Xw4norLLwrlS4ybZTgYFO/N0zS1e361ZTBl4Ht2GJItPokaUjB1S9yiaTAYA5ILXiVtiNkKdhxL4\n",
       "FhRTwJpHMhSglJIA8g5TXski/UaZaA/qUuz8nOgunehhN3gtt/dz8ys8iXG/pLJ4F/Li1Jmzpn7o\n",
       "n0ELwCcrqhtZfUbxL7vJJWUeKIKvK4r2MRh+KlCulYhkxuCoJzySL9/4sK7PQ4keE5PTovfnnE+R\n",
       "++Z/wQZFM+KPSQWFahrh+CJz9HUAvgfMKvsvLUxKneVIxXizACDm3wbbpbB768GhBqORIM0czUvZ\n",
       "kr4JqhQ/TEJFKjGDnoLanjoRMcFNIB52V+vjtK9d1H4PK1vlauButqt8d7BiP4tKb2decA3qbYny\n",
       "aT9Z5Xd1tIkx0vXtSwErWdiRW4NScTTEurnzBVG4FcxZWXFW67aDOQHkmRKBGma9J+VFxdbuQtRR\n",
       "iIda23ZszT0iyp1nVBYezNT8MLZeJhArDBCwhuqnqnPKr2oGtdR/kA2KQHqlrPfbbxazWJJXkSAo\n",
       "KF/zkF1WDh5Hr8pIPMdHAbynXfZ5+G9CD33QhyTKDpvi8GkxgRmk0p7yqJCv/pFS0iYt+CLf2p24\n",
       "ZwvBmR6wq4aWLcMeJlYNybzubzh+srenye7I7oOmUFXV1ORh36NqqAIzad8PXbS5nbvy9t1ubTYd\n",
       "jy+pHFb1lcoG/1HB2StHPi6RMULJUr5TNqJsEch5S5jVZB51QfF5NXy6B4xXyAK03yBjE2T9OMCJ\n",
       "vgJtS6FCbxigw6tBRIvNSBz7PnSuayYUzvBqLVHkep8V7DcGFQXWt6pthaR7kUygibGFS0X803Hv\n",
       "TgLzk2kgg+NF0i6VkRij4VcgjYYw2K1n66+LT7tU6ZSaX/zogoX1Vrl8kls2cpahEVjMN2VrKPxD\n",
       "CQN6dSWdktc9YCyz6Gzwl8TNjyuBe9BmE21QCLMH1N1aO/mUQjjxd4En9XArblhb086nKOLr93nr\n",
       "u6xfPmPRdjVdIpvrHVjaAaXlGClxBO9wDT3GElEJW5Oeq4KmsVRhUox1Yyi5cWl76swWPenanZVl\n",
       "qBpWWnlb+CZH9f2PePEX8YcKNasgwTHh1MKUduduy1HJuV4Z2gK+PaLW+5Dk893HcidfnWt5sUN+\n",
       "sMGJqyxYm4L5tZDb6DpjefGabe/1omDfh+PK7pV+fLoKfq5SPbWPkDZpMiDBeAT77Nj0qW9cURUO\n",
       "AkhW9H4BbfJHAyp9j9De9BzK49euF3DuTNw0zrx3JIZenK82+d3WPY6jgNEUCLxIW1LHCew3t0vW\n",
       "d3KUuFC7TIvMpYcAkSjfMz1n66aT++GSA7B/P4r5rm3Id+VV0tKWYHc9+Oi4O98FGLAgeMIdb7io\n",
       "UyXm3JE79Wv7tTA4kvx13MAuskKph1QL3UtcJR9fd3t7RFDxpoklwsKgXlpkRE0dqOvo8iXvlEyE\n",
       "sTlslSfDo08g91OcUtwAzhS9jiWJjWHidnjgGvuzSiXP6BhxXHzrZyHxnrEK8xFtM5GMvf1eamsC\n",
       "Yy/u+GNQB00hhw4ImdyBIdxdvpOo4WNYeZjjgjTXkZIynA4C7gUACfbBfaKpBfhgvUWY1uwIN9p7\n",
       "6q64ydyEhL+C4vESHIO2jC/axVEG2MV+RPGFX3FfJT3v4041Nsovo8F9BfsP6i7k64qDiCZJy8ve\n",
       "bn6nJrHjrFK7ZQkAZD5+hGyYhYxtv7t+iqA6R89vBMyVT35L5rJpfnpLDWBwocoqiIZfmHc278wx\n",
       "wwQKQFZTscdZ4f9pCvTYYcC68kYd1BJOdfoFxW4RgvfRVbEBElhdBnzWTC05M/XWxLrZvPYAlHGy\n",
       "A934JdT4jI68QubLTMM51FGb1qValhBIqMJ81C1IpsYsUedQDmiMlXaBC2gLBdxtzcnfVB0/ismf\n",
       "yVXE8UNJD0S1lFhkAfenZ0vI2OSMTHnzC3gB94NjWs+QpqsqR3+5MAIWUf4ivQ5MEM//aUuq/9cr\n",
       "Txf9mxcqBRLSof2rxc5y7xWPpRLUi0e9ntX693FSAfqzHv5m2BH01ynv9hEgLMw2+teiv5A35iEo\n",
       "Ln2aswlKhUROuo0uF6CYtiyvAIBtLoUrQ+Eo6V8sOgm8kk+nL4c5ufLr+yhGhLqfO+08XIpMc+xG\n",
       "HsiY2IS5a5IHHYnWjlSBlu8HTEelO3H+hXtgN9Jbo1ArJABqjl4RnnJwUoR8g1rkW3HaRgq8+OYl\n",
       "Pxwk5wRFMEDOTDx3YT1xrbIKijhqRk7VPsoBTbVbN7k4q9GrkoIe6AB8MSL5lkgEapQOXHuRm0uF\n",
       "LKjjIUpc2t5WrYdLzkgLNjFc8DHLMdUQhyl/yBWxXKN6dfvHexzjPl37tNE6FybKWKDkdYj70/8m\n",
       "BkJ/DcQYQNRkqsvuj+Lz7+N8tbATkEY7P9PVyobAhPMcmcAH4Vhl4pbZ4Lo8UEWFNHg+ZXqWZH1s\n",
       "COuZFXXe9lXsmkIIzAAGB3hL6bUtcH2lW67/kfoSoeRbzfgV2GWnO0zpnoI0C69qaHHVtHwZZ+Fm\n",
       "UDkeEDMmmUGEsdjDR0gQW66CKp/2tabzwagPgDWO6zOJrJKidREgUij/UIlmhmps85nPC68Dk0wz\n",
       "8OQjR4PT057SHguo5l0o0E47e+tYbT0g0Uec3nlOBiadI2K6l56ES8PLUcAY9kwveScDWbyyxefO\n",
       "ztDCPJ/gGve1DDc/5Bk1a8G+gjBHyqPbynLWBGCHYHL7bZ9vG4ZWH5bK8GS5OOuh7Vr18iTSZGXI\n",
       "qPI9o9p+K+svayB+BPSoyW8vgfRilrdZQxuBJ0VURndbQ9D0Z8N167OKreClmoSL8+ri8Si4H0VD\n",
       "cSr75rILGuvGSKl5y2aCECqyTyNAWHPG7i7MBsgrhAhswJ3FoVJmpz8deSBzEuu1SqfSxKNkQ11/\n",
       "0H9dWz7+Oi/5bsMuRuslK6iVvdDtXXZ1zbMjw1FgUJyvS8baSuE3ki8AsXgJI49lkAI6niFIprZ5\n",
       "UT4wfH2RhZ+iR2FRfpWgrKbkkzPj+FxJQ32Xc2pbkVfaW674PdYFh9f7zZ/6olU32e0NHiR+vT0t\n",
       "skgSw4p25sQLAg5W0iaAXgq9ctK4PmS0i/W4ZtI/+B95pqcRRUVMO4jImJS2oIyPQpeWgR732SZv\n",
       "Ep4+cLdeqypY+m9/QTq3T1YqrnTq/ayZVIw/9ZmGaOFRng5gpERdaGZ5GM4wpgfmqrRPiDZVHfaX\n",
       "76ZVNaZ2wyNo+CfPCQTWkoSCtvhjqyC9e0cG48LxncWldDBEqsfOPxkhcKXm9h+boZBgbmGkdJSu\n",
       "bfLdecyGD8TQLp6N0fxGC9GuFZnyAvI/TjkFTzrIZV+6GgL2OFycoyDo4c9zgMJd7GNRD63G/tpe\n",
       "RiXckWPvDz91Lsfplm2FfgaBz/Z7WIpS0ACeyBNeeJiSdFu+V6OOxMU2CBC4CTJ6RjjNdKiBJoAG\n",
       "PnrMOLAS3RrvGy2bHh0YtBs5g4BwIwsa+Nt04aFbFhW6OgWzSE6baeBnA1B5uspRuwYyPBs4f5zC\n",
       "K8Ys5uqHxPrLp0/WgiC9M6dDBcCkNDX4TdRKy9Uhlp7aFYI9b0lq8ndMxEBblUJo821fZClyxOzb\n",
       "F+Q6TAc3OcqGi22cf3qDJAK7unvr2ywah6O6ZCiRVBAy07kBHkxvrfNGfz9qXeMf9VNs50SDQJ+V\n",
       "wVsjlaapqBLfb1iYF4Jy58BvhVbvqyGkNqZ0Zo7rAVxgcqWwz9KJyPpqVQwSmdoBsbqPM2TT7OXY\n",
       "y2YC4f6HT2l4WgbQbQ0HWgBjt5H5+igxEP9ZGHV/OnrYjnF4A+w1zUlz0R5esp/xLUycdg4eiUSH\n",
       "ebTsj7hvDLLNSzKuobIuYOYCwNo//8JhGd6pFf8VIjJRlhbj4CnRT94mxfZ4mmW9YpCmEW+BnqLZ\n",
       "w/Fmwn1HqxeimDw1VuEQv2O4s06imSAP1gSV0N3HBZ6ZkBb369iuU+8vKsmzvx6euEHzB3MXqR01\n",
       "UVT+Vl2SWtzslaTDP0L6r7mIBWFmsOnwfGxdsR25LMtkj4JfHaaTz/efHyDdGun6sQNaq1A4mCII\n",
       "cCQAkRjYjOnMrJLRQRkAbyKGprAKScaktFYduBjAEwHytwSo6kq86UEqJo8toKvS4U4my8XynT6u\n",
       "2pXi1eEe3y1Tn335ZMEWJm/HV59iWILkFhQ+H9JjCfMNjOkgq3FKIF2z8YbCmkg8J70V36YYpP74\n",
       "G08RGvcyJBcSzcnlqNrXvShLcfVn0zWN5PAmmn9kkvEJfs2FS5icd8ehia1K/12YXvvO8uFuIj72\n",
       "Px7TU/s4UTV48Uqm6puPcuQTz/+61LaoHVb2EzHArdD3Bua5cEZyFG1I5nMJmdiYsLwaiLsfgbH0\n",
       "F2boZFRMwilwr0cFjboM/3HSsiv4PWc7CSc9rNF+ZRz8rKyFSmf5M2twxNVrKQb+G2/sEzpwd/do\n",
       "6iLnva+CvYn6mbAXgVKzNpIImO8exv0u92IMwYNYdc5uHHScOdRKziCJVF4Q2nbnavn8GTkyOBEl\n",
       "6GS7HF4WOeRWi917hMd44HoHfnmu641OOO99DgIR/GhPxc7hAuh6cBrIVBgnpG3ofUQEhJt1cLzn\n",
       "2SI9nDtIw0bxB0psrLzJVZE78MkT1jHpYCYO3U+MAf6a2sW8dx0yNCEPhEaZMIPBQfEirkaVaNnV\n",
       "71Q1xt4bfL0lhoDssxnhprdHgQEO2WYvaEPIAmcfaBz+KeCUkk9v8fw8g6UTnxQbfwxz4dN+2+TS\n",
       "XK855PafsHKoZEo32bA3xhKLBCZQrLX0+t8QUR/wegpDdZlpt9ISi99b7we0vv6heIuI0sWqBL08\n",
       "DV4le0xPsFr6Uw7yLB4KmGmiTfEvbtMrpkgoysctlE1mmjU1/tQdyjrpWiCT2dhCC/9SUn0uYzGR\n",
       "jHHpLHiAG8ko/1OSg5YBGMnSuRvs+6hGXUWU8wXxmkI0zyoTTCWMJXeQ0KGkuCoBTPqluWsgnUHp\n",
       "fd4F4zG4ANZ3qha6pCQ6Vt9tqKNdicmLMufg/FLCMpVR+kTauUUMwJqOLe1IOVvnjWHEp65iJNZK\n",
       "VmfsgXO7oYt+OfeE8UeMVKwUZ7GGZeg625ANh0cHcsZlGDMza/KtMEB/uJyom1Dk7s9In2i7itAe\n",
       "51KVhfJljSLahGLnhd6UIGLKlkAatrX87DS92EGlouaXAALa3AkPNkeuAAAHyyt+ocmegAAAAwAA\n",
       "AwAAAwA7IQAAK/dBmiJsQS/+tSqAADpF9VVBwACUqw1dh9PgVZXoOKiuJwcmUjVrspFmj4gOdhsc\n",
       "2XrFPBAiBIEptR1u53+Nz/pSNha+xCNUoXFmTXTPvCPrUREdmgDkYy7bnGZAzcBgMyYqq95ffyF6\n",
       "yzegjZovIdvajQctPgTG5Mk6C43q7OdmINx3I06kQ8h5RkBjGxwlJ1/YT5welz9oV39gNmLfgjji\n",
       "CS94FysW7HbOgKZv6NdjhS+JcIHgqmiuUob43RrcaL8oxnif3MXp01ZbQfsteI+UFwf9Qqxx/CRM\n",
       "br71e6LoeLywPghagGoAZD/0yLucDY7mYLfLWWlH55k3BgfvdkDXuhfbnOmVSkPeCmJxtcUj4zle\n",
       "n/LYS7V2Mdo+0C2C6XlgLuAju4oYE0wu5R9poNJx5QE55LOdJWTBOQ/AFeksTgnML9+BiCoSHvv1\n",
       "7XbPf6MshyDIjAzPmpZyRQ4zUVkg50G/TQMprB8wcj46DO78OkQqEttMjXshMMNAZ74i/AsP40PP\n",
       "P+RgG68lyOxp1fFF9vNGz84A6MXlS/iAEbH7k87FBUMDf7nIh+p1N06jJmpd1TwJSeFFfyBpSiC9\n",
       "6VHWasV+p9gus/xsrYbYumWM/gDQT8ydGuqFDiLUJN60xuf0bun8CyI+koA/9fOmNfxYXng7zpxJ\n",
       "yPY5OemSCmsnimJpp9bx+vC0fJQGy0pne9yr0oaTAjHM75bw499LKasJ+F3yZqsCdJPIvLhpzEbs\n",
       "XSJRxg7EP6BZ6PeWEgujjCm3iyk7My7WZmd1t+sRnElSPhfPCnrthFTDZBg/GL8JLfU0ZXWrx7Hj\n",
       "hkLVHr5PSBNv6St0FNRmsywfDhBk8XBSJ+Nc6OG06RhaNPsXWL6FkLnyW9ov9ntsVMlj7XUW7lf6\n",
       "CXTXb8CHHVWGuh0B647MHpieiujQc/+Vh50j+vOIM8XUpLxwkB0ubxfHNr23ZDiJlI6i51rxk79t\n",
       "6bgL0FsNo7F0qvvvErMpE38k66YxU2iQ3QG+35kz37NxhZpE3EM7WuR++q5NaD/jR5xsGM/7t/pT\n",
       "9b0XXjmGBuG7MbAFzcJbf3/euPDkQ2HL0bfTtT9OzAte9oribKcQS1lY28ITJLOeQPpY7j7SBrTZ\n",
       "Op8ihKCkKYkYGih4HFANjoUBsvbgZ/zz33AK7K9OB80HhAuOVaahxDdCIlENPEnewegzUVW7WExU\n",
       "kBkm+EOC/I+VxdNlzzMHEjaBg3VqoCOCnjhWJkLZzw1RDSyfw2EGTKVYsWTH5ntaqaXXeMk24a3n\n",
       "lunrCYvV0sFdyH4I4ugsczfSdjRk5u5fAaJR2dHxwGKqejPODzfOPD3S1tLYXaxmD6AiWBizxzDz\n",
       "8fAmxEWW2R0Y8eNk/oIfhMiofPKNgc9kKenageuxBiLqXMFV3n8yERCAcZL2PIfhbiE7rSkkOas7\n",
       "OjFAyZnkggK2a+fd2N7T15x9yO8IUFtbgI/Ynx2srV0yXt2qkLWIPMrBZ2YKZICr5QIBh51t63q/\n",
       "ZNIan1vfb9adJy6kSTtxUdQZywfVhX9pTOo8T3tili2IHQfuG6AInzIx7MFyaT9UBm4wFldbl2Z4\n",
       "2wFfjdmk7mf6XwR3SkzelKx+DVx+tl4aQRsFdo7KNpTfKX8f23vc2Bu+suBADiiiC2Uc25fDiMpi\n",
       "rzn7BX7uqRHkJvWoUHpriN/zWcanh9njrRWulBS1AJg10Jql0Wg9jgjF5cLH0x00mpfVDz6InH1h\n",
       "dZVaoVrPWThaV6ic6d4I12P1Fn+JdDQ8BcfwQ355bRtxKJ8nrcU8rK+rFkqOAC6m0oXZhaUEBSUN\n",
       "hK2P90ISh/cQ2kNQ7zLd5q62medv8Q4C5D7RGtJCG12tLwzIaPhJ3ubcRzva6Fsm/hl/K0bhPZEK\n",
       "JgkbgZj/cRILK8mJdQOUwFL7PDBN1Y1jwcgnUC7Os/XQqbH19LRE4Zc/Nzvoe7baqgi6Tkzp9Bfu\n",
       "lFUshCCo2tBCyDAbd09NoL74UG/XicuUl5shYF5/42fH1zS5sstEFoAoLkbD/hoKj45R2u5zlFXa\n",
       "wwhCJd19CFz/duvCZO0O5ZV37cTKIoX2UcKGob1L66UBMj/uioVIb50culr3dAsZqPwYsmMq0ZFr\n",
       "DJSendJadDbC2ZR/WT4SWIARw8ShQDVzAcrJBu3T/hqfGHxzLHhEhevYRzqewcOQDxgkUu0rUxTd\n",
       "pxpkRMW5MQG5nkUKV7aQN61QB9tS4iuGqoOIkbGEYe2lYzxG1GbdF7nHCN3ad2mGjLRb5BkvqmGk\n",
       "T+kH/D8+/w8sg4uI3mXeyXAhn2uDf/X0TmZatTgSsdIqExxMOwAE/B+WaH3mgvEMv/IXu5wxiIcr\n",
       "fRIHj268qPRafI6l1wyrf5b6pfEpebR7YND51FgP3h0mYRG4L4uBHwFiOAGRCsgKBNcQSoM0XcIW\n",
       "IAnNnNZSCp0/eZ/4zHjZIzR4m4vl3T60pa9mPMEgEy5nCX2gV50K9shtxEndbyqNpS3zmUvFFp9W\n",
       "ljoiv1yHLFt6sf5FKYdGdzV/7mGL8vljhjbSDHXV4C9ZWVcvjUJEPYC6FFxP8RjMiPv5MM0Kfpry\n",
       "T5oTKfyTIRuAM4UqUtYZ36PXLxk6uDOG7Ooz4JeK8wFqgQXeRCe+yUSthi+52YrYd/ka/yRKPAgH\n",
       "xa6rp+2P5xKDqVl+yAqvheQphkka/8xDIhImXKtVfmFIu7uvci6o5y7vP6G7RV9VFDU3TRe/6Nzx\n",
       "RDtyufyksocnn4s+Im1k8OjCsSLS8GNcjM1H9QkLlje9fHmH/ALTxqhcrXKPvoOh8hF2qE3qzIT6\n",
       "Cv+hsU1ONjOPHytbv8ZlTV/BTAvut6Do7I9PshHtm2XWBl+YwSiHFN96O2e5ZVZh0W4f3arWOl3i\n",
       "SBBaDwdiyqhFO7F1OTzlUMLiui4+Zg2TykRqIkAxFHj4t9o/y0REghN3XmdgGAr1QiA9k6STSHAi\n",
       "Zllx74Lrb4dkQy7jPjoL/EuxJUC1GAROT4cTCTdme5dt9jZ8RsNTN1nTEC4GgSbeD2RYLYG75xlC\n",
       "WIcEAdgjUBViZyGT/LkfWQnD4W7bVonIuyZMO8796cY0cdXyI3+rbRS0MNDIsV8SfYxTCQSpxkWa\n",
       "4oUKcPMgPZGPOA8UD+4fXf4CvRzeHac4rVonqMWvuK4SEMm1dcKgvGuTC0T+lVrVre4HPj+Rn4gw\n",
       "Vs/j9MSxkxzn38X8CsTdfkcNYpu3L6flXxO3XIbpGN1bx5RxqnsBCiDfVaeMD4d7nuKS4Vdd6Qet\n",
       "5e0v4ycD1c3xB96d38J3A9OsKS1UJ8nfO5veEkyCT3Dt8nIQKk5c4lLaIum3vOuErJYsosDtlEhd\n",
       "JuFu4/y7Ln2iP9c15sUgs1p7mBclL48+rLzllW9OhCgxKF5Vj7JizuCsWAu7/H8O6peSAiTkRpZn\n",
       "MTbrH8xzbp7Q/7F2Ahp/rCWFkwdcxJ/Ds7HxFVFUL/xIoeQrYF0FeeZI0UmHhJpR0wGFgvbjuege\n",
       "3eIOQS61PbjZwwM0y4KDPq7Z7dESIxW38NucLgpJZGEkQeaVZK8d6zqikFg6FL2wDCM07x/nuEaF\n",
       "zwN2eeRdqLfIJOe9bnx3e1fxWucaKui927xDHX8uIymjf6Duz0GW1RtTk7gjTB3iZmEVDtNK3bbC\n",
       "pwW3kEUS/WOHmepwEBcKLI9AM3aA3oDEQJpDKRZ4A4yXZcWv0/otEEj2ekPyTVRGw7t1rB+Ruuvd\n",
       "sDB3flx+mk4q16dsnJPZ+X013257I8g2C48/gUzKtsay60wtLTaynPbQmY7wn5M2snRfq1SYCZTi\n",
       "tQfZ1Po77HFQzyWfhsPZN16eymPZgF/pOgwfb9yq3wJ8/KeEbnRmRSCN/32nLEI8tTwpLxpl8Bod\n",
       "JJpwe2mcO77PKps5qRVx6yHTs7LpC2ZnFlouKYtI0/EvIgGrbkb0bb4PRPjcKpao+4uAKbYiNLIL\n",
       "ZqZeqseTNBeS190JiuyIXsEtH/3Tth4dR950XsF3vETpGTP8YKbtqd4xXHFf76hxWsGr5dpFDvKh\n",
       "FFgrvOK2TpoSgpyu0aTp86Qg4htT0H3oRH8VsnR55Y+8f03XpeIeDrhmKCPVJj5+8xx5aMvNrpaL\n",
       "FXOrv5WW9qg99jHpJyZiDRG8k7fT0HlofYvuOgkGH9X+VOtxKYCY1NibnCDb82qBwVc7m3LbQoWM\n",
       "T2XCVvRZPhXlqDFPl/+S4/2RBxhvOX6kJO2yBmmeR+GJ3Qe1rGNPz6V58QfiD0TXebYkhCgOFdpo\n",
       "lZkQJdIZcRSi7GLVNdUgVG+KvfqEBKL16dixYB20xjP1Aq1p+BbSlg3lYuGcFHpvDgdmESe8HjCl\n",
       "kXYww3Xo3/dnKfv+jGny4L8GnidEz/daunLFS49lF8p1HQPm211LHS+K3SHw7ZaOcLhEHMov5wh5\n",
       "g1VRAxNXu/CAIgYh4tTvRr/SUgh9sETGRZVSkWr9Z804Vdtv7f124SxR0PrQ3wELNe5BlHl42ZPD\n",
       "9nKsX5U0hUPU99PBj1t+uYsoKNRaMxl6oQIJwflXJL3FfRSFm28ucmlRsuPYieHKsE7Rjzp13BOD\n",
       "dmmkjM3PTWR3k4ryPLtoTaZtRuYx2fpzb9xo9oeyjDan5xd1NLRn8N6OrvTMGMrQapzzmRQ4rBEC\n",
       "adfea4GkFzwENsSFo14Os1Kg+a8TdwdpiXb1lOv1FQP9Yx7dTW6O2rXfQOCPZegGrMS+xx4oD/3S\n",
       "/IzWd+zWT5iYO9dZENppy9T8az3L4ysh3d4yQAUl5zOpCrNewgVHT3N11GLMtfXWRZE7dDFuEfxQ\n",
       "654mF4d/caVec/dpnshqM6Sl3ZAXD+uinO+fhV0DHpR9AV3CoTbc2fa/IsQsf8N7eGRu1+ENHHxn\n",
       "yYu6OkeeY0wDGXv17nXxg/xUrxeQtKMGQdWy5L5oErNLPqhun2bUSmKcJQTGUCmOld1sc3P81y7M\n",
       "vRM6TqamTfBNv6tgyCI2AVwCxZsPTjFpmMyTpEvFnFlyS9PUWbfzUBnLYdADq3+CymQt5f8vZBgc\n",
       "XpuUB9GhpFtKoHgpWOiWdVcvCTqGfS2jiq/JA1Pmsg8nHPtNXltJG4TB3VvZjqjc276gpg3WSChj\n",
       "drq1SXTpc12xvsYImOh3a0qXqPlbuISciAJ0mwMCs/pTqRovVXcsonAGywtW6xrwNnLA9QjVE3ut\n",
       "qhG1Q7go5wgLmXh0KOMLw7ZgIC1jx0Zd2F1Dtk9KxCk29WbbdMKoONDa5/FBU4913Pvdi3QFvMbj\n",
       "FkW5zCuxfnsIk2vjAqhkdvS2nJ5JxQMVa3kHi/LkIWSWfrNZH62xgec4uXGHYZO0USyQawUZom2P\n",
       "Si+pl38Yv4LHyFZEzxc9o6yTq34vtD6AXUWpEjTPH8XH2Q+o55zc8S//03NoxgaDkGusJA2EqrQl\n",
       "A5dqKCk5n9xei5V5YjzMslAZxjTsKpgfoYN0veJP/gE6AE8u1uMWmvhOe/OPuoxcCz6C6Mwudb9A\n",
       "ajw5rqVgKRSR0JQEKq7lQ8B9oSQ0kXKAFz80wh06uzJ5F9flNzTeBuIPeKkfmP/Vy/5uHJivvf03\n",
       "kL9lvsPY2YMdlG8LIvA1L0uEoIYYw+GKvLqNPaEbzu34offbOokGjlEgVQxBRPc5nNN744aFbjZz\n",
       "sG0yX2uspOTjkX6zA/FlEEuGqdg28mgap0gCaFuBZhTmgVXsW0Wbvhp+/XGOHGqdV5wgdhQWPF/a\n",
       "HY1tXPQ9GGYDv27HP1ub/NyU4H3NO9nx7mVNS8gRrPbcSEar6jRLpJl1ReleWFSepYbYBelHCK9Q\n",
       "mmd5haVAapNZUVoWvR+XnDkPzeKN7jS9wtHC2PmMIOSSVey2wbIqIdO5o6dCCDS1LHWsIhPu3CP+\n",
       "hAYuDUWPCX/R5NVs2WqSSBpCt+UNcM8cru5X3TtK8d1illssZPxnMDHSvS7nCJiTDuPD3fAZ9/BB\n",
       "/QLWHwZylf/wvSu8dR856qNkYVkYXANYJD51XvXN91NDwIXhusHPBJYMVvW0jeAzJdNWcHuJhbW7\n",
       "JR3O0cwIny2mtBN5fLc2xu3q2vOURGVNlsZqb8wNup+ok2dwDMqU7FEy4RsJvX9u8Pylur/59RXR\n",
       "dwxIqJmOkFzVVk1BKZ4NJHkNaQLZjlToogR+C3V6pef+OBpttPIzX3v5oSStE4jpq2+VQ2FKDXvk\n",
       "jm/abhMtuWPWQYMKeBL++2deom8QG91SV9zrlQEDOOsBxmee7KzCtxJx7d0BJBMJbbwt/QdwqaZA\n",
       "DDSunPXicb/gY1MavIpffF+S1RMdTQEVUUKVwLKHqaD1jcQwKv3VTOFLi9CvdEg8Cpi0jN80rqzD\n",
       "oZnByWV6g1ND2L6P82Wbslr+3U6QkxjrI+MLT4+eAWkR0l5Hqdaxes7gz9F29hod7WM7lYegmBnn\n",
       "ZzfDv9Cvx2472/jzOi02rpwx53mMFMj0JpGUXkGvDLT5COU7yPY9PXHlOszRQc1IsXXUWgmsPTYC\n",
       "M6z//mV6f8U4O3p1W4uncbRIUK327L7DWYOjtgOwwzr+U3cvM3m0I9dykxQf6Z1FsGzxrrxaiUXp\n",
       "decQmp0f5P9zfAiT++aOGgiQhzSayAypg+aBPeLPb73WWWwg64Ir5dVPQ1i1B15Cz2xEcJCrpUub\n",
       "iHVzMrmiI0Q3ys8caNAa3gxeqNykyYEmEZc4TuzIwvIwAI+GbcMGRoYS6/7wpyyEVylowGiXehcQ\n",
       "tN0xc8FZH3bzKu0B3ZOjV3N/xhDEkf/Ymyx+LaUeTwjz91D2qZYlWm3I9H6iVJQfGK20dEk+E4uP\n",
       "we7g+1VP/aW6PV6O11VQlDj7pISF0ZL9N9azk0JlVnMNIs7C0Eepo/SLt0nUclji0UgvzvvU+xtg\n",
       "RwYwi+Gf9K5w9TITjyN3pc47VPXQzxwQedyI93MJ0w7fdhpfeSVziQ0dj396IduwNI5TUAAPAbgB\n",
       "bARxzOMEb6PRrQgzHOttfbq4skTC+htXpfdthNjpB2f/UD4sSsxuFDzbrX4JOOsdVN1XgifJ3ysO\n",
       "MMZrQUR3+pICrALM7SJtoHDicqDr5hZEgR6saZp5VrIPk+Z6vl2pI/KAQFIu+nWE8ZX9ybrRVCF/\n",
       "/kp1XRlZwyM7+du5X0nWnLRZHI15hs6oAk6fw+OD3LL3bBVcgiJ73QSaGRY94EyolcQK/5Vso0oK\n",
       "2YNIHD87n4yZOZPRnz+Fi8C2o0ybf7CfsLFQf58Lp9gc+w/VJo8sH0VS3adHk/7Vl1aSRCIkHeio\n",
       "aP4QUPqNRMvBx2licn4VoDCFKB4IL+cjpyzuLfGo7lerfZk/2Atb4KR0o6njMpQpmrqwnedMVHxV\n",
       "+XeBKm1a7Q6u3/L0HqKhAfTjdrwjuPUf5cdvBnuq1ovzRyBSTFNWETm5PZxVC83a2gUoLmtBNrxd\n",
       "1gcQGE4QsqR5osKPhiqg50XYOyfC0NrWVp7+QpJkbv4dTP6/l4idii0/AMvkQfr32Er22uAHFj6C\n",
       "aOwgqDov2nWSkNTuQCzxmduNTXMI2ycwZUAzf8BJig++gc5ZKHiaTEdpWqX9E4wK++Ql6BWbd14J\n",
       "vJVvZ1nlAeXayW+TEYUbh71bGYem0C+KENAU/+yIc+kP9gt+HMufW6sIapbe97LdzjGUlVpo9IEi\n",
       "WFVP4rT+HVuqg3AgwoNQP9uGGFuitG3MgKgpfgDjVGLhyomAqFTWKhcE/lKOQfnd5gn752AdMrSg\n",
       "ofBL85BI9u+E6ZEmIGgEdJ+xUSdIh+hn3HMuRq7ZznOyhtlcKifc1jA4Dug5GK4zCPyOoRd/cSqU\n",
       "wQv1apq8R8i3rjmlkYImRVz6S5UIB2wnHSICSMCHeYQdCecAD7FmTXJ0Gv12WTE0saHl94X/035A\n",
       "R7GcPgU9aDPdHCLHbszY/2ynudAnt2reDQU6FXO73RwOvhtAh7foH/W1kK12HYh9jNRnICdnwWN+\n",
       "G4lnoieTlRpuNSPy/Xv+ob53BOU//Y7jsIjpYclxr4Qh4IEWbXVPOQDMLJjgJjOQl8kqCwYh5y2L\n",
       "CJHff7YzGDgUB8fbX0wmBBi99gYzpO5Jm3uEVIZEoaly9zJBlFaq2nZWF1M1JDddKO8F50/0NKFh\n",
       "tw3nBFKeBpBV5sTkSXnPHxh8M/Lx+Q5lYcPFaJNMaXTV5xQy0l0e3h30wGSlGll8C/Wrim4PWhtS\n",
       "mt/aOsONFokCDZJ1BsQPzmQexqpJiCxyxo1KLMCpzU1VfoTe0M1ijRobHdVp2zzc57YUOKVpjlNM\n",
       "Mtgw2FxlVzV+GP5VjDNPlMNixwQE0MYS7JH+3tcfX6YmBactnf5IPDxFnMi9BO3SORAwnH6yzJIY\n",
       "HdW8AbxnBfsfe9IRxVwjQRfbwYk6tHEW3dw4UKP4Y/7alj2JNqMJWUKFw3gOmjrD3qgtobg82kZk\n",
       "cBD+vGspTj5R0THPPLsXaLqzcHZYXE3gsslnvMkjnM1baI1q+cgu5+OnReBbufERL2WZ+2bSFi1i\n",
       "QwfkjW5jE6FVXjQbDE85edWdF+LczVBhHLcg8VK5yKUrTf9K5EE0sSsaifmezaOBCAUDPCdcTpE9\n",
       "8Jaa12CGGhuoKjJryBkO6Vgk+TwppTj5pNkhns+fKcgkizRUQk6u8acgsSTKqEocjbSCekRI4fqg\n",
       "HSl9RGtODQXngvsvgtvEi2DsN/ofHJWg3ExRLVMwmIKK/CLM4f2VW5TCJIurq8rX4AfrG7tZLLpn\n",
       "JjXaMuvr9C4/4/Babv8aFCvDl+e1GpeHSeFAhRzQfzqJhYhQwUHeoIDhddoVTecHpl+SMjucmM2Q\n",
       "Wrm9Hme7p4F4nOdNMRvMksWZHak11PR7GdistVighyVdKxOKib04JJnj4Hb4m6Y3NOi3eXXukP1H\n",
       "HLLU9hcLEnGT9i2FkEp9nCxUNR6SWRdsQ/xULAILndzXPs3LVXAKYsdYFfed1Rm/aVtnodc+XyqX\n",
       "pFXsStUzFuI/IimeyxfJteBVMzbPPpXQWvncB3cTcYStAMmj4kAfv8Nm9jaZyFr4LfjjftMPCgo+\n",
       "ptgN2ccEsi8R4BXnax1bCxISQ8OockEl6ADfhQj1eTcISR6I8sfWNlp1WEbv3yCJH7VwpYsS0WSf\n",
       "TCO2Pf/YRUHhEh7mCx7MvAOlaS64F28gn4nOcV2oAx1UzvdSyitDJAMzB6FVJFQ4bEu9e7Pf/Gp1\n",
       "YPiRpt28SOpX+uWBmrTDzcWawS43xz4HHam1ipYZLJtYS8Lc3vE5i1c2iIO80ueFGT1VHZeNh6GP\n",
       "CgwHBFxnqIGDu6hf8qW4NqvSSXh8edeSZAP4XoLIcSw6U0+RAgx3QB+2rj1MiLxocmh+IuhF2ux7\n",
       "aFId3mhea9KrrTWDrjQLjYCe9CNhJ9U65shBJ5yiFS6zV3aUnpjtIOgqkoVruXOGp9RVS1mJJRFN\n",
       "Fx2YfaWKwgqY+VKciNbhrud9Hi0hyud6HoIXizGSlL2IplyjJRd/mgMAKytnEYANPGGr98LVSuFF\n",
       "wYrQXbK9Tj5d7rAiok4g0S9J/vmBTbGSqqTq5M4TyyknbICfj3v+ozw1/XZO9pbahl0hEiZYtinl\n",
       "HtyNvUpHdlZxxgUTPQOCpEhSbS2CZQ9PCjdRwRLYardM9JQI5d120vfJcdx1Xgy3eoYl1Rc1+YgD\n",
       "EvxwYqVHe9HXkI9Nbt3DQBu0CYALmCwe3u7CL1+SMakyXpRFPQ9MJk2uRQwfSQqNGHNAQf1l4cjB\n",
       "928sSc7jVZV1n6xE56963rU/BGsmSCKqdlTQpayrVRt/Quqhw3Uo+HDtoX43nlAE244GghGyQZHd\n",
       "yqhfexyMJv6N7sE++uchvr2aSN1GKtXeKbgzwrYTd3v9LLndYjsR+uC0iO2QSJUgU2vUa9vli7Wg\n",
       "rPcAmLjQeAdDEgvbRfWdjzWzlrttq5QETvW7auwu2Lrc1/GiDSt/1kUmeiDfT/HOTBqhBtsRtSIf\n",
       "4AmMzHIlTTmVFApwlbshbynYof4WPHkKUh5r0ANdhBL4tzwu35GYPsHRDihyFo2ubLbc2bMLIGvd\n",
       "ea2A3XMxek/oRys7cXxdyf9+PrpH8jydEemYHNOza8sndwCpCOCBD1Y7dMDvawon0sXWA/qMVxT4\n",
       "Bc5cdUm7TeWP0p8n1npgNhtrYBjcpo095riqMzd7mpl9xxXnr/hXAfUW68XC6IAtQPxKUmyEE2fo\n",
       "twcBzXpy+YpDs5DAxYMFt3kELlkDA1piE3djIQdW9/ZM8v3M6K1u3wBFlc7bnOiM04OVizcsDSmc\n",
       "+x1+JLtNmNjGneGwsl9OEyXg/Ief3r6H7h7T/pc7MjU4pTFZhBtCSiYOoiERexnn78llqzyD7xpv\n",
       "sRr5bOTJgttUwuUWF5/ArVFz3lNqdhgYsX3uHrkLxLgyAgWq/nPD8W0O2O9oh+rNyLuTK87ztleB\n",
       "fITy/A8oDGwHSgZTePuaM49DqYvbS/w20n+eOX8kOFw5zPWPu4kAi3AXS7Iakk31FyaWQBVlR5HQ\n",
       "W554jeOhGbrKXc+R0NTX0XphTfpaC9UEcJdn4d6Oo7QeImU3VfeQCt8dj2wwlGyxi1FctxJdCZnJ\n",
       "fSy18RJN72WQc5YhCPDtVfsXm72swUyyt8oH3y0bX0oBvQbzDIavabXfG30HokCWjaVq3KUQpHJa\n",
       "15+z0TYqwH6I6NyjdYopvGAwDnIFmQde2lKovRuxeWY0qZRmhG0JUIg5Lfy9h9AeeD8cOukSSeUU\n",
       "hrCPa8/4GgDVITG/9QgFbiKnR5g5C4lOsiPhLZzWy9dZcsjqENFJDLOvB3WfAl1q6C/1V4IZkXVx\n",
       "7SG7ZdBcPYm+0J+mtFPxTh1qPn1i8XIjpPVeCmVwPXhG6XcaGSYebGbQJkGVel+3HTye9n3eA4OX\n",
       "OqoPvAv5/HAnqRgflhTIbGF979qagh60pzhmxToXuaZUrd6qG2OzWtxwn173okKzLlBUWXgZPAIf\n",
       "3Jg61PQTJ1u3BGKjRqaXap2G08P7F9blLFUvzSzYirHDnub4tdejpObAwDFjmGX8H4BYDTcq5/fq\n",
       "+M7KIhlppIJh7QQrcDZfiUdg2sE/OkThGHnYnW42/UyqUEMVndWgptp8Cg/uITnJ6HSfHlBkiOXe\n",
       "P7INK5g7ISgP6Si+t66IxUWT3umBTTmGpfPnHHd9fZX/d+Cd9L7xrLEc7gg47f/GIXwb9p30Brcr\n",
       "TFw5uX8p01r/TktktXW+PoAHmtApTzjwkv8aky+P/6GsNGWg1+j7EEp6c8IF80uP5YTJS426kKDm\n",
       "cZ8hXf9W+88bomzPx1qUiR5kYAFq0eUtyTdX2SrcvM8OPYnk032Q1HnfktQqYdtGqVd22NWU9f2b\n",
       "lj7IcjOSsnm1AzOIDB658U5gPvw8mqcKqJM2vqTyFl7CfezGYaLRRydzftQXxTmWRe62xPNeEcLE\n",
       "2ZijDI/XNIa0qJ67Pd4tGpSHMJYRMaCuXxTLVeqzu5ixPggoF1DSdPJFIF/1PGEwcsVh2zZqNUpH\n",
       "ZVDPE6l/2dnsk3D2OlcZaI60calnx9QLefwvJug3P9xFOuHJSNp1yxSqR5RQm4z35HKfdwRb5eLT\n",
       "+iiXncNGhPYLzfHQC4+BR7lkp+N2W0BYm0hp6dWfZf3Wy/DrwYW9hDgh+H+fIFOH6Xgyj/0eSfY7\n",
       "LOf8e3p1PEqbYgPz9aZcR3nu88O5Df8G+olNnNKdw/wVn5e1K4gQ1yDugzmdrKmMmnp69z6eztC4\n",
       "u5Yg1zYA6e8PgpvwJWisNZhYOkmRa+4C8A9cPSqCXLYKZaTzePWWEsZ/QqrOqWGnHSKyzsDIDF7b\n",
       "Yu0YerJOs+OCtJ24PhsBfOq1JKTjXXyfotl04x5GdJdIzySws519cBx9A6bNNEByvhmRKvg6+Rdo\n",
       "mRiGe0+D9/RUMgqktUlFh7TshMl0+epJIJpd7Jew0wJzxrggiEcHpqVQmg+qZ0Rg3cbGgsYpPOKN\n",
       "ZVzM1nLEViP2/WXrsUTwZvgsKm1jAiMLl5heCjOP9esibe9VXaWWr4Dkq0rMcok8JRBqFyNkzBkq\n",
       "FAkrA3E7ncp64TKwx4J2ZFzV8ZwPr3B8VwHHNiNlRXi2P+lz6fATD/CRMEQIOWh8W3pwEuLfBqOq\n",
       "loHVjKvQ+tnEcvzKM9WKmilJMiZ5t4BfVIZ3wmAbgxQv06odyzWGgq5iD6t+MUF7npZqao0EDR7l\n",
       "b3AMcQXid/pR1pjbjxiAEbwyVllDvh3/85w5D5yDHvKZvmSwFVZHHYAs702sV+jrS5pgrG4ihcyL\n",
       "k+8YTMXjFieOb9SieVw6xtNOL/VEN/BBenjigkzw8AWV2pbLbjBlWF5yfurjHISs6XvpRl0hO9cz\n",
       "S7R+XUxP6nI9BkXhNszXLS89tYVTNalNppZP3Ks9/ohFxPFD+qgD8bjd6Iik8xniV7/Uz9h30cUJ\n",
       "kBVOUBVb8J1NfmbBvaK1XwqLCfsFrEBCVc292kXM7jumtMthTpEA3jN47tTQvo/0qakj0ps1t5G4\n",
       "5Zvd5VqyLfxjknUshucugx0RH9hOc3dLiJEhlt1/zs7H93DeC0FYZ5VFJYrNExzWD6lgAogitorM\n",
       "8Jl1WZq31pTbTrmamjt8h5Fz+Y5yo0Rzaa+R7shMwG0CyvLw6pZTvb9OZNpPTPuaxCSfJ/HiG5aW\n",
       "fP0QABqRR5FZvvbr0UZ40uVmuuvruXCFH2l/e0eh9beHmemkW5QCnQuuj9d3uOch2KYj8Ki2QRMo\n",
       "UUdGPmXXscEJS1qgplhP+f+S7B7JhlLAzA/RMarN6f/x9j+XRMeOvu6N9o5yeLvu4U3EYoD7m7di\n",
       "niEg+djA0Pa5hYQSIP57eb5za8blHMXPqd8LK6LrB01MsL93pyhgOy6Nj4WWIadpPlUTrDcAgOFF\n",
       "L1PE2vp87P3lz2rMMkgvGvoko9E+jVbARajKMNOPgQz6OrqWW/kUwuLSF8Y+4Xzs5WHPyph8V9fJ\n",
       "3t8GLm+qLy3oztmxK0e/xyDcOq5vCIVze4Lx1BXT+ZY/PfbDPgkkM1OGFkdQd9xAAR8BLHRHnoKG\n",
       "JeTCyIP5PcsHqwDAWsUVLz5Q2m3E+dXBYufyDYLYtz8+wjavEDIPxXDtxTQL4VtE3ung6Qe05vQn\n",
       "hZXaR3IEfR4bT0CeRW+rTSLNYqcom/ITk0hSaTsvRpDqdYIBUldB2h1Owo0fxZji0UDYGHZ8pjaW\n",
       "TDLUVBrUwyy3yvmBDHE+nqAwcQB0bhAzDfm6zkXTw3jvp4CAUEIH09ZlgAC/VUXxI+6fQcuKN5A9\n",
       "3J6rPHiDKcNZyJ4WqreCbDdtgbzpbi5R7uzHTNxRjn0NtbXTrwcsZnEZTYicEkBcdzy6HufKMEHw\n",
       "eEakZO7G3eUEwby2tb8tm3wTp9NwsQtKZsEXEiZ2KmjV5D+5uTre+M/uY6AgZS2rP0vtxodO9mnS\n",
       "ma2zC7UiajlqpdRHYzJx/Bx9hRU4gvBoMBeMKhtKnr0jZ6XWPw0M8qQQgFmNNxCbV2baN2lx28n4\n",
       "annp91c7VyUxtmPpWC7/mWqrNG35kv0WK5eyhjtJviHj6u5eV6vBPQF2nZ+6oPlCFKaXWrsd2efS\n",
       "AAtkAaqC+UScm+Hg/iUxNd8Ap2hjMl/KBtY5IN+Uh0cfMaiSKYymmUojr/b+iPsJVaLCFED/Ht1b\n",
       "7VTOw7a2GuY+S9LlZC1W25Ou7lWNsKg3t1KVVvO25Pn8ihE7y1+A9+zzcBr23fzojCCGp0ImDNBD\n",
       "g0LaggAreDIiVcz2bruhzXCTDApxVp4jWBPbYgPln3ZLGVrgQro28CbfxQ9qhXb96pCsaW156kKO\n",
       "H0smv8jLk7Rw0RfnyQnE1+Np0Y+HI2urEaQIwo12wL36DwimFVOVcRWDMdK+FTYOc65QB0cm5BJd\n",
       "KPk9uEfrhCNLwXCNIQpfGTT08G3lp8Hry0HOUJwBhfsCIBIMIBlyabKVv3m5+7oZBfBXkpJFWimq\n",
       "8CRMu8v0yg2m54i8FwyzclZNKrA5N2iR56s8BLtwZ8r4dD8YzG+k2M9KNGRLWhjwT5kNrxM0wc8i\n",
       "io+2VWs4aQB5ark/U4MFADBUGX5/06/lLIfWfLrb/k4ZVOG7s4yxd9sP1CsR5fGSw2VJl37KxHzs\n",
       "t3ds7bqh1MHXQVb3BKOVoFbsc1K4a4+mQeBLs9rvaK41roN7QrPp8NDsiI/sAfI+6NU4hYvgqFC6\n",
       "GAmP4y406xZt9x9YsK2JaX9rcfEBCO4pWYkzEG+V+tiW2T+vyeFEVUTADghPcgDXD91OQ/NJS6T2\n",
       "CRGeYht578fzjaYHR61lcOhIvvUakQHzfqZ73MskiENn6aGECL48N64ZA3NyciYzwe2bTaxh7mDh\n",
       "GTKRZShEce03rVP+UaF9iPiQljWisCxXBtTawoi6LJYvecn3EmwYgMVN8OV7JTuaOucRokyyaTpU\n",
       "R1zzzlArfih3uKXMfcaYLsDDuY3moPx0RHyP7Cd7hRcZIbELEEKZ2fhete+xDyR//ar0k/w5SPIP\n",
       "PWc1EuX+GIGS17Lh2MlS8apDFX4u4kd3NTPNZF+rvSenbDZd27gg3XTCvYVo9sDyLSaFt6jEDMZh\n",
       "gMzptv8TVCQNAm5rivI+Lha1828OrkxMBKfCVmm7EFvRdQ59/Jp5k6bMr49YJ++Nv6+lXRaHLEm8\n",
       "y+xRyJgPk6TONXqNmZHl0WtLdD4Wm/DHxXBzztq1jG5kr9WvW/thtQP62alQiTeetTGNhpCTSnPw\n",
       "jDh3VxIuoQgkgGR4p4CoY7jgGcbIRUBCyCpZ/PaEYfJNrtH6tBVtTt8d5ZV5nyWRqOI2rwk2BXYj\n",
       "eA0t/PpNIAKidEMrmswEtBiyTNqaBqEXBROPrnJBoVWz0PT9QZN0ZMcEOjfNm3S7ZZf6j2AiJLAy\n",
       "YNSTb2D1SETkOK4xPKc/ynMVePui/OxxLij6bBKiGXzTd+XwgkTMJdWq4GlUURd/wn+HjGiWxM/2\n",
       "R0NUm45+XI0uqwjjmnhEeBuEjgt1mj8Dw0TRi604RvcHPdLnKRDkYaCMCZAz/Vi4+wxiFiNJpClB\n",
       "gXc4OPX1gERN/ud4AjL7ssescsevlfD5Ij4zBUZwisJTwmpL11H6g5egHx9GSgFdNPCMHiesn7mY\n",
       "sBKJMhfxHl10avIF+caMHYQzyIgBDwxRlaMcqDRINIGQMpooCAAAI68BnkF5BH8AACaZihR4ZcuB\n",
       "1Fu9jaWxqQQAGqevo0dRwVXKHf1vMIp/t9KHzl3dbPuJLvnQAONbec39dR6iXqqfN/XjwJ+dJMKO\n",
       "/1Lgz66D32iV9c/FnG8EST7H6TlNclYAlysTxTYARx9y2dHhUjukWllOL/5RDohqUxAHzbfvsq79\n",
       "eaH2r9gnJIyjBchTT9ID9BSi9VtA6BrUdqE9OupMHL+O712f2HBvgY3M/k57x/ZBpuyNo41Jr19a\n",
       "UkpqIBIzDryomLO9l13S9OofmWf5fKWE1WAEzoamNbP/Nzot6yLKjd9VdSlubvM4AlOUibAqdK4F\n",
       "6palMLwCYB8+cT9/Sc3kgdXjB746mD0IRbw16+va5r3s0kIBth33LlsZ+LuKivUvpIEcLtkILUdL\n",
       "zuXj2Xr4xI1DtmGSLuI7mZEvNUg8ckvny4r9577hJx73hIXw/FocF+XGFPJjhpV9mgVQWddiPJSQ\n",
       "caNQNGJsmZOmHAI9PLziRn7Cjksu6xapYxJUcSotOOoHAL2HUVNXBB+XX3yk7xlSsNZ64eVL96ja\n",
       "xXIHrVWjpi99g4CTush07i8KIDlv0z3e3eRr33JJ0B1e+dckkg1Vzs0zD6MLBoKR7ZrbxA8HEEtr\n",
       "0BWMtx3h3txARtV5bA+4UAf8GiGPCJSdS2y6UxH461rntvt3QE+GwUzZO0d9zCcTe89pzyjdeVTD\n",
       "MgahrHsjdiOQH2AZ+G3kc3PnvyhrJlPvfukeOCsXPU9uV+nHBDW7CiTwB2Z/Cxvp2ADBsnOAiIxA\n",
       "XvBF/scHZz+XnPCBWJQeN5tyWI9C7XSDSR1opHiwno2DmDIJh2BrhEYo5j/sGG7153WRKC/V6wPg\n",
       "726DBpHZeJvYo1uBgFs9ZiptS7JJdzl02+HGFRh72TD6i/WPIaFFH99f70WzPQqcTnZesw1Z7OCG\n",
       "iHl18CVBXKixwfvKLOjxDKahO2gNocvFUJFoAN9hgw5ZABpiQCIPhmtlPzjS3TziYCnl0WqPMPhZ\n",
       "1anSEYQEaPhw50eODKvwL1JEdE1F0vS8pSw8FI6l0Kke+sQfK7hs/BXsoT7mpgvlZnoMTu/xuxg/\n",
       "oxXdjehFekrKNIlwCvJ6yv04JwSR8UZ/cSvK07Ok1gKsA9tBwU9w9JsRTyMtp5lAcJq/wSGBdsav\n",
       "mOPbuK2A/RCa3G0GpOsLGo2Mi7ZKolsC3nrSl5lVfxdhCpieZt0WfZEVd39zOAyLX/NGTDWEMsVE\n",
       "s2/Iz2T7G9gFVtLHWddWQoQ+yjLYlE4dKj7h0YTxsMKPesDkF4BxPoQ1oUtnxKeOgTfFl+kyp1Zs\n",
       "UqbXVyR1up5Ku4E59PnfSNdoWV4L5QJs0SOe1Lf/qO7DxLBLCnaImRRxJi/2eNNdkrcVYfpahM/H\n",
       "HhlW++jbEF9Hu1f/t8PFgvd+4PbD0TgQ41oC3qjNXjK9YGpuq2ogTesOEPC7VswSQZGzUOtVD6ad\n",
       "Bi22jFe1an/QNZMmyaQ5XfDUQPgIOLHi5hstj7Uyut06JHX1DuwG+xT1e8wMfBJw40PXJpvbwV9B\n",
       "Wu62q9VnFxh2EZCv/3ymndpWCxxzQ8EzyKbXP/1E6l1WmqcwXyU6ZRF5QNY5ST9pqeROmF0yTlQB\n",
       "fPx48XMYK2QdBnSx4ueu9hcoBf+sLTtTSllWiin8CLxX7q/6GzQM/nuzzIRE1Ojhrpkm6+Ndxt3v\n",
       "Psb2RuIQUqXeG2jDylusjvEfgykOwr7XmlCgVxyzeVvvV35NGxJ6ivI2SS+xXuIYfZolBRcscm7K\n",
       "7hQmUmVR527/p9jA4MCH7VfhCaqnbNIImJ9I5MuQRkLmHUlhRiyWrsCT4FM4fOb4Y0UZQ51T1Apm\n",
       "NQpOieCrA+QsHOuO0QgyxL7jhXvLp4THTjPv63IxN7ovFDumRMM93Ypu5C7zQExt3zB7Q8QaHyG0\n",
       "XX0mV5Pj3Yy/XiA4WOi2OeZsgKxiActYJF+DQKfB0Qq1TDS0OkXQ6B6HdKkLFBTjRmLt6TauwBc3\n",
       "CoXsGcuPojQ+1SCS+WMPh+0pVnOZ5GS4sTgpZ1zyQd1rStCDYfzLaG1Sgp2avmwA4jQqAIbxyZob\n",
       "FugjH6QHMO+3K2bT7fgCdp8s/j9fXYs+AVUakdDdewlS8CSSSISle9FD3Fy7tP77JwhfW8pMATOQ\n",
       "GlVT4G6UNgozfHCNcP9jqADR7C8NB/ZTm6Q/2VL61QODE6hmgs7rMcyA2VP1ef4r1P8DZdAyvG++\n",
       "mFpmnd0I/6Qb9qTsBI5/tB6rlWlS0lwRq/vU5hTuo5h0mGRQRb6vlWSdl4VXZyUXDrAZ0kq6E5xj\n",
       "lkhBwBrtPsYzVVoAiBEtKzzv4DTFKGjqHpxUOxjEzPLX6Skr5DgSe1jqWcG6PFLbXL3Z5sJdnX40\n",
       "w7HJ8raityJDKFzlUarkOwVZXApFXcd5SnGY1aoZ8QPOndC/Hi1v1HZcL3ROASNVs0CRHHG4rrBq\n",
       "fmoYA41iGKzP712dUoOb13Bp3xwWMDQYaD0e4QzmdNNlMIscGN0E+zmnj4C5XAIoX7wP7oVdS25m\n",
       "5RmBrAPlVFIe4PuHo6vsceztCDrToJEU07nhejFD+aMBNqktkA2BsAKz81b5FDD+81UNFJuS2MHR\n",
       "QIzPu+7Jmj3On4y+pNxqbyqHsdABELhRf0To39G1CfeKUrvN7LuckF1dYI7qVnZooSJZ25qapGz/\n",
       "1JIJX+tSEcpB9Cs3rvCElcEtEh3PLRDKvu5mbe6VmkdOjo0frgFIOC81UV/earrxOoaSFdKbzF9g\n",
       "7JLSaiwPNeDIJy9+9yRIcWvdh7RkN/+8S1KEvrU1yGqwAEv6xgxPOJBVGwrZ9FGsqFXziEQHOggA\n",
       "H6/bj5AMdvrdNRVhHZ1byNJRxiYn8mlb4yIrhEV7z5fat6wkbURQDpqZCG8ERV6AtnRMuXQNJ5Gm\n",
       "RITRNfbRVJIkRqAyMwjXKiIehsntFmnK+6Q1jMaCANd8neNDZGAe5Tkm6Wq9pJvs+4pDu3Tg4VLy\n",
       "qoq4E/qhhsRWJIHh5iel46I+Yf0paAEOtqcmNsfkTibRZ9+HttxtDL4wTw/q5a7DF24+VOZ3nd0w\n",
       "Tihb+8BFnMERveDCj++ZTC/xHj91o+RoseYxRFo1BTnlwEHCTVDSLCClfZICz+vdX/HRDQh6I9fZ\n",
       "XwD/4r6tte80tafO2OFepzvCickY8MMgXCWQm+cyMZofxhUO/wAdIvOI10esBN7DuNzdRhHeAkzP\n",
       "jMhj5pAspAbnIJu8Uzv+qe52i4BwDAwDGzbMWbSi6Ia5jUGG6QJDK4wTUZInuZELo24kksWVwGQp\n",
       "FnxvORXylV6vlnh+yu2WQkwGxHhy0l/J1mnjMvkhjkS6ewgZAoyN3kQ2CeBS+10hzTUa1deJvi3B\n",
       "/uQW5NRYUNAcjg7Og/XtJ1htO0InObMCgx5+yp34ByNlGvJET4fC6omuvLh0Lq+dwqrAAiiWauuI\n",
       "Fjd2jUd+aOuzR6Hymrkk5y2uKdKfyDsGnxcXo03VQkChRqRhXZdqfZHiMG8tDwVZa81CyvKyqDuC\n",
       "0a4YwKI0GYxvYI/IGdqGWmGvATFPsZtQAE6TzA9CV+Zv3JbqHeFNDN5qONnPuXNXAVCVvMh3oqtG\n",
       "RvHd3qG50ydbQbIIsEGHV+8FH9v8vt+QXWtqLSBCh0V/vp9Dn123LDiZsoncYBSAqjRLjzmALT/r\n",
       "3mkgSfn7yMiRqEsEDvp/w/97CbrBSGdL38V2K/WrlkEo0+Vakw+GSHkil0vWKmWGuGxbVOKRDLTM\n",
       "dwcGuIh3P4zCkZQweF/NHn2xiIg8ZMvDsrcvMQc50U46h4pAHkjx5TNcXhqI184DQuOPMshRC8St\n",
       "XcWnSK8UTJjiL0leEi/aMKdAY17+Vl01wuN6fQ94+vu4CMFE9QDQb5v1mjx+VWTrtthc8aC62GSS\n",
       "pg+hnrQKsMga5ppCoCfnZqvaArFlvw11qXn2GlsfmxN6Ab7nJiSHcXjUaz34562mRoIPPX/1S5l8\n",
       "9AmMC0yyI3nFpzKNAabIaYa2bqF78KEWj+1ZW4zx4aNLBz1ypsYvhYmsrohviw0YUXw5Pr7UEviO\n",
       "jH6A0yASm2IDL4vXBB/lBoLpwTTCTUmme1fc1Kyg3+Ab7+Ybz6KV+xqeg25laUBMCpu/orST94Nq\n",
       "3gAmcfFl3N5FVANgKIZf34cIMsyB2iopF4wL4wYPOIgyrqGDskRAXOtxSgC10y1FsNx5GYtxXVV0\n",
       "80CIgj6q3IBR8CA/c3+HgFWuPSvdwH5Rdcv9CF1ucQ0nRFm4hrdlyERqbr+nTivpbBR7D6reUmDg\n",
       "dYzZnMgrdggxCcA4DYUrpRe8rvlw71amdIlgTkJZ8HoBnJkunczYjfrlBRGzuwLgl7Dgxt1HZ1fU\n",
       "gmvKd54jt5ip2oUkBJ254YJA6qhACLKj5029hjom/5jf8ztTNdel2ARntl/vdJAKD8sRSIRyhppn\n",
       "Xt7DOSERF7M6KvgrVQaWGZ5AcHiJbv7kSd6OELUTOdLyDzZM0PmddqvCnMbs5Vkq5C/i2GKK36ni\n",
       "PJXWoIFkKcobgGPaNGcxy13J5BWqhXeD8+IzRVFa6zFhMrR7XIWbSi/nfkS/j4OopuI4UGijw+eK\n",
       "jPVc60+uD0IqTFtX2dd/YcdLIsddcrb25IKTclyzMKG8eJ2yf+uFW2aTYn2hPlBjiAqrKiaKO/Ht\n",
       "wlnXA6H2wVObtFn711taRS0LJAFctqBKW9ApiJeCNl7KxSMCO96km72W7ccuOxa5/vOCiX0WZ/nQ\n",
       "rG65NyQW6s7U1ghxU+7FLQe0KXhvN4YDKngzL+1nSP3He40bqyqM4QY2sA5XvuJ2WWEYV4A1j6T9\n",
       "ZkyZ7SZt//Nqe6lRFJqC5bUim5UsyW9qJcoJYPNDEq1GC65ByvMYBw8VXUhCWkl2vK4yGKfUB+JF\n",
       "RfbZtLT1Sui99UQGRMHwpCiQ+4YozE4nH1Hr42zvx/EAPv0n4b3urko3YPY+zF4JV5jkKBR2jcIb\n",
       "tuefPMUmILG1HMuvypxlB3hWsJGv4Gf5n3tQevPNjigUTRqQAbfRfECOvKIWpvl5xhRFnJZRvTmX\n",
       "8zbwGCunGZpK/opDsWvyt9i6TKnPUiL0tfjKQ0sO1hZQnosjlSyAJjs9j6fZO9e+a0cZ7iTyorRL\n",
       "ovxPsx5rLbU3nhb3E9adrzIbXx4Kkti7VIJ+P2E47xh46lReSe50XDJwcJ+8NHKa+Edr0p/4oBER\n",
       "2nyTvka/55FtWdP2Up4FYOJOdfWHhyRReEBG9yuGjT0XSZDMUwHc9fyx7OU10cfOdFB01SlTw+Li\n",
       "bczrY3sHoXGk1dc8Jl1mLA19QvCbVlrPKQNehl/kWEe9vfPqK0rWikQCeTcjEvnGXblFQgVgrrCS\n",
       "UZWsyY7PsbZt/tt7Q0e3Ch9wSQr3rhh1V8vOG2HvTmn52GzeSJei9UyLWgX6UKl4i+aZ1DINExMY\n",
       "vMur59rjDb96q3cLaZ0wDicJYJAyIxj+9AbGmWZIAuRwD9OOfgOj2Y4ddGb7PnJx4bypqJxJufem\n",
       "q4iUKiv3ucyqDtckVUCZTg7c3KaBI7DtPSNIVvetxwVjPraKZA1oLmwdiGhUzOgg2+a4VH+jWkcf\n",
       "GpVvbKGN+oyGlu9mfOP4Gm4yMZ/R/y2Qlvg2HBCgmU8h7QZxgljG2IKnacSYlzkfL/+ggwO7dInR\n",
       "cik0Sf3tCU+VrqktXvJOHo4e/rSpew45AmR2+CGCgBMUJaJ2RH6Lfi38e21FdYGRp8HFfEFAQsm/\n",
       "+38FHVfxFjEGuPTiDQA1y7QDajou3kW1pj7ufLgyG3/OW7iWBjI55976JoURT6E2qJ+q/pGCsqRP\n",
       "GxbP1rFrn1/HtY2yWEJ521PMkwgGDFVxwHxHUdnfstgNOnRu8OjGlucQWHswppRJcEhTtyWekoTy\n",
       "Qc41ynjideaoWZsV8gH/aJ4TfF8to2NHsNlCBepWabCgLMKMCt7x012GkDuIUfPUJP27X18Ij5DY\n",
       "sucICOQx/dner0ziRceCfE2vbBmRF7PdO0tbPtLaTcW/9SpK0LcxDeUZKRR1h8hClJxUP8M12bUc\n",
       "sb/JQ93aHaLsTb3r6rfxrvx9ZtB8A1sN7kN9LEy7AQ1/RSL8496poSapnGpG0Tp0VSAG4OA8vvn/\n",
       "wkkvqQQoNzXz47WEzIqf9APU6r0cjeH8ve/lX5q1lK3bLVC6ViXqr5I6r1d0JM72TY+XXyCz1bPV\n",
       "CCnorWvv7r5fbylMvQ+GN9zmRnCIzSCRj8SGWmnvwIv2fGU+NCtyI0mJyhdsos4uG7DNaH24QE3S\n",
       "xdML2ruQ20lkTOf6BhDRq2sETwZK0rJX+U6HTLNCNmJmdCOTH9Z+XC0HX+5zb9FCrH5Iy3p+k9+c\n",
       "Fq4NZFtUuYLdGuV70NdcQ8iaf5wGJFehn6/Zh2p3fZDzTPZkjB4B5wK9yytn1A5fO5rURItvBga8\n",
       "pOHkhwTm4bF2gfoxUqsoTlTkTZMtTrgCubIEi+h1MK0Fe4C0CEzbGAuUXXbe/j/8x4284YeRR2Rq\n",
       "s3aC/1TI2hjjIV6Pr6bF1DmWf/GbQ4MbtdWGbp9HH3CsArFUp5miKihfV140tba6f9DnK9l3C9O3\n",
       "i6rnNgMvn2+1kY9tAM2SfvLthYt7V6UKkn/F+qwm8Hdz3lFuOHCvKe1opyxRRFDNpK5MP39qYcQz\n",
       "UGVwE2buUC/96g/Muz75RpctUv6564To6Pwn1y05Wef+hmi1pK9uAnPKBb5x7kLe9ihesQdGnWzi\n",
       "rt+YRa3Y3sRytJG3Yo9b52hA4VFbZtEK5dEGiQwNiaJOgWsZxEULnTjWAdZKDF1oX9rlOMZYKFbs\n",
       "yBEmvYVAqsECDv1CjvKlns5ZJNjiBI1uvcSxDV7mktLLZEAwWobxbl7uEe3Hmj0SYDm6H9eKM6i7\n",
       "ihaYhMgQdiS67S1PNhOZVpDqhyB2a0CcKBh8gQA0Nox876oI5hXdwxn/AMemec6pAJAAaQokBIyA\n",
       "QFMpcjsiJmS/cOXIqWFbGbJ5tHjL9qUyLSyc6o9BLmo7kfp2QymLDZYmFn1iBQMKdUvAoz2E5v+E\n",
       "4PIYTf0+02/2hi+DvVUPzkig80T3lNUZYQ4VLK2aXtzbMSsnVpxvldiI5adcpN20MAR0pZpc7AMv\n",
       "lNqd+EryfcjZX1Ef6DLjt9eQC5ZqS1vKxjxYCFRVW5wzhJYAv7c+pMHOj/URI+55vx7zSaVX4prB\n",
       "juD4RuJqHGOmF7+a2RxXSZm+j7atID97p9PbsuJ0tG30MTc3up4cKCQ3+18OVKfxgti0fmnkMt/s\n",
       "ZCXK3f7f/lu9F22jU7vFiP8Cr8up9SJCDJRTbnVEsuDZAEjfWB2WALkNK3X5y+oakQ1xiB5riPS3\n",
       "O+0NXVXbK2Jxoh5dmi+//o1r33jIRUq0SQGnUarA8Rlqnfc0e4gi1WH4QMQALz/4isAtcuXUg/kb\n",
       "56mYPevZcHDb9ZOjFNot7d+1eDDdnAmKVz7n8qr846xChKTpQlxgSKTkG4+czYjhlLtnALOixz+C\n",
       "IXWK4BOa5XHQfB7ToQi22VzKG0K38zfoGs5WLKBnZLrrSMsHAQXuTp6BSzWL2qn05V6ErPwpGVpY\n",
       "yTkG+0f/JfS58WDHYwVcc6r8+1hT7vbe5HG9fpSW8KYBohmvLUHXlAIZTEfcdOJbjzwFRl7/KaOg\n",
       "+xsBOLZVNxNA1ukIQkqE142moSzGoV1eKcGydTQ9Z5fGfmpiYzQjNvx+HgFu3Ai8thQg8gCTY0sY\n",
       "8KKp0ch4W+69wUGkZLCKNbFQ/S/73CivNFMILUSSul3FVY3HBCzrOnhyQ/+zI65TP6giH0GZuMNJ\n",
       "jIiK2f/z2nxkvZXslQTl0JaLdfRYmNfCbnYNMlWRun8KZyIJUHQnek0ICgmbh5XrtiJfLO46MYl1\n",
       "7AThfV8MJzplbBte3+hpClqSbQWOycjc17L5TrTAwZSPV7eCIH6A48nbmVoWsDUBPp1SfzGdIdz/\n",
       "nsX2lRHvtngW8gb2QvEIR8CO9GsauICHuu4uemX0yghuWoFV9aKfLmvj86rI2y9/ANmmSodcrkKQ\n",
       "OviAxju4c3PIhqb4/JRWhpMjhZuYOOe15X1mNAKzBSDuJx9yrDC73N41RDbit49Shf5ZZNJfpHTI\n",
       "gsi2+THJGLwMBDh/wFt6eX6pvFfuKxd8B7yUv1y/4XpvH3DwHX5mGF4INwVaejAPp/UyfKHxQBPH\n",
       "D33ZoPXMJbie7H+OdQjMB2cNXMRzPZJiDmCjf5jdqN2VytilhfpcvmktFvqGL6hns1Y2iE1EQDHd\n",
       "IH+g8WNcQx8X+oFZMfhb7rm8yIEBP+NUSpTK+MQl22777LBAxemuV5CKmx+n4NwF+RAfw67b2mLT\n",
       "5hhvsFdcWEd4X/pJ56QRAGFjshZE922+8cmYK5z/SFOZKQu9z8uiMAsMBqt7SFeiLZFYOTw84+Vr\n",
       "s5d96732EdrSNYy2pP59O1euO7DZFoAEbdKTQR6IQ7x/QQ9V0opzf+BlMigz/+IqT2NiiqgRwxZv\n",
       "qV1HQ2D6NDjgWtmf7ZVzAhtCV2P9ENXheI4PtzBKmOPVMo2wlWICozw9fJ9FlwavLGKtDzKcKvwD\n",
       "DfuhIU2/WJFJmgafQ6NTlBVfnzmbzIusL1vHFFzthrQux9AxSdLH3XTs9la3+6l+KYrA2Cg3Rnme\n",
       "+ibdi4huAg3d8ews/53wqC2bCbij5fAH/AcHyDVtcJRR+bPyzSBPbY60npD+DbDcxafqHBb4bbik\n",
       "81WZk9VM6lKNnvaZA8DeYUQZ+PIYtytVGVU+Zlj4GmBqyyIKQecKfcLqv6GICUgVvfNuTWMU9f2E\n",
       "8fidzh7mDs5o9dX7FCFVMuF50Vaxw2UnkqQTMaRiwLY5uEoCG6GRwAXKrKFA4vfLvrN2GsQITyOx\n",
       "sq139TSbpB8ndND3hpj73hRCGE1L3I+2v1UqEsMyLy+lba+SvSx3IVgn3EC1pDPdwrqEjs8c7beJ\n",
       "15wXuntNqJuigPHRFYdoNz0k5jzhwJwtM4BKTuU8+9vJRIspuVHF0fDxLQ8g5CYHDCnRqE8Ow5pR\n",
       "Pq/dvPJBEykeC+I1dG6/OBYI0Ca05vxBAALPUg/DuTKtFzK61mmcTsgqoUrv6GYGt8K12s+KtbXO\n",
       "Bj9FdSbLyBkn1EuRJ0OX0deijzThtyhpSfbdRZ/3E+w6LIU1OdAdaoJ930i24ItoROFq+UfmVNEG\n",
       "0Z53zCNoxgQBsSZgOp3zCzAu05OfZQZEEkqxhl20O3bUwI9kzOERrFkmDbsgIyiNz4bw9GPo460O\n",
       "qhshHqwTz4w/DIufJc9/7qthgpB3pdbhIbd/H6njvE4g5JEYYFt8MmmGAmKd0iMw4sHjuM6fFGXk\n",
       "ZmUmVKKJKd+1pDZrDSTm6dFxSsQHT2dEVc9sdZAig3yraW930BZJghDeXF4EYm0/MwkleG7Hqwpr\n",
       "Zs54So9KkxTg/heC4ixwCYdBtbJw3+x8bwomMI+pj5S9eWvFLxqWwFrterGfYEiSi/aaWsI7kMH1\n",
       "6oajJQZ72jbW5lazNilQ3oBAAOdBORNJQ6BtQNDgIc35eG9GKK+lSCh9v0X5ohu8QFaajQNdnHSl\n",
       "NoliF2Ydv4E0JlJZoJL2vENBX9qTKV24rEod0/78Gq8fU7Wop2NCCExZX/auzrGyC8IG66CEMubj\n",
       "zzazGuXrgjSCUrZ1CyEx2z2lejoaDw4B0ngE/WJdGfQ0zEAOsSHYw7V1vyqWDxH+RgECYaDTbO8J\n",
       "GZUxXPjWcX4e1HyvI8grxgjdAB5LdTsvo/stIwiK5aPDM4lLVxB19tkkh8cb2LMwwDYz2T/B6zBY\n",
       "LeA3UiC3MI9oZp7HKAFl4mfz/Qr0hWTgVLbhWFrsVteOLyUKVXGQVsvKzSvo3y7+kZ9AtWQNiGo/\n",
       "4ViyztLV+OkPOxz7lsbN0g3pWidAB18q75pBG+fj9jPvF+/Sy9+SZRYkmGwsbB/46xzLExylAwun\n",
       "HXkuKJdHAqyqc9959soXq/W2s/aWtQ0gI1dwKPFE0JVnp1+PwYxxH8RdyZv/z7l2N8HJjwIFWUyr\n",
       "AXrdp2rXyRf3xn9oq1z6FunSF4hsJXuV8p9KyQMnuPbuA3XMEpaKxdw+NQzxIuagCeNnphehlMFD\n",
       "ca0ehl2HZrb1Kpx1nqpY65Z2JIkAcdTuUIfp/CxBHpS5i1phC7uxqckDguYmnqtI1hJ5f1+pQYDB\n",
       "2QOmfNwyCFrDL4wV7M35kLUOresrRZyNTmCib0Sl6qFO1UqyL8UHvZf4JjxGZ3OF+E57g1ftni0t\n",
       "T+PFB3B1WQQbS6t53vAid5tfyB1Rq0DUtxxjnVugOjRB6KcL0cNhKj5OH6/EZNd/nBtQWuxGNWFo\n",
       "bvyfNiE0jCKfXJsA3FlMWSFwTaKTHM+hEdKh6hIT1IiKj14jfupcSDU1/czvev5Y8mqZQ/ndpZIo\n",
       "jyUi13JioIhyEWRpHskvUPCmdpaxyNN90SGLaEsE2GL5irvtdk4Pn8f9khfTMonOHlK2NCIzhIsu\n",
       "1yZHNejaOVjvLrH5BxL9vIPXbU//jEOmY7AiPFfgjKRZ5QafQ3MGsHyh7AjlbH8wQ4B4Z+xT1bND\n",
       "XtLiXLvEo5dGCF2Wsivr+xxJH2Hey4/j1D7EVM5FCz+qygnGtIOd5iy6nDSwtkoPh1BJP6TAjmUT\n",
       "jshAzX8ajUmW7ov/TDc9ZYHcQ/ZgrzeF9sk43y0zmLcOM6Mhaa6XgCYv7mZcgbzoyJb34VMZeouQ\n",
       "WWhbnplD7vWXQSALQelXCCvKwgxO9+aIPYHhLpWcp0WhIRpCsPJT88EWmPK6paAngy993HfT8woe\n",
       "MfRZz6QItchs8t5lgNTalMk1affpOdNJ9qTokh/89hkAj+Oc63aWEjMjtVQ5kGrLOJ08ld40QPZJ\n",
       "ruh3MX+hnrwDs2h4LqirL7U1dUQ81SlOl1f4qofkX2+yWpogqhnXh7KohET2Pp/agGsGN6bpnged\n",
       "Puci8qIencHa/XJ4KMievj9KL5rkiYk0fnFatMblHBknbcuqOVAgxv1oW8M6lj3J4sgHUJhJrUxH\n",
       "4WrbHLYCFjpTB8fyRGmpwgWCJe6f7BiBiC6ABs+3vlTL0hbJSVPGHFTI7vgnkzJ5EL1nB1cK+UAp\n",
       "5BvmPmY0jaFxWx7ywdWk1Kg9k+qLy7seTjGM/24nIPAbC7zCFTraMtBe+RdVdXNrAn2eokbrmCwW\n",
       "q/7MjW9twvKXgRdrg2EuAnCI5hQOcyPp8/GX9WDy3OuDNF1eIadAUp6OgH91wndGi2GhnluWcD2d\n",
       "lcaXxyr0t4P/XwiU/323pUEjysXtoK+J6aylzF2oN2R4KlFvkOA+zEDeShpV5gN5RewnPq8f8mK0\n",
       "AtDJR2hlyo1/KkiwyyzzNly7QZjYaJs6cM5Te2KusG3TLnf6y68Lyf4gMMuHohzMOZJ7Ff+3ZY1S\n",
       "FS+4XhpXs27cGxe8gi/cwXD4Uv2avXuYieKL6O5A4CUJ1HoOJ3C7HZL/vUYQco8tYxDpwmJhL+gb\n",
       "kbPRCdy9yVSghXehIcuMPdAiHTwrCjVn7ZOICvvaRDq9oUWRzDJ2+GBQVRcPlwPddnsFLicXOz4t\n",
       "qhE8fQwlmoBYaAYWirZ6bNU87bQoiOw4iAybMQp4G6XTgq8yNczf0U5Q0qQo2rEKjbOJ0RzXlXZg\n",
       "hUi+8uqPmnovnT2XAc64bEz4TwpFpothnMueOgqcYJTy0e4dCYkRiAD7hg9EoesICFHP2v8xAJC5\n",
       "Dsg4HwwE6Yd75S4hRNu1mVkVJ0bA8/qJR8SSoM0ZrXOUl/G87vEMGUidIFgkDNBRGy5hV5f5arkW\n",
       "8/bDwcJdO4FeyX/ar8fE3W1S3nadvCUQkuNwYgT5/Z1Ge9yn7irrRbRLo7X7xWcEUiAv8wrqPlQz\n",
       "Ateuo9ihp5IOX7OfRT5HQwW++1xUmWBkblJkR05S34aVjuPlfF0b3F8j6R05V7/ek2+J5g7GeaNZ\n",
       "/wwC4aydUfxGnaOPbbAW9nbmh14wAiPTs47LKb7DD1nlB4Xof0REeI5tvZPfSCqW9U2hbD1pAXHl\n",
       "HYtnY8auk94aqexiQWeWn3VQ24cYZRdsLuL4fjmQSGJ2qaLHC8HeteqzwXaFIxQYN15oK/CRhNDP\n",
       "lP5H+jCmjHxywZRAw7//VvWFuh69FSZNknpcs7BTTkyeWAPLNpc3Kg/OYuidJ33bgnkedX1aAAKf\n",
       "fXgbbAmHmAJkNKfuY9790NbTu7G/sVoIyCk2872DhJlNg7iCQdf5C2E1Kr0n1KUKlargMo4AWUEA\n",
       "ADbXQZpFPCGTKYQR//61KoAAOkXgKZMAHFUNv5wcailkcO8rR4guBB/bMP1o8DsTuZh67gHifQrM\n",
       "CG+DZkjbq/kwpOMH2LRi9YYij81zYl1TEmj2Zoa2kFP7YKhrp6HgWHHMpN5dT8TS/8mz315B7Bmh\n",
       "OA9TgVBJIHOMkrSpzck8ALN1hXqNYzU+IczrbUAqKkiRYfXvvzFkz3xvIyehb55ceXyBG+6u1uEg\n",
       "DvbToeOcBUOm3bjhrAA4A3nh5j07jFJfyLrBFAgiolY6ZJIN3d0WiZhpgg0ideqkpfY6gB56cBTm\n",
       "wtO+vD7NCRQ8QfeONmnho0XzO/tVwN7xO3Zs3fWrfLA7dTxyQjpUPUOqIwLL/ZIYbWRK/5QMku+3\n",
       "kUfnT4TtokmhrzEWMq1iBKu65Ywk3wcmGsnLodYRpZ/KK0Haz4bd3/5RJ7pmZCIyXpcrn3b15dGg\n",
       "s0RDrt3Az1LHwDc/pqkB6kC4hcmvhwFIOaZgIFou0B8NaFVvy0n+7DU4xgMlgS2HHLVJ0x2ov2d7\n",
       "+qbmqQWbHBc+G6wDT4zbkDTlwHWiCthcf9U+NuZ5XQoWk30gP/G/+BLi7eKTc32JBuMrbeml129Z\n",
       "6OdXBo6kmhg8Pts4rSA38p0rnv1Ma1VcXuLf0hwZwsHbvT5QhXTIxe5KmzTX2XWMmY/VJ+mUIuUt\n",
       "i5QAQjCXE83P2hN3yJiZvPydK+YCpJsxSyTqRNZuphEal7yeNIS4vzk3ihHMKZ2FCzXH7WhpF/Zd\n",
       "txYGltp466L1RQSPbIfck0iQyUR8IGzmp0GYI/UMbsF+byCxYWYb9zMn34mCk1aswnt6xe9tzI7m\n",
       "+adWffEyXE8Ij94Is/2Jf4PsvTr++ZTgtiU0d/IOgnjgb320cRnWXT6Pk55AC3IcxuIPjihPVq/v\n",
       "TzmKuZ40VwIQwglJmZF14hI1jyJT5DuEKTWDVVbCs4bw500cyueTxSSJ3rahEcRO/FP9T/PDqDlH\n",
       "P9L1xQCkIhChOR7Mv35Kprq2bpGUm5OFs/HjQsaVI4I/BFcOsFvf07rF4cWq8L9644n3AUWyxg1D\n",
       "FYi+5sv836uHGD2ttDPa/slxFOuc5asvYzUwy2qLQqmhYZxTT7cMELys4rAWrKJorXi3iAq4Kw4P\n",
       "WkinDi7nUp2dKnFtKwsxtjFQzBkCCEmX2n/nS2RNTj4H5kkO5yNKz7vfyDZOfjcZvDQnXkzyRTho\n",
       "DZoo6uxlKdgwauI6Q0aYOeRzd0E9aBEEOlEXdw2oEgCreYB4JzyzWJYM7KJ2SnD4g5BrB4pRk7jU\n",
       "CaKRtDe0iHJ6VVbc8PCskoQqoHrvBmw4TUU+UNsMWDd2JfSORjvQ+8seeodc2fNqHwD4P8bXgKYQ\n",
       "5z+lryBeenLzbzYC138mtQUcjauG1GjDZIiZHIThbZNRl3ZSEwGs37FsPnTw1hyv2JbPhp6OCrUF\n",
       "+M/SiU4Cae110SDVlrtSjj+Ri/lhOfP9sK2H+OqDMCXB63Bu9PwHUkFou3iebd2camtTOY+AVAus\n",
       "1erYKs5HZHcRs6UK7BpzqjumP2OVPwBxSeIrLkaJE0ns5NML1CaTuIACnHpEOHWQxVT2coFiqdMg\n",
       "bCDCI8/A20+Rb2ZblqaulBEJjdQR4UsN2Jxk276UYove9z2kg1AVLS3gv91AL6hew+JRbNT0qxPG\n",
       "6bwT89BluPGTFahonLdQ6PvDup7cvAg6bZ1R6s6JHYzykGmQLa9wu9rULw3wVyzBvBNKLvsBwHCg\n",
       "1EQBEyV0ySd9Pyzw8ETgSF2qlcqDV1iaWcOIXIoiS2PkXVd8195dz4SwwhzYZ6RNiUmOQ7ghtszS\n",
       "xH7ncvktRGLJAaOR1qOmOuEWzG9+NO9Jn/d+3SjNSjNnWUABb1iAfFmlCiYAGg2a0C0xxvHJ0uDs\n",
       "CzAg6mi4khY8T3DDKQnQC8a4Jmv3BITg14lCc81X+2vXW2yhb2da49QA1zB4cyMXQWzdCjgHxkH1\n",
       "EQ0RF/bekVp/05PXjyl1+/44FWpfZ/DcTWdmGnlrpNaNH6uhFmhPflhm9zYoolCCkgp5rJxypPJp\n",
       "enBH6HsPz+CJl4iLmMr1jEVELBjP4gXVhE24iH77yFwTTtSAXmauHj7eqpAHgfJQlE3FcjtGfZz9\n",
       "mGv6ik0PaUs7IPQZPZ6kEe70vt5V56C2F0G4VzrIsBrJCGVEKJdpTgi0faeEzbWSPqUWXkSSwzou\n",
       "NX3Lby7c9r5rZiC8Y39YQO67rICvZw1x96E10bi8fuxokQoIPhqWOmTSAqdsj8vPvmnbEmFIlo37\n",
       "YEZjEX0hAGBaOcXnNjlynedp09JesqNwU2ZbExrNGiUj/99YikAehZQYldNivDX4w9rO3oVMwg2P\n",
       "WlnP3oVOuNJJTZYQYBBD2pUAimEJKjAfKtvUUIM9rZh/HWHp/l+aIe7i8BLXJ1c9GAPv8t3bVaaa\n",
       "UPQ8ZV9mbkE9LgWBUY2AlCBCRYnIKwOS/mshHc2HMQR/nIgWLIA4OKPxlAXkkdnigcclpasMrYhB\n",
       "rf5HHKtp7F1ZEg2pwt7/1AAaDnuOzxUJr0t7VQ+5Ix1nSm9OiYqA0+qEEPTaFn5R2zRKoG27Tw0i\n",
       "LRroWCNDqzfms1kfgz5CsSE9UWIFqgkmxO6pEBijaxmtCr5kBFKzmSqa0Vxuy4LelqTEKsqTWA7W\n",
       "bmxq+kwligd60FfHm1jrsmq8jX9NtovVXPPnu9qPO1mJdehj7Rw78UD8Zjf/zq5XkBVnfDqVRLx6\n",
       "1A2dXHGD1zDhC2TWIR0L64g1yg8J6ZSOCaFfELUBhGTpI2afJFuhBpLvCIDt4JME+rOCJLx00VmX\n",
       "SESqhROv+yPK2f2U/y5QERHuBAuCV9D7Wl2hFMLrfYoOBXYyESpYt998K9EuwyKCht2R1aWm4ODC\n",
       "iFvU929SuUBE//XAkKohxlmIa2zIsutva/NxA/sDQsUlEgPL3anlkY5AcUcmRa8O1YhSdd/J+UNV\n",
       "L7YpIF3iPzOmjrCPnRDkpzJ5U5Mx8MyFSh3iif0Hlkh5z4h7pIEdez/dXas0Tv8uoas3k6Wrxs9M\n",
       "Pu+1ifVrLoUV1pMjPkrLP3kNgNh+LciqSnkv7PW51Zl8NlxIeDOOQMV+Gf5CNHNHixGeRX1ytJ92\n",
       "gIRhPiy21IXNgB8BS/dGBymJ5V6xZlpbhgmjSEFSKO69R4qsfhDEOE2kn21VrH6cXQKdRGpJzfLM\n",
       "wIh8VBXBNF7ymjFHPYzKKmLM6/w2PpNqwDIAnJAp3qnQ0AKwS0bcLv/90KiG/2L3+beN/D+g5z9s\n",
       "uFxp87zGOqaOkbY2AhB8ylzyd6okRyj80C79TSCS1FkvbyoftfcQY7bhSsO5bVhyU4y7QZRG18Z1\n",
       "GpX+dR1Bv2dsDCeutqqioXGcOxacQ8SBm9e0+zKF4HoCKAH0tbNATWCdHt0a93CjpxiMHPkP4+Ms\n",
       "XaUOlwuT1Bjh8FZgZsV2s9dlkryxH/XYz1CZiwgeTX5tnu8x9T8OMguHwSzpXOchI9BAXqHgjfFc\n",
       "fCSPlfxfON4dBOPKR9pLvkFLonPrfFuwusFxGmwsYRpzizzIlCpbUswuva6n7CK6bfg7VS9VhsVe\n",
       "KcRdeL3VkvngKTDcAd00B7t0f5tiXIImEwQFzpSHR7vsdfvObSfkSY6ec0wkhiB/0gZZIOU1fIE0\n",
       "o5UfyTkhTCm2QwiV8KuxFFT9eNVK+Yy2vh+T2sGk8TWgqr+HzzV6tS4mKL21kGbF8z2EVGgZHPEP\n",
       "1nMCGwidHc4G2mJ5CSZbL92WOCIcn6HNmaTIhU0QYJVODn6lMMnUo9GPm4hSRura0M8pYG6G5FOC\n",
       "h31QsOGe7PtkJcxd89lhTOMoyhDzqmTaBbWzpS6InluDJkh5bxVb0cpFNLqlLvW9Sbsv/VaJ312F\n",
       "/N0pVuSpslvKhNxiDWERmw3hOwvPjhA67eUhAt+bSlCokoYdev3JYfC32v9HAwEkoYkJu/8rzttg\n",
       "zvLEPVWXzm6FoUyiygj+OTJoQn1v3PyRU+Hu9bIXfEHx1GQhi7RL0U6CQJyaPJ/T1RMTUevEpTPH\n",
       "lqrm4bK5pcWTJvkD48tJyHv1V0NOz8k3ZRQ8UUz5YltyMTWRJ1rYkv3wgxHF9KKzEhQeklnkQpFn\n",
       "ShrNt/DcnJLgx2R0ODpI4cBAAtUscn+r79jJZNPJeiXfz7R6pZRlsiDyQNtbJsuqh1OMcszq9uIg\n",
       "Jl5on4S6vx0BUhqsjc3qCHDUtIYXIQ1iDpTLCROZMfG/bbDXoeuTHwO5fFtIhY1PUxBwW1tUVLz0\n",
       "kq1iAnWDQRqOJQs6AbuWaJ1gagkxf9264Sguo8KQD8vQZPn2vXrI2U8jcAcFjfYstryVAhHvvX9y\n",
       "ECJTR7LIY7v+vgiiFYDysJ8374NDwwMUapXlPEhRiWxJGsUkdX6EsOvFDKgsiaI/7QaLe4Ea/76v\n",
       "EWL8v6u9Ai+cF5VsQOcdfGVg5RHkvNRmG/9Qi+PG7REb3xf7/sO1+1Mw/E6UHYGif3n3DGaoxpQG\n",
       "4WbsfY7C6H3EDtrQ0p1L+WF3yMAIgIkL2t8GVaFb0pOp5a8l1VSNi/oyF2Axnb87kfDsyIXWr4O2\n",
       "xbeLI+JaqmT1aK5k9pjwn6oFGqeWRuTAYiC4dKcwycn4Q4zcTOguD9WTlsLr+RaLfwzd8OKeCxU1\n",
       "BoRRQGh5Na2d2FF5b2dPCAbVN54pg7FSqhlHu5FCVvjrxPSvNkYtdIo22DSFAvztRBacgcZSqrGf\n",
       "TZlsz2W1WD8sU4eXOX6GM0YCjF0hfW4aQsNs2PCijOX/jE+t7x2m1mrsZAiw3yJl7v92FiZKq6Uz\n",
       "0JDnAqcPxYKMy8MHUcZANMgWuNSSHoEOyeUK+1FGL7KInGc4uSiZMQ8D/vU1dw44EHxwkxHqsDSD\n",
       "MC7dgUI1cyNEMq9TJN/0vT7sbb9Bw6dUfHr4zOf+EDmocYqhoLappBHwbvoxmXOlYDUpDEb3Ag/r\n",
       "p4LZO4w1gKEcy4L+CHMcCg+BC9imB//YT4R1P1qFgtZZoh9CYkKRSa5bC0cLMtD4fFF7BYY4xgn/\n",
       "INONHIWIf947wxwrgldD2JPtol/wwgBvzab0saTPo7GlTqc7jcp27BjvQ/ypgFPEEzndjll7DusN\n",
       "qA/fsGgwYvD/jSGvR+VDikfJaD/EgyR0ryTn8xHB8V2fqILoPEqOqiAXmGJc8y1bY9RaWDHPIrqO\n",
       "F2vJJ8vxswdiNiXur/X+uqD55brDXldSVyi77eggP4MOGd9M/O7DD63WhDblNdJ9dkosI8/1wEa6\n",
       "mVO8RaWeMGQ2a83aLJhDKz3b/2BzaYo9DHaBa+lNjmzwwI4wwK4WGCtqc7pV3v/UFONYPHN+5eoJ\n",
       "IdFPj2GCGU5U8ktiXSpLQx/+XtrX1RM7mDh/wBKow3jFsGhjI25jREmG9plUqzev/eqhgUoWwXMZ\n",
       "C5/VQWIdeUVXrfCgsDXO3WvxG7qJieVnhxMsqfXG342hdPsizPhMDIXW5U3CxHX7vWeGpT/ihkFJ\n",
       "2I2dhZ6aRUmuHL2zXhCPGLA7kJw0QAjXMzFMZylVGMEZDL17bIq/65YolPdX0qlHZwJ1bLfsiV43\n",
       "czxWBGeVE7X9OixuSUPvV4qYOakfAtEJ+jBsqWuCTwBupjZ3x8y8PAEjeB1WnR4lf/yh5Q9Me2de\n",
       "5lLRhsVh40BwIZDdSdTiIybDRYjtcI8tgifa2Mr5HSFy4HanZ97E6j3lOehkmvtRXbrQ8QmUS8g2\n",
       "6n8MwTUaHalacL8Ml0JmIoHtk4xCO5457RsVlr8h7bgiDEmFhsn1RwaqQTXQF1CZcQ4LXsH2lIQY\n",
       "YLm5q0LAlUtLn7H9graImEwlQ/H6qmB00dVlGsJTAjAQ7CmT0LZIM/owjxo8zJ6FULQEfsgV6V1o\n",
       "FIXRiaPo7dirbD/tTc/u4ABfVk30uPY3xCPIJgSTkpXaFl69+7X/4DP09F1l8rEx0PrPZ+YE+kOu\n",
       "+e8ewHODSGgClzJMeyEU4mp/MyrGEIDfMr69/xYmonFvcNSz4/DAZ3ZZHh3kA5kQdmxsqHWaVh6A\n",
       "g/bW+othe6672Xim+uxkR0J1Fows7C6ERvUIHlFKZRFTRbYq2PwB1QNaS0T2e2REnTUpdz1yXINn\n",
       "B6dknUbZj+hZHed1baTGGRa+gee7VpiEEpcFckSZJlHFSViffu3b0Q3yeFzTr+jJ5FN2MLOX7Qh/\n",
       "yprEoxNzRtFwd8YkFj9LW/lemCN85Q867BaNUy7GDWH3KbQt2HMf0XQuqWleUJm22Sq3hUKau9k6\n",
       "kMAVKrHcvdgF3fYwgPfiFs4PZ+C82mNg6e2SriP92lodBIfD/eu2iWlYEpGJcKDWI4dHg3zeU34C\n",
       "x1HRCZ7HMHqp3osjOQSj7qfrF859ZPWDAQVpN5uiWEYxXivSlSWzfqV4UIs9912I9KtguxDsu41g\n",
       "D9L4Zuo/+z0vrZJaMQV0E10wBjXBt2uAYoc4ONN32HO5BOPRCCrE3D9/eeGpM5jazEaHuR+dOX4u\n",
       "qMSK2hGsoD5Fnm4d7WrJkzt7UQNTKScpMh9PAgJaiS6aWsSzV2mUj/PvthoW2Cs89EBcdCd5a9dW\n",
       "gZ5V5GG78suNlLsJkEnVWveSMMn1WtBF9gDymvHp4UX+sR7Ib26bsbRmKGrCg3q6y2unlmIOI4FC\n",
       "KhFLhHvZ3u5gMWYGA/xd6SPofr97JkDgtFp492kErFSqUApxIsZklzH2il0Ma9Gk9JBS27epjfAm\n",
       "prEDVi8AUKhz9uVGVQkCXf9aoGL+6RIh99C23sBO6zf941jw3vazrL0oho6CD8s9UKkdvHXUyW0N\n",
       "kSd1DITyEqRjTmVQytOHfTCqHl5CrcUxCVUaHVthir86o8v4tEVR2c3ug0CASVW9nGqP95fDyZ6z\n",
       "v5nn4SMcmamOK4nWLJGk1b/Yy2WZ+cTSFNznVC4yUrc49AKlM/5CJKpQAre1wrJy2C/89Iibw6O3\n",
       "6hTcHba8sxBNyz8iF9UhFjNorDffqN5uJI41fvkRqA1zYOaNbBVFJLR7+HW0qeB4+ihoumH/j494\n",
       "4pFC1uvVeOtnaNWFcPEJXBw28X7XCYNHPvxtLKxDJU02HWEiXmVe9fxbhMIDTxv4EqhLdoNgqj7p\n",
       "oZqWI7oXuwPxDiyweBWTD6lMHVM0blyFBWC7qHmlR1/rrIIlIp6Bts6VedaUtlB34kGzF1++K1tS\n",
       "g7/YOv+DRXqa2CvDLjZrWhZuPzPWgU8YECGyB7fCZOo0qH9rN5/HH5gV+IiK1/x5gWG1sWnmeBug\n",
       "6Q9kM50gwh20T7huVGf0qrQxxNIvFei/uwL2cKTc5EWQlp9EL5N64ebJEClGG9AG8pDRy9LAO5O/\n",
       "7sc1kYMlyH6d4o1AH/8JY30KCzz22HWImych9G6PbL/uv8Z5OOfr6KcrIwjsG/Tsqc5S4qPok+je\n",
       "McM6mLQlt0G6+DYOY2vAL4EE/W+cCQu/djNZeu90VQxAXXKRJ7FjiywokRdRKAVdmi5wQ24G9AxK\n",
       "WmnxchLemOKPdew4BUQqMYPNBhhGa+TVD1vajQIL3HlMlbWRqkg/E78zs6ngeLjje4x+H9VzkvgQ\n",
       "VJaZRO8lGyi14bh4Ine2K7mD/lPUNvJ/ZPtfcd0iHQYP5xPDvQWX3XR5UagTcMNM9NXoQwA+yGuh\n",
       "DLYrXFvs7MiYVtsbk0Or3MP6YxsxDUPifxff4Oh8n5rr1K3x36n8vDiiW7dFR4AOf4qo5EFCp2mY\n",
       "0DYe3sek/Gk3mpLD5Kg2r4IC4X3pFFnDPieq94WQxYlZb8Zdm4U9jSYjvo8grpYRl3jWbumy5/e0\n",
       "KFkreGTKnDA0ecf+aCOj3gfbTUlgpHxP+6VCTmUt/tuUAOtsePXXsFOej2R6FyllKeIcOse3vb0F\n",
       "BJzEt7+vSh0IsexnS+1DahkTOTADrOPaL2z5et2ygBmLmtQjNk7Kj/pFAJTAJY79Tttv+u1wEt9S\n",
       "ZdbonsqkHKg6g1yT1WOyhEQEsmIKG2BO8ZUX8stYuk86zuZ65B0jxSlKfAGmzJWVuxH/XwCv5PxE\n",
       "igH91Z6liCgEjeCtfOOyDVq/miILAIouSx1ki+aMQ4KivnmS1UNKLwas93UoKpfUsAxCh4E632VC\n",
       "K9F62xXSaAWCbjTgozzXkaVvP8pKnjdCmrpBiTStRKhCdh3LDIzdYsAgH17T0TUBnfjpkMUto0Zs\n",
       "wBWCXkqzI9ef9txKIwZ7XMr+6N86+0J/YHNljVks/vMWYeAHNd9a5UPiTDADBmCZUgE8EOm1vUF8\n",
       "LunG8ZPWOQ25/rW3zqLg9FA6tIZCrZbpVomBzy4R8wYoCn7I43SHbmCeZtn8MTAh36ZCUAUd3GFe\n",
       "Enw7RqVB3UUhnniMuSer4bgV4FZjI22J97enDKjsqeyMK7LkrnBOsnoPd25txS4Q8oQb8CvtLcRT\n",
       "XQ6yqxig6fOrhBxsNCUvkdZ4uKY3NVDCTi+vI5lk+qDfCxCPhQf5/OhMCGwifEwvCajuCs5xDNiK\n",
       "e8GyqUx1/sRSpH3eFNeZ3CUhPsV6+HN6AEh/0zQ5OxHe3PyQZ7iBtwqr3DDRtVxN7O5xtih5q2PV\n",
       "kCL2KeKb+i3yFSnBpIo5CCEXcqw0yfhnyEUHXl0OOSrOFSattHMDDTxNZBX5KJ5xerAd+71tF+9R\n",
       "Q+utiRlIM5+NM+4f5llaHyolHZ43XXjuTUbo4EOAsFi/JCQX4xazFom4bHjDHupOKf6T4Say2pke\n",
       "9bM/L27f2cB5q7LyUDCgAp1Lwref7MPYjVIy24myqP/LRKYYVffTWwRtRtzXxyZRaxeD9ExnUiIv\n",
       "jGh6CMZJ9duUhG6Zge8eYui9PoqoZK6lGR3Nb9f3vxlvGwAF1rD5tlbdNpIuYpnf0tkgFdJie7GW\n",
       "GXSAZqNwEvNGfOJE413dLktOiViAxnXCuAuU++6PALeXpm9hG7VVBjyyRVb0t5+hxW3TcG8aOC7Q\n",
       "2P4qqBPlD/FD4XUz4QKUr8h1WL4eogCCcr1fPAcaqFmrWx4ifSLN1zoqMuvqNPtDWWQKsVDcYQ9D\n",
       "Aug/T1mQpmZO0hf0FNAwxLfd0Fvls2yiyE5ss+g2xKZ+CSaKAUmdD+1ts0mtp+gEWhckXJoX29Hz\n",
       "Ut2m79GmiQiaLepq5ApTGWbSkBlJ8zSkFY1IxQdumaWIdO5H6JN66gvTx45+TPdq8+PuqKL3j0KZ\n",
       "H5DgDnw5sSi40P5vFwvR5dUq9uCqFaziocQeyp6Lymj+9onNc77OsSJBZRRUGQqbJu3iKwSfYpk4\n",
       "+7mU8S+rsoXstZuWHix2EcFos93sZOphNGIZDKZlp63P7DXzJTgJ7Ddg9tzg7wgT0X5fgmy2sZSd\n",
       "ecuRpAZeOiki6Dv51WP0w3IW6aF6szR8rCKultMI0NIURG8dDO2mac+A9UkE4uCIX0KMhY6S3Osz\n",
       "clTKq/+A4/Zk3HcH1H+zGiXFO+KH0IvfSkwZmJvbhvIpGYrjjFSR5SopfvY7CAMYkmm82eKp57eS\n",
       "wXgYjYI8xhHTAirfuyteGCdSYbAU7YZiHUgXibKIau37Q8HZAAI1ZPa4cHh0XnF8Il6IyPI0rB0o\n",
       "HnMYoaru97SCazg/QdIVJEwDoqaUxreMhQhbxePLUDqFqWZe97160Wmi+GtpqnPD2L2bpoFX/WmW\n",
       "BklGeY+uVI1tpTeEFbGk+1ubib29plCbqSJxdREbn9Xf9riI4wgt5kuQ0jp54e9okGZ/8+c0M0Wi\n",
       "72XgfGlYtf8d9sRHfLrELXDPYNgoXjCpIVq0dN3sFeHTNxAD+Dd9vTsExj/askwJRRe+CfUQFg59\n",
       "OmBpFIzhYPEtDkUi4oqyDJSvwe+CMKwT+vQXQm53wQ1za/GSsI7LBRZogc93jUNtDjCCT+6+hE4D\n",
       "PljOYbDpA9ep3ZmAicAlZKmJalCRiWftSHbgNwRxn9ZmSBI+9sROLKEeYpNMJC6WdlUsTcao3pQD\n",
       "Ml1HijD8qLDXBIVxKc771W/SAZDkNnXgSprZPpk06R1aWeN8TopfL1GtDBghfVnBUHoxucubko9G\n",
       "ND6HXm/EIrQk7n5UKNuQTuxzxn+uS1XU04y9gJli4iraF9h4wJQXiyH2b7NegpettOEU10rf3Tz8\n",
       "/uBoxG2dd+PavNbAYTPwbW8OA8tmf9a/X9+XXPPbCJPOyoyiKaEFzk57N1V/xdKVLrXFIyXgITcJ\n",
       "QfykgK9xWe/s+Fxa0Kg2bHBte/j1UtyD8Aow44mEX9Le6DZy62EAtVZ1oYRj1LgaMWTkOmQJg/4A\n",
       "s1BEy+3DJLMnGxacLcXugazUwG04G5yXGhQKkJnNb1fkfQSnxjHuAr2bysZu6iVcwi9T9P+nb1RH\n",
       "wXF2wdO+/o6/U0q9OJHP7UmGDNulgnb/A7Uc66SkpIXvfn4w/5x96YOjVChBd8Q+r646ru8Z+4v2\n",
       "86B6JZvcq3Espu/fAXdAjvTcnrzIDyTsHQIFZxuzlQvwMhVyndI2Npyd7jvU11xAkWoLnZQ5XV/u\n",
       "r8Aw4Lgyd+keTMQ22TNLDXlJbV39kcrlX2+mTrWrahfS9ay2+lNH9d9cLYkz44aef4xRO2biEKds\n",
       "pBrLDsM/Z+vNm0OywpOdwbC76MjHckQQlxI1FBqgAJS94JWjJDZ/y00noCPiIfl+q2t5FSdeoD4N\n",
       "cnkDdV23sbQWoiNCdPOCtjKlvdr3HG5T/5HdpllqFsPgHrEoXTW/mBcOl0PsUFUD0zaRdcz8+srB\n",
       "MrURmudkLo/Y3y27xnR+6xSTGqX27idaqbK9wATi9UqRhmgENKpSav7Esxnwisnf/8K6F10iqLlX\n",
       "ewqLTB5B66GxYjR1jiccsHWwCTYGVwvvs5+rB5UECJSHN9kz1uHJQkqz4ES3+uzrEMpMoJqgluNv\n",
       "o6OT+OeY6/2Xc8DDyv3qMq9HB/+11j8+A23aw5wwWnda2PTz/Wcd4TcZg8xfUPrdOCtL580jVjET\n",
       "kZE0bUgMryaFdim/C+5AerfhNmcEivKpJVpf8KYZXLV4Sr2vRLFBYQCahLCMEBT6NJ746Zw8tc9o\n",
       "WnYFpGkkq1YBFFM9+hUKx+30nk4WOlu0NaPTw7tOGHgfs9VbW1DBbyBAWDhnx1AJhyAvtKBxyOU6\n",
       "O3bdtMSiJhZF2kGmXNLWZ+qLwZnUR1I2iydSNG+3lIbIWYweY6XKMUZKZ1Qe68fZXkHOuLlHeFov\n",
       "iI1eEQuD0JknG2kvAv88yeCJvDEEmIWtIs4X7VmfdyNREFIJRSsdt26HN64/JRNQJTonDyiQ+VAx\n",
       "Pa6+/5EG+77A56ynWoC0uBhIDhOMnGletlVmeV7NKHF5IHxTA8DX3qXcQ03GEJ8G3UMUhoLoJr/Y\n",
       "iDIGmbawx8GDgAbo8s2FgLcKiiJP6rZa8Hd0MscMZMitX4lOcE8iFlYydddz3btYRPnGNg2QTPWo\n",
       "/qcFfkSLHPWuBWeBMyEiUDqNIa4X4oe1yDRtH4f9d9bBZpPYAJTNGgrLZPnDJE/K0McI2tvInr1B\n",
       "Ey81BuSeMmZXMlFd7uW5982D2NvF9ENG6AVszlD7294wgj+DFAMuFxY08rA/upDEncZauHNpHiSO\n",
       "S3CwaLFBbvGKe6KBztuPTYeRcYbEEwcBd3qZy7lT3by/w/ni3qnOXR6QqN0tCyAeZHJ9v/ScmBjW\n",
       "wioOTcBu2H8nF9nUqcYKm1jUfoaAv6Wh/gt9KOqWWWhBDAcr7rRiFy8cP4YhN1bmuJwV/SjyCewc\n",
       "Lmt8pfQv2rsChgNtJ9EwRT78lKZ20uACCvYKknsPqoC4m/FaLtr1RyKsg5JzmdJssYCjCRi/Z0ll\n",
       "Sw5Xauvz5a8nZShr0KReaRLTsX4KUZjOUq93uMeb1IYCl/WfF5nUrWSfq8mWfEn1d8dZ9FsNTFEm\n",
       "Hrdbi87qrFGdlDnoKX2O9UciWckRnKEhKMlzWk96o/EvYdvw8CXSBiCETXOg4X8Auz74MSoQhCGp\n",
       "tW8CcqDgjR8sBjJaSWjHNJCNWv+rZ6XusgJl3tzHYChU2AA4bnEIY/sc62YgzsEpzT/oR+03P9Fq\n",
       "QsQKRKVWWfVcab+JgND4QQna2uhfLdQsSavfv+OF2ocZdjT16lsbUp+QP55ESKwpvuCV3KJFMxk7\n",
       "PBmWcggTwr4MI2PUSVb/fveJoCZPsssiA5kfIGNrnpF30mQsHdfSnCEendkzFo/di8w4BshxMnUp\n",
       "bE+Pzisj4je7+QTAJwcNQ1eWHzGxJ9+GTt+a37XEH8xS0zWJtqOk5+RWCbsi84+eHGNz098dV3e4\n",
       "NLICFfph9AAnE1loTTbeVo2rU9uEk0t+dx60QPqqdURxbaDABmYbY0bGp+HNm7BvVZvwhGxWmv3O\n",
       "Z3F2XpbC86wDhF8Pyt9TTpzevbEo1dHbtx9YIRSmadhF0gw8ZywKniYK71+EEVI1/rp8C1Yt1mqN\n",
       "hi14UORJGP0worSOtg2qv/1cmv1JZ7sbI4eIgEc7HG+ER8qxFeSmvahNJ28XXoieIpd/DWG1D21n\n",
       "hB7J1racShhh/ykKWPj8fh6g/94YDFTp81v5skBDXFO6JPnR76EEejqKedTadsy4inglIj4eeC8V\n",
       "Lb95H9sBgGGRx/WVR11XlFU8tviP+PF+tx9YHn/J1JPUZaDC55LBLHHqE4bIYVPZw1HwNWIgBvju\n",
       "xUvbNPyX/S6V401WmCloaXoNdYJbat+tX5OqzRlhQ2cAjd5RdDBquzshAr9+ovZfdt0yk9GNys2B\n",
       "AoMMwzDps2PvbLJyhwRCeRLNwVwwpBtWgMTCXFpkgKhYvcIK6oi4cRUcaWMfsQSu4A2aeeTCjxvi\n",
       "T556DSvgj+2rENDrMF4GYeAyy97uA6FBAApHPWaDzw9ZMWOaf+BA/KkJ/DLDDlpCjVIA181Vi3io\n",
       "U9pbez60GBHQQN7UOs+YEEHIqQighlN1R3hvQ8RGlCEAjPLdNp/9FyAHO/jgQ49x3WyPuIbxfTLR\n",
       "nEDbeJIaKMeT18FgEJfSmYZrkJ/9yokpdU0R51Ser5vH3kfikWpWeNK/LDgQFjFp05lHNE2prARq\n",
       "k7DmY5mhimAX91xhWa+A0PgCWWCAa18/DKkHX5wRsTHpN5SsXt6jq2nrChhd+QPg6LKZpBvnSkrS\n",
       "uL3Ax1j7Dw0JBZEu7gnhrdi3krk7cqRsIRluuX7mA/X5/SbWIEsKI1y2Mr5/mu8f0Zgyo5BmfscD\n",
       "glkQGHhwf2Ai34A19XA+ljsPq1Lg6Bi3IyrTxbHD+1MA5eAOziX+1vOX5k5xrLk/ir/l9G4+BEY8\n",
       "cw+nJv17FbgY9Y5N37UyT8nvYQb3lKEK+Wiatao/dzfpLX7Qk/vjdQeoS374sMGjOlB40oqaGSM8\n",
       "wvUeKzJ0qLMhd0/PiN2mgcWqvhi3PmGYSKBZy49aB9esiJBEbm4gsMOVg4SdkeW+ZTd9HB/8T8t8\n",
       "QRGm9FNhoBaqjOZYGu6XnIUYMzdLHtZiTDwSOSbLGfBQguqYRy03AAmpsYJIBEb8kjSv15REkvAR\n",
       "AUhDodFtU4FTQPO8Vzutz9Yng86dztjc++JDs26kApvMzrGR9y7tyNzCqPsm+ej3v27rQYsYmaXM\n",
       "NqyKGLisGZgjnOF0dSWE5m3tnDjmwxpjGes90sGRFIjnqmEUFSlwWRppXoRuBnC0VjfTN4VuptsS\n",
       "EZBz0r7J0pUQXLimOslIwNWYM/A+WSU98aBPfPIc7EgsKhK9UomhBBooLklNpjjiojAGIaQEbfZg\n",
       "J6iW3crqcbq2PvPbxnBIcuFzyEDDpf+WIRkOX//858/IrD6r6mqZoU2h4Tg62ROLqKIWUzthxa3C\n",
       "JbpgNQkVhQ44HaSoNvhtPq/Xjflin3mkTMG/POPYiMkZ7bpuPEMknDjEujaUVD7fhdobsmQu89pi\n",
       "zd5zhOc9pItN4jirxxBfPOrHa7jCihoxzw1zUmamLkqXgckd4xz/3GJnLMXj2vXiAzWFaYk20rsw\n",
       "mpWjcHUJyaIiIDR0v4oo2+MLtxYD6NUM4lLiCCJMv0re8JUvdDIBROFMwlqTARqDt/y/8+eSNA0A\n",
       "Pg3O1grkG4sFZ7X7DyfnTSBw/XSmu0K6rx6LYiiyiumjaqLbNHVQiFeDOkPMAWHi88HV938SNHVh\n",
       "7c/iMwIRP9CXJ3rF9Of9abe20EtsSAA3NL2DyFZFq9qWLiPj9pii3K3kzLLyAqvBZdSHFTGrRHnB\n",
       "j2lsIUSNS/M3ohzjbQceYSxPaVp5L/k8t9yAC62LwsfUJ4Jp3ro6FoTsamJfDxRBQLFC45LY7n+b\n",
       "8wO/TX1LP/EuhTx8aZcsNGy48d/lxw3q1XJFSso6qlp+TwQu3De8bJbwUd3vtUEr43iRZLDEMXi7\n",
       "tFuI6JlUt7spunYusLVGxM8IAAHHYsLfSUpFsSj4YHksk3X7zVPahZpMuw7/uWoOKvLMXD7Ct5/w\n",
       "WWPuonMsw+SkUAxYalGQzCleLNUFpiVeq9avIkUTH3cUNDqS/WzyBfh1lUAeysEg7pUGX50mw4ih\n",
       "1qJXsU+1jCCE1aHnzYPM0SaXlDS2OtufzXDyj3SP6CfTGMiLs/VnApHdxvH3hB7Yu3gzJtOeywbk\n",
       "QkhGFd9Xq8FgFPwnFRa+hmnMuBu8DgiHq8IZbhhRkR/NjxyP36qWCDaehcM7aFUjF7ZYNsXXF4KU\n",
       "d0jDhG75RpFfwbj3SosY8PHQBZU3uaq/ralPv9sjcE06nqo2WJ982h/iqD8pvVaaIikn7pw3DHw2\n",
       "6YD0pRDkcM+EajPsGnFfp3516YnHswgy5g0obZ0E315umKRrIoHcM9xKHctYNtOlY20KXD7oT8oo\n",
       "WgF98OvIwhYIBOv8UQnMDODBgWK3xtFe+r4xfNOFhdIh0vrmeOnOAnLCJpG96WcOxY5B5XMaLZCJ\n",
       "17BpFVF1n+QtVJMb8zYGlkWl4dF5Kusc6QOo8M85i6SfNG7P2ctxLsHSLZrPcQAz9z8DKwUuqtVu\n",
       "fc/ZDThn9bbIVpNBO9AuL0p4orIwRzk3L8vuZ2djNhrFqZZtY86CXvFQILu4GaMJt4uRXPIuaQ9U\n",
       "NizQ4LO9MJyUEaOsv2oeuH+c8fZPp4Ym+FIGPjHstbWr459dGtiwyfhRLIYHF8/02zY0ZlpypY4n\n",
       "vXEIHX3czOn2j6FQT60c3TtDBKdBtyugAUxMCoVk+iKn/RjJ68x0A4RaladA8UeqZVgPbMZGGZTo\n",
       "lbl2LmZkqy98P9Yj92Kfu7XJvujXORpdmXkIvtcM2R2k2GDhd8cLjW6u4ST3MFIbS0Cl4vYOEgQx\n",
       "p2vAOvNHFk2PODQLkiQd3XCacXp6hw3NXXWLxE9f27jnJDq0xMaarVk3H9UBCNnriUZ+6ZCEG8oS\n",
       "/5oElQjIKihrJCcd+6joeQly1U/gR/0LjKLABix99evzX3msomhnXlTWzwzXJMUUDMVIoX67Tpkb\n",
       "+j1WD1wWeeyDKPK78mqktpm1edCPaxNGo+IFewBUE5e2rplIMWpfFTcJrW5ZDWQz6NjCvvzMZeVO\n",
       "HCp6YrveMdWTC/6Dr1PhQYZcPY/T068pAKbcCpK3ojh1HtzyjDII9aEaq4t2Fw0RXTysxFCNmQ6J\n",
       "8x2dXhU8Sbdh56HFOAH4JeBYZARo3mO/Ig6oCnLAHoSAjxnqTDxkmY+3z/AH5hHt+OFEjj4FiQv/\n",
       "wIJYUF7TUPjnp5pXZ+7FxO8rkJOwXNNqecxF/+I/rD+ozrZzCbb97Dj1lWenkqmzg5IhmUK8ivZi\n",
       "72K5gFTQjvpvC6GRp0ueqfNLq63HBfyFHvCPDasgpnM1oCQlUDrzhoe3mcy2N1zaDNyR5daIEdnD\n",
       "3J2bMbeRbTCd3Cx7rDOIwZ7VyuKmOu2x5DiUa8kHIIiTcRfKsPRb8wVxPmpxtr11ImNsRgHJtJeS\n",
       "BRjy/O9bjnlIkugZACHp89M24g53V2BPVs7YBfmHWLVAXA9ZTWnk62EFh/9WuEtPy6u22ELIaMVQ\n",
       "RgW5rwnH1KwHLEBeIhKc3D/MTR3jgA8jwPR553n5JXgbWGynqnXF10nYRaEmL5B70P+liXBevXnt\n",
       "ia5bgrGR7s7aHXQPyWxbkj/rdqika/oL5Wx3mDHYHZCVmwRODbVAO37xad1TOwUafgF0nWy89Xc3\n",
       "OJrqgjJvXZrfUxib19hRUKHARUEuYwT/xypl4X604+5eaTGv569+//x7FCLhfCzrsNH0sXZn7hef\n",
       "FRNE8hG8e6CYG7HNlg4+SyhnL+QhDGq1B6p1v93sO0POIHgcUt/XFQqwzfwwV+63zj+L92qw/lVk\n",
       "dvyE26NeK11ssSOeDE8hu0iB5beqd0DvHSfpDnQezMj//q1pql/1jFhfiLxaFX8Rbnv8/JjRt5mW\n",
       "6ltya0p0SHzqOqbE4zfS8fgf1Cn8tbcLOzzjwMVnwnfsw6PWOhqkCi986Q7PrgnfZBrCxM4zXN5l\n",
       "jaEuK48+tQiKV1Duu9uD/xPZPyXPS9kQqdc8xuXN4Auk246wyWX6m+4ph6f15bTgkeThY3fPUGpG\n",
       "yQuEdcsFz4gQv2VDMil/1ymGVItQx91yrXCcazSbU3WxCB0yPjCS6O0hPcjh+NOA7uFic2pwt1Zu\n",
       "D3Q0kEf3DWZTTIMa2Oec/aYFI5qW6lrul2RYO6BDI/7amPWTE+A25VF1yBkjGL0Gg1QUSTFmjXPP\n",
       "Um/wTtuRyTpx19GEKPrXDcnZDofcgM5p5jJXEvg43L2kAQ07HieuDPPONGvJOehz0u85nE8lU4VB\n",
       "PtA0UTc5ZvCPAn8tjjSnqdZn6lKOh01WjeGuefBz59YbGzCaVNzTtjirg3WZPB0/sJEa5VsObvhW\n",
       "d2g68XfaskbxBCBm/R8P1A90tbdQSZ9LtVQjnNd1/YUvy1mQxSJZKfBoPaWjOT0pMfGqOsmcqoOX\n",
       "V70kQ/YXpBTDD9D7s2sLTRteFNdd2vU3SjtHkulHzlJ/JA8+/BQcmILvGe2eFxsiSON2RRV6bo+L\n",
       "Xc7G03+y0oON9t3gu0fOq75PJUhnwsENYYW25HnpILKzUSPDlbrkDquBbOU86HSvFgms4NoRyvAl\n",
       "uGDkKfTiIAKyt4ndyLDU+Tsg6kwjm22rQx+H71BKfDvmIZZtcTMS2LK5k/TeOI5iccsZCS0TekFH\n",
       "/Z0X4+U9i1lAnKKbmIHyoLJsqUGJzJR2AYN+tMPvKmsICA042pYs9x9fFErIXsrzalFT/OJGqQ+f\n",
       "xJ3igKWQ9CWwSbUI7EhYEOo57h+sX8qJFTyUtgTElWXLX7ZiDJk/3TT8tyZmBxg/q3VYYrdWtHxj\n",
       "uZC5u1kQDuWtLwhk2rWPV8RFg8n/ILNi5XZmpgCayQ+Acin7y9kOa3jNqlQN5tfG+PKizl6b0luU\n",
       "Vq6xpYK8BGnQhVJa4DjaL5muYqQmVhYAZTJeOvnBp1QVfqPIksuoJZTBjHVpgT8d6PMyYkDoJTCu\n",
       "wongLv+Jw03L90eRgkURpsHt/ryrTKFeDeue4FGFBZ98PgQ3D7WZVzSb784b0tHhH1YXHFSLhlC9\n",
       "Ykg8U/mozRjWYaeEVWlaR6wtvwCxka+MS+LJ9YM6rkhZrubptOieHML5UBqRS39sctoKbC5Y3IcS\n",
       "/w0UuGwBEQfVn2uB8shiI7y1gV8ZR6WyBidxBW23Ci0dX34iomWT++BlhI7Hq64cy/joezq9iE0X\n",
       "xNJu0mVW/CaiuHDH+V2FyUjcJ1CUq2jUaIOZPvOvF7uBXOp99I77860Sjn3QU77IVSs+wgYXz223\n",
       "MkkuOFuON4a3Cw6/4hQNsFBTkAPTcrS81N1j/Lq2P56oinK8DYQZUSzZH0/SIxr4HC8uLKcChsMu\n",
       "gMEYf6qMekBbb7xtvD8yTAlRSJZ+oKg40VcO8XCt/apDa//zbtyv8XucxhyO0QP6Qx0C3HmpSfYp\n",
       "++D3Xkr5nMVtqdmd3AqzPbzJRt8nMgWCZWC63ilsXf9Ea6XBPbthndKdg5yYo4cmB07unrgYRv6v\n",
       "AS4kbJPlJShsl9it7+lcwpYhJiS9GDxbU6PoWf0a+ITYrWohycf2rug7UVGkrQRg6P/4N4SV6gU0\n",
       "oN3vQS41utn49Tb4jpshznoQPNWpzNPCzaOfWGGgRdoAG16JtV/+Fau/bE0lDDnJpSGjMQLPHIEZ\n",
       "8b/C0+YHtZVSPX01l3GFoQJlDsPp1tYdshVs6gsOGAYxISb8Bj71HXnVkRw3t/hgaQb2O/8XFJKE\n",
       "Sc5Z03G4oH/fgWKkbAJPNthLXjstiYFIUwNyPxYS91Y0b29nkuINia7km8i2OBfABImDZXAI5Z27\n",
       "A2GdgqognHtxBbty6fBH1/f0zn/f5Y8kPdMPL7p13wy4PwnTXk9vGtcOm4cup9Ai9sa5UxDza/EP\n",
       "axiCyDHlpCiH4Bhd3J0LbVcksQ9eFUa7MXp7hbjPww0k4a7bams6Xg4VuHbvcZuMu+80z6CZK+68\n",
       "cvdVjPI7xHP8f6aCrFwY6vUtCWijO25H6EBz7rfI4pmYFNqXR2I7wvz4gRnb3WwT/EEPmDyJyYcW\n",
       "JluSU+7BG9CfLA5MVm3tePBJar6HBdHoNO0OCNkkNiRpgWqz38m3k1FHsA2a/9kg0UnXj5Sm9aSG\n",
       "eQZVhf8kHTcD1J3hc4NcmJJY+hTSBzByaLUKJosJGHkinkiLVwnjghdJS4dRJBE6Phqc/0NMtQJl\n",
       "1pmlQXY1RP4n2pUfwGMubDGW6+Q/sq7WQNDcVJIZkYMYEYrq1sVyUT7rYVKwHXnP2ZpNw0hdWhgk\n",
       "iAn9Fbax9mACxle8EYHxCK7tYrDg4fR2Po6oxbJjwnyVlNxL/xh8R9x6Q2e0Zm/p1ShA4BXEVfHj\n",
       "SLo/ZkYex2tELCBgeaVPlyNBqh7XTfuXOcYhINhQut8mmIc3Sz+E7SXQhayhFVvcZe0B8f16p0o2\n",
       "yBoPSMq6grXuIcjg9wjZpuPVkIAAACYyQZ5jalPBHwAAJjS2gpM0IGoANlVqAIrlIH8NsvCWCql9\n",
       "vn4aF8vmObvcJQruFpZ73NPbTbS0Z57TKXAx27eNR93jI30VHpTdSs+Pu+kqgN328V4oRqL1aSat\n",
       "SiYHufjXUWXCSwRzRp5Pi7MolDN8HY3ch2q3qR+1gadOSSdhQGZEGcJqv1NjBkRnD5P7rJg7bjpd\n",
       "PSDLniKQc+YXKJX7aB0Xy63hsBtvh6LqkONx8BDJ/kSpNfwyyG7FPtMZL/SIhznQuTVdfv5Fiz5v\n",
       "dolQbZE4Le3uKLfH3sDfs0UnsNGs/7UVStxg4tr/2A3IuxS6wm2fQxhKUDCRhaK1rOibVwe/xVte\n",
       "EKp0GSSNj9GquwY0hpdZr58RnzXjUVo4BG5cC9YqyAoyWK65I2YwESpdCY6GdrxW/kT/3TVQVxkV\n",
       "bJ4y+UAEcsMY92C0ES+bz+updyTYtyEoCBtulYCXlT8xni0i/rd+6jmTJ4FD/zsczr/CCcVaSGgu\n",
       "1AoqqeCLjMICW/U+Q+mvoBxdS4FqStXVi2BQvH1eRBSpQTr1OpcPkbwYbw1wlUPEb6/qzyDktNqR\n",
       "OZekJJfavZ7/uiFyrla8IhDcxxX5z29XcDT3CSw77FqDi+uktafaBeqOGqJxE+GHrosV9HsDJ7Tm\n",
       "22LIMUr470ddlxcHDZzY7DQw33IBRozsubdx6B7oJjYMrtW1aPUpAfcPkd5JkhvCrBanF1UIATFk\n",
       "IeMN9BhfolO8x0C2FscbdTk+XHkeZx5h3tjku+ewggay+9HEMJqLpKqEDI4xD/vEHZo+TVFjbPu0\n",
       "ZVcAQLXfcwFUqAbz7ORVeFVofQVgrf1K4+mi/OC/vjhY6iU+E0e9XahNaZssCpvJgGAS78y0A9Rs\n",
       "dZifmh4H9LZrlghVA5tNhVW2yAPnRinpciCNSy0Ihoj/GDLghubyL+WrSPCd4Ec8rjVVw2wDPRrD\n",
       "lF6/Jq6LLe/aaKi6QrFnBBzb9oyvdXRDGLW//v0RCzK8bDH5kjLeqVAhYrxdQSBJ6yjAQ2y79bum\n",
       "OkLZ3rNGLSyNQmoNmtGD0Xi39HwPDzRB2DN7mamtE/ehrBXAxWo2AEqjvDLu0i/YC4UEEMkEHVP4\n",
       "MIprNDKLRNK0NLQaLbdzS56q50MBnQD2SnDWwMBxZ5YBKstQqClfug6WwuM7YHRx7Veq/k1QA58R\n",
       "7N5++KViFnTypKRjNkheqJxu4veStc/1FuTFH3m7FGsT/+cejmmtIrAog9sxi7N2IiNIiYD5vFak\n",
       "zxf+9XXhcGbBc2hwzpPCMpUwsU1yRs5Gaemzy1XZpJA5vwRso+U6+1jldmEHpYiz3toC6rLhkCNQ\n",
       "E4xTP0Ma/TC1VIjOQcVd1fBFUuq1hz5Qh18gaqduGykZtHkzGb/8xzx7ukKMp7BR/l+il9ZLT0Ah\n",
       "yPi0fbMvuMDCif5al7p1NDwRopKtzpYyFB2Y9FlpSuKEQFLy0aX4NqEg61t5tNKQfdwCL9yM/sUJ\n",
       "PdURUVyQeYkrhvuCuWOrS7fMi5S3R7ISr2bkNlsCCBQRJTsyoQ0v0Xc5gfDabb+z4Y3knBqroTFE\n",
       "HUA9uwTMtGyrDufHYxf7WEL426Fj5lpoUIoojo1tZ6p1O5BXzRhW+6CiszxUKPd1noLfTxI6TBtc\n",
       "nGFV+tPDjg0dU2le6YhUnR1DgI91vHoGn75icFcUcakCWJpBSA2SedvS8xXCUsglS2XGPXYbcxuE\n",
       "0nmGnXrzV6ZBaT3eK8rIJ4vHONZvmfEzOUbFlA2fIeDr1sEIrMD4VlqkVJBGStegXnzRpUT05CFa\n",
       "sY/P3gZdu2WZfCcU+WD2IYRT5m2WTJ3VdUthcmOL3Ya0FBs8447S7mZtmWNMPH013W+X4k2+LdaE\n",
       "jyhkjtGBxyogzvUeesC7feqXoFwQ6XunmcL912LRAEhHLUNCzSi8lPBb0hHxGq0JgNwD3qoGgewz\n",
       "3xdK/hn+QirwozBbg7y3o5BaNKfrrbHsh6T+phmmRmzaZpPuouALNTlX89shnhqHLCfwkeWfHazC\n",
       "KgqzdO6bHW2XYXM6zx33J+EkTSsj17snojOLp//3I1hJCcdqxm5zqupbfYiN8UsoldeBMSBCHENi\n",
       "AoP0n5ntrushVKVpQTVhmfjcgt+GZWMtzumsRD4TsUk+jrKdkwKpBQeTXlTooE18XSPIE6TYBXjr\n",
       "REpy5Aty4ZrIJTPrB98snPoKLPdyfMaYZmm+g8qUqcduabo4mLKlFR75XPqCXdotCQAlMAWxjlX0\n",
       "ayWEMxa7DSQG+SC3+PM0UoH7sXl5+faSynFRkQh4vLI9rkQ59kO8VadXti5zgOvQpep4ABG05Wws\n",
       "v44K7ahIR5YeAzCAp5QqyEiehYxl008TDCalQ9thCklBYfyjiJyEn8Ljx+HQiCpZZ2PQ1rZJDgZE\n",
       "v2r6vZCO2BzfQq0OCTvCri1SOvXQqgpihpkl8eReCo8FFwG3WZWMx+8T8xKAUW9CfHRo3nyhSVir\n",
       "t3RpBFIi/KYRe8usijbG3VFDo+IK/4cmccUC7Pk1rHaJOSTT2eYwgn4ctmdQNBgAAkPk6xtDxKVY\n",
       "Jz1YJbDa5YqqGwwyDtFVVZpKIvLBgSUoFbxtRM99CJFOIswdB684yGl1/dbC9s8+i5LKgpPFWp97\n",
       "z3ujMQm254zkhp4a5sv5MnyNHUBl905btcIJ2qa3UqffbN6uyyPSe2b3ji+VvJNn8YDMRTIF8ilg\n",
       "s+JRQv+TPVW1rOMYIRkAv7EDYvlY8J+2+zwROpQK5ymPEgPb5P17itRt3F7d6uTXMP+NwnQKI9B1\n",
       "rDOWrMiLNocxiUR6aX7Wj0FJDrlVntm0yd6k4gypuE3+qVDlGu6rAFSbmavmxe8TPc+zoq/htN6J\n",
       "gy0wMujV3vtjeClh7w/nacX6k8cze4amhvR9oIL0e7djs+YO8J6BFSfmKE5Zh+0lzU2cGXatA/9U\n",
       "1SP5ifILdfzxVfelqIBKxO2yxvdIFzXA72KAkpb/6q7r2HLuSKLJiui4327H5aTyv9cjVUIkaxRX\n",
       "+ySrA7SqqiQESP9lg6BPdLsJCfBYCy2XLqXHBscozgVllHaVufKN8cVd5GctjLWQLiF9divLFxFF\n",
       "fa8AC8CXKRexiRaadQmMsZTlf46Ad2/oT5kUlJq2937ch1quJdgoMzB1xJRozrro8ZpjFmf04mjP\n",
       "eHYE/UbyIYF4PAdfdZnUC+hnnfm/+ekMBpYG2e0eBZHDwgOrqaye7sP48GAiG0ESasXwPKYpO+tS\n",
       "ytADnRS0n3LhK2Efdd7hyX3aFXMyEw5QfxnCQJfQqGSxxQwqfpNz4iPaZP/HD2GsyHJzlVfjrMEH\n",
       "2Vmor8Yn3hY/BOd3UieYNEJ4R8CSZsgX1G6SewkFaF8YkyWDIAZcLk2DtPNnWCkXsWdkOIpNjmsR\n",
       "mADHleh+MWwlj1BjUxQnZtHqyvdZXo/2R7zQsYBJnDWMshXoSfraXeZuK0sNLJmys2MLuNkbdZM/\n",
       "XUFPD4VCZ8Nn14IfHLNw6i373kBWj4c8RSxuCJaI0sr5xxSyZFc7lQhLJHnKEj3g/+wBukkmkzM8\n",
       "fOtI45G35M3utg4SU2c6vJ4ecexXMcDbrWA4o7YRjg8z16eJgB71h6dTzA2aOzJ1+OHmcGEsrOcK\n",
       "nmy62kcOZyU512cGfbPQBqZIuPITCv1hQ+SQJFQA3Ys2iKnWK5Wr105r0+eiLxY5spfafzZWsKFe\n",
       "D1H+fFzEhG6dxWdvZi6542ab3rC0ktXCdkgJCuRgGruWSQZOpsDTztn8gt/8ez80RHXwyL/nLrfy\n",
       "mozNkThx/9No0WQsssulAR7joUZXIADthhlYILFf+iD/DG2vQfKZUnsTguW2ZOffzIkCzRIZuh3Z\n",
       "HgsbmV59DuJoFFVihAlYhjdfaYvK+MFAcrf4jIk6iK1/mJH/CCc2qjXJSy+0XEQ9JTWuhhhmAqrK\n",
       "QkIicURxsZSfNpzkn2f0SS/cxp5jJ54VN1mcjM19OJumMDO7QimU7sN7Xi7DIyO177xrDuGTU62K\n",
       "7+/YU62wDxIbUZAh29Zi1i/21H3UHWnkmDt5dUjLXisNhxUiERIT08qhe07dUSJy0CFK4mE4cZDg\n",
       "Fq2ajhnOKtnGp0oTSedKfScF9GRWctGk8zAr7c+go+Zyf7prmLlHY40ml1K5rqEmTD9iPWN5m6RK\n",
       "hZy+BdWfl80UH0xJD66es5Rv/kGlMhDgSodnzctjTjm/gVl6LZBu71gCbt0FtgNlvKWEAL9Q5ov0\n",
       "f+JnE77iS31rUa5AGvYHGQWDfxrQl48Iv24ByNlpD4ShRaW0lWynVZxJNUM6CqZ3daDsa88iq7PR\n",
       "tiemQxyRdflmK+2NCdd+YeGsQ5RFR2CUs7kNDxbZCP3XbyR+59+Y2A0cponzk9GTNUSOn6V5DsR8\n",
       "6ZQT8O+K7nNGPs/1S3uXFGfrHeceGzH1l3hoyQlqM8cthcRN8dS614nwQyiHhTCEkNciK+DXWQe4\n",
       "DX5O8V/mA9NiADaB0HAOpx3JP22ko1NfKL66Z5HjjumZeZMqbUmYNUBE/iUZ+EJOyosHiomH7PbY\n",
       "Sda+B00Id0Y7QRvKUnKjS7cHQ9GzMEoRcSgWQxq4ji/Rn5pZLE+uGj6VsZy6jcDJFAFoV5pu07eV\n",
       "+tzMRH1kAvQpYuhQ8Y6nkqqNRWxKu/yduRp3XHXT2WBzdJvOWS+wtl//ukAwMldjQrcWciVlSe8w\n",
       "0BPjB1b+UccQF0DmIlA7s7D+/vZDN9mlVGIXWQYQQ4IfxaRiD/MiohhQummC367YWrJlCrA+JcGP\n",
       "jPp83kiWEvx8DMxJgpqTeASnY3brZoHPSsq2KKypKDnf0+giqwB6YJy9+cmTbVL0jMlRRV6L3qL8\n",
       "hmlGKbw2ZSNj71cm35I/sJ/s0rPWs0ZioZ+dOTeBhxiBmvrWRPTYVJl2hpmmRE7gHqTIpVXEgmL3\n",
       "E9cCWMB7J/H8jvqJlfXXty6FXeZHhDf3cmln0vEjXY8V4dTBdrhzaPwgd42KpXZ5KcNsPf0aciza\n",
       "KbuwPnQRzXkOtWqZNIPVjUheFFkiV3r5Yk/bg91aQBW9rKX52d5RA17uwI7wMr0FGKD33lcf2p2m\n",
       "fBzi71o3DTNK4taaGkRaOPI1ycZvzrP1bYnPXK4NM2TZn5HnjXiyNt+9st+3d9rnca9WI0jULfJP\n",
       "Rnjwam9wa9BJf5iCIrbMJfq8mAhRrOeWthleIJqiyvB3/EO1pyMKuNijqfh3wfGUzuQB3dDA0Cpn\n",
       "A+JOM8vyxOuTcAcFD/uJtBJYF8oGnO0sea4Amo0y6xM9rQumnm3hUeldRUhtDtVG1CBC+AzFPTKL\n",
       "dWRsZhtWQ46w+TOgHNcJBztIchtpfZo/pnlmt/l2NmaiJYFuyk5SMb3d5FIOYDUaHgTdINMN4pbw\n",
       "aGm8FkGqfsAjfetX9aYz4/TudzU3CBe1mqG8cEBBwMC3oamHHMROiLLQXEH7HCAzT96aagtTutl7\n",
       "qBhyhnaWr6dpOnPLeDNEkGaXaoYf0X1+zI16E37abLp7sC9pHCPKJS89lMXBExNUNS96nKgQH42j\n",
       "Ihoka7gJCQ8+ga+DJxgRMUkaKWj/r2qEV2eXpHZOscxsj9o6n+ZvSRcN3Fm+61uK0dxNH2typH46\n",
       "B39zSGRdht7XRmKCaN91NZv4jyky9/CHbvHlTaiGkz2lKH6DxOLxZIXf32CBkJQZXoc89Zf0qHZn\n",
       "whrFqJtDevGwlXXW5U7ihO/xOthdXSmBtFBDT/fOKENyOIKbibWKXYguPEuowe9Kxrp+yJk8bl0U\n",
       "s0OLOWpxb8WIP42FTCJ01o4FYv/9fH9L5G+UVuqzdHF8OMFCCPDOnXMyxeH5fwjrVMCA9E8DKEAN\n",
       "rAGcgSLMY9BRkyzvQd2aSfxBi+TivT42wEM+eqiA5i0up2eosHD4upBi1XcovWdyrVSfF84hG3S0\n",
       "eNZKdXNR9eCwJKut6NEhfirmihgeeYCWtdl/eVA9yT67NCDQ6fJXCsw3wQFSn3ZDXlf7Bj81T7Di\n",
       "0gbUNq8zJC0VSNQZTf5Ip9KyRFqaH1WOwwX9T2k46uWiEkcevxPJskfJRgqS3fzyRj9DDx/fQiwB\n",
       "x0GUohtq+y5bfX2iTAVWhJHbZY6klgG9dmsUA8P4oeZI2Hyw7ZSPwFWbnZK5ussRM7/guTf2rP9f\n",
       "PN50CvL4M7+H/IZWWik94KIwYyegv9rbJbqg4MxHPR/tZZWhhzicVrL6c8xDv45Lf/fK4DsHWRRn\n",
       "mHMI15ltMz6Z1TUtaI75Iqw4vsbJb05mSyVw2FjEdYmu60KCrx0yV+zzTO79DzqQKSaVRNhKfzEB\n",
       "CnzunXtsAUoOqOwUw26LPtKQrS3V7xLiL8V8NB/a+Pw4MOCbHBIxcGkOSsyD8W3Eq9fjYEey9BMa\n",
       "VFEIkObs7Cljz/Hbio6iHo55Z7NLaPM54vN/vvepNH3DwdjuRe5Valf0yi/v72SPMH4OXqx+8LSO\n",
       "tIVOfC89/Va1MNgU2fhe4gIE/GzILMLtF77wCSwOm/ECK2O1yK0TK2aGSodHME5y+06R1y+41Rlk\n",
       "usJVz6mFlKIPeefpPO1OxB8iHZVLVKxeLJJ0dv+p2KOh8yl06Sc7fYuNCQIY2WvIDnsungSgoj6T\n",
       "FY1RKV3uc4oB/nf3u9yYOe77aeHoG9MfMUZu8Syjceve+rTaA9Fui7LywA5pcBGQLIGHeHKJ9GyL\n",
       "ONjsEXpn9TheRheem9eP1fp2RpbzVaI60X+VA/qEtQS/4MyJLUB1WJxQav6Jx13pq4OlJ/aFSHCQ\n",
       "BEP3oPMwnuQ6eML6vZqnsyatUREed3X9qvAnrLKxNe5BqQjqtEOvIcFluCpsZBM5bsJxg63+I2Cp\n",
       "WV7lP99gpMdLZXXaHbEMT/POtmfZcpZipA96/agCz3QmeiFdiofYOXyZEulC+k/Tqc2QTbL2YoeC\n",
       "JNkxYJcT1PU0kJwWGynuPB3j6klCc6iY0rdlhz8dd/VfKgoj2PT5Y03dKlK9sND9eUfFRsYSVy+P\n",
       "QyNbIiVTHfXNLx/xEW9CoiPPl1AWFZgioGdK7hVwqeUcty+BcRTdf/u6sylkJK+uek6wTu1ESk88\n",
       "25685J3KE1BU8e7Bj/FZQquBKeNrBBMO10C59YtUqJVVlK+O2AIkVF/pEXkEbWd+fXX+DHktaWHT\n",
       "xd3PMxgbfWeqrhd/nsKlPWfUv4GeBBck+NH9lQP6/9QphPZZEnJCfc78qTz76JX2i+BJ87r1qlNL\n",
       "E62M5ncCo87P0392n+CTnZLVqHM6KUxR6H2TxJH18zZorNyc1UbqiRfUZ56VQwIb1Qe+XhbJn2Xg\n",
       "ExbrVP1F5ypA1xF0VODuQrJJ8ocmRE/oYgkwCk+n0n8P7c9sugVFWiYEdawVkG0B7P58+cRYx4Vu\n",
       "63otdi5ccYb/66UP7ykGcS1+J9vW2/3ctPhHLuvOBbts6leL3g+Y3g0rnjnWdq2ndrKhXkofpmZV\n",
       "Ik7a/58z/R4dPa+WFRmd/qcbH/7D4v3yTn/0VYrgDkMmMFCCVaCdkTQ9ADaXM1wqCU4Z/hxlxQR9\n",
       "qB63GWYaa/OWQvS+Zdms0KyDJshHQ8AAjYYxt7CrzrufQ3FWSJ8QhRgI0ReDh6PIEEQVFiFMyZcH\n",
       "oAvBUvPnrix84Oi+5E7CMOHBLCYxLukMGHn/AyiuuSMOlx63GF/SUwluOwdpTki2YR4jbd6yvllP\n",
       "pLhHp39KyI1TtwxU72d/YdCct1u5ZCl2bi1YbrYtIZXf2UUl/shzqTMHh8xzDj5kyp0qBGhiPNk1\n",
       "0PxD8c3pIRYqIqGt8nC778hmuGxPF1mrCS6vRjZRHcuVMTTfc9x1GTjb1p6ZwmiafPhHCo09Yx8K\n",
       "fe4sdymt9EEMhUkq71xoEedPseFoTSbczzKxmc3yOH476ugDHMnyOxpRroxgBOvpifVeE/Fcwd0j\n",
       "0/n0VxBdmv0dRSk2Auj+BQxMPyxbQ0jmOegBHUWAONN4VsSGsBoLUQk5o4z91NS1rrG5xqIrLGFB\n",
       "5iIPkWGlFJuMcJRFWibMHecuHGSGxFSX2F4FylhZAgUHldTms7DbRkBAXCJlt3coEtGvGdb2yjkt\n",
       "ici9GiAqwNBS+D4o4C6r8QVbvPMrsU1bpYjYZFSk+thCSdn9bafvmNxyOlVzngrHUyKgey1Sp00d\n",
       "0Q15doDh64sAXcV0ZcBeVyBO+Pt3utnojcibntyinKlwqFbQQZE027ku7coji5oYpwHFDt4u8Vmf\n",
       "Q/ZBzLLFAWd9a6CdFpUMw2apJD+QXJcNakxlx1Vg74pbbOhaBZwd+RFTXKigHaUGaOXc9L/G0zBS\n",
       "T/ZB0ajU532T7qOrSzt8E91q94wSg051CPvbYFJ0A3XfSKkpkA2lSJ2HuOzqILycnvDkrchk5+e/\n",
       "pjZ5k1ztCEY2Vi4HtKc1QjiM6ZWDN624as1l49OSlTNIqjTfW3MV+xqofDPINGEMobntJgRQgCG6\n",
       "cqHlQu+PbNPqCTgmgF3JgP8oS5NBHnYmon5rNPPsQOnBKMr1umsiMtU0SHUCPv2zsWSh874+azNq\n",
       "fryd0GsG6RDVxh/0FWFOrnxBD5wgxDT9UiRUbATrYXnxdqjwHfju+yGn+xUBjrrE/oQNl8MkdVQo\n",
       "eqaOGz7BrC0QeEOO4PkCmJTandMHC9r0Aqt5b7XHz/N+iH48jARbYfRGC8OI0Sd1x/PXsZ4MVW79\n",
       "QJD6RZht2xVFJTMnIbKc6KsZATutkGHTBLKWOiYr2SpJogSwNXl0wmzg2LMBar7bjb2hSTraMLGj\n",
       "ZvKC/1zKwDpEBfre8rer+sHWHT4aoPv/w+Ufd9NwPTOHnuEUTrKVBbtW02ojWxbui9b02xQKPfzH\n",
       "9erEb4cozW7Z8XeJUcuNU9iKkC0jrf+DwZy+rRo91XoyoqJlE+eLNAmSxpAgLrY6ZzCkhKg+MzqR\n",
       "UaBV3pTgCoV2g6NFGCk8vrNHKZ8y8WJg+cyGIN8poU7+k8LbZb833L7D00Uv81/zPeaYmOybRWJz\n",
       "CHSRD0o247Y9oe2V4lE7TQ0rrNJJd4IjM6Qd8e3vUTsWqRIsbkTPMXUjhUwie4SjOM3DmJXgI92H\n",
       "dW7qDaUHVZXlWNezdZZN9YkZNqYCgY65qX4flBrIX6SJSypwm+Ootx7GhhQPHTfF4l+CT6FvbGRR\n",
       "Aw61hWARzXk4wUq+VwZtufZqOXHkIRwv5GNpbFvVsUw6pG09+Oq8kFPg+G/uz1YEBs3suLq4dvsp\n",
       "YVPR3oBEOe9A9ho2S6HVw5xaQWYjdLlBmMbn0ZBAgyaqYBJRFFQ2QBOOiT1ZVtlCsrbeYffgSaFU\n",
       "4lQbdvvIG5swfoNisYPV8EM9zZz++/fJxg3S9dXxvI3zzOEyzeHzRUSUt5RK7TdWnB0sLQjCIa2T\n",
       "9ZIW2Nhz2LiSmLjf7D8iEdzYSbMTuQ+dyhkkx2ppvDDUUn/9VQOpqLVx7pIPhyiV8pXMynxyM9hB\n",
       "S3eoNwdp45nmwelQSzpBUg8ynyOO4cm81jL187SZ2Paj6Gb3JWnNVD280WG7zNrNBm8byg072/P2\n",
       "I26dbkFZAKiRoQyzBrV0qUsCzdhUTYt0AULxxy7CkpkEnmp/KBIdKlxFsrP3Y75gJca1uvSxs9dE\n",
       "4ICSe78xWZu7BMLdDoKswm2y1m2N9nWR4G7aG79fLK3DgG5d6PkMNqayBzGqwG8Zqkv+YrDaGIne\n",
       "/G/30/PDuO97RcgSI5Gk6/F4NohnPijzqqEl3mvsc3HEEzrAUrd6pOxnHDwRI5w+oumHRaPwbKD7\n",
       "LWo3hpw1L9YM5wKYVsFDqFCdwHuXCsYVGbC7lfHYstEmmxaZgJLHKfQXzfnFF8IubOShQ+mgvW4d\n",
       "f8g/kLXGLpoLCYSHPxYKLk3ea98xTsu6Nmevfa9vn09N64JjzImGrPd6as89D494u+Cje41nl9GB\n",
       "Q7XdOQ1DHtgHy9PwAy5nb3/9hnejq+OvL9g5gj24q7eRkJjbywbvVODOplbfsQ4Qaa3XjkrELW4U\n",
       "ZBPF0hajr11D9BVGRDvd9TyGpENgnGwg/HIVhX0Hf+pssEk/fyaP+DLO+Gj9but+6oxRZO3SjvkW\n",
       "9NCgMlcwBbmtN1r6NAgzdGkgCPxt5nj5dhAg+o4J+2JqqnN7vRo0R/oKbAgJBNhMxpGjXz2GPad3\n",
       "b9I0007Sw9tGX9v/kZcmEnHuHsOsNZvUmiAyVBuAXUxtoNwZe5DkaFeBHH27UKjWB+WCS8ctuYjK\n",
       "XnZOqiLoqRqCCjUnZVdEchF7UOUqhoIJJkQ/HNzj+ADzYYr71UA/FXuugxlXpeYG6qPa32s3/0dY\n",
       "JmcfchLUv0swr5+/32VhfsfeL7srROoSTtUn5NPTAR+oDPIl8+z1uHdvN4CYo+PE5QyisWpdA1OU\n",
       "C3+ngw5jR9cGUJSNI9VxmbBBkjcpiuhrxCAyfHvG1USzSk5LyKLhbNesaNmlwkXevws8QQopoxjz\n",
       "OVDO9/sjv3+y3BaU1dXAZWFKBLVf53aNeOvhYyHnb6uIio1TQlcBvsJupTh1J8yJXpfKsYuXC381\n",
       "CAQzYIr7nluzWjGGBte1jCzUyHp5v12UGvOuKt9NxMkWR/67yCTiHkyqq4XspfZQOHkL9iECwA5m\n",
       "qKDvu0BhNKuMiD9osg3VJ2buNKNknm3lH50BpaL/ZDa3+xPKgLyekmVrPWbU6hmiJFJ/miZmXA7u\n",
       "hlj1airGJQ9PEXPuLN7K4gUfqfEcf4ZAoxKs9HUmxmsCRdm1DojrxPTgHpGtRJQz80tNZOyYwjR4\n",
       "rh6IoNg1GdJ5fJSOaCVq2/c40lP/NwbaXSFMIAtSfbpRXFvKQPY4HwaaaWK7aSn2hImpQ8ouHbLp\n",
       "IZ/LtS64WTojMyJi7e7QwprOQWZwH9rt3Zpv/MjiUTHPJ1WrsKGzYxovxNesWuzRBV1yj76sC6KB\n",
       "An98vjkBGLh22vyOc/uwXTjHJzf8HY5uGth46fqO9RTcdf+iIQXIxzFlw1mmziJJJikUaH+Ey20o\n",
       "FwuaLbcvhgoZC9CK0coDWjMnBbp3neOD/eE+TD1i3js04HMyf6dTeuhDDSGGFIc1XzpHeavu45jW\n",
       "HyVHdoRutrZurqMDvT5yfGBzz+ZrQOcBEU8zJdwxh03H7zt3l+9PYy4p38nrJEV6EdWPD20x14Yd\n",
       "BiTMu8u3I46+43LJA3BeyfGX9bM7QNEueHkzkovacpLXDsiDnU+k8vQmdoHuf3WL6XEU3ayT9spm\n",
       "VJXcQFU1chT01+MxkBO26q4wlAwR3XLy0kq5j1Bcw8H58TW6KRizvbB9pMCLxcJufiqvCeWDl/r5\n",
       "5V/Ssp27d7GIlYNnZLCzv+E6wvOCeOaOsn9ZEeM1IRKbeRK4s40tud0AlgJPb4etdEZFB/pN8Vvw\n",
       "Dt+aJty3Z7c6y/mKjVBG1cIPFHZJFpLQLSvWFeIG02tzt7niTq1XSlArDVha2TtI+CXGfD+gYUP/\n",
       "CMDOIyUiLqIrXQFS7d/j5YuNX5l7dXy9uLATbxoZaOpyoRF9a2IG0KPmeC9Bwh0xvkyORHiYLXXt\n",
       "zgEJSFSMSB6fHNybpLEPIdUwar7VuJhGPZtjM6BZM2R1onneXy1AZF2UZVBUc+r7nelIvAaqOjUC\n",
       "LZaqmWX3ey73eKksygLBkOODgvpJOJNkhIHTSYEZM4W2HUleXHoMq4rOR3iCQhbdUcwv6K23r0+t\n",
       "DJIDmyTBIgf/y9c4+q+zEc/HlzBpi7CTnIme0qzevb1pQHF5d21CJll2jPKjyeSjhhTTeWFlUabg\n",
       "+UQcL6waVH98Vrvg592GP15+aiCByNF1+znYJl1nG5JQkv9//yCN2uuXuP7H3kHqWztkz2reHZlh\n",
       "Oa1esLlS91ujyduTDKKKytqoacjzHVmwwsJNdtYB/Dm5J36lhKdBTgNm2eS54RasqO5r/CVYix7u\n",
       "OQEV86QTd489j3s59qgkOmy+g6+41aSqctqdnCfRVQ42vtr8dZm1IAqp6aXX2R4g82Tvemy5/U8t\n",
       "EW2kJDrSyMBTTWyiM4kLMYEBnm7193q9Orqewr+a5PiIa2hfUY7NmpeScyLqxdfEQN16nPyNEHyu\n",
       "bip0ByI8CcPbnEgpc7Tk/bPkThtmYEANwDr11aZdpQJkQxy2l1giz2b5e+Aw5dBdKqQ63y8kqgr6\n",
       "jmX8RFZoc+kXHSJIGrbMXHxKdrPYAzaV0Ruqwxye1SM5dHDQkuLqh1X0h/5fB8Ji7+eEQFOi6bxA\n",
       "WTzHHo6lVu3pb6dvpqjRhEJZpEJJSt1ta6tLldi0JCWgqNf8goH5EbX+Q65otus1XBX8oRIp1jLn\n",
       "su6VIzbIS0nWbhMxtNwuJjH1c8WfMHLIaujPv0KU7iV46EmUdSt5C3fDP2z+QSSgorDHabIYSY2c\n",
       "L0nstASkiPJd+WROjxeKq3A/IGLc8kDxyOPfie7Hvd8/CuPF8ZsFvYsHJlawHm0rMmTMcn9Ms0gf\n",
       "wNM6WqnOpXB9ODZ4wvG7ApqFj1c54oaRSJSkbNc8Z5TVi2lXShts/iSwMMFyqcP/mbk0nqzeHENP\n",
       "bJDJMF8JbcTl3tvjyiuq+dh9liZjchp+oJFoCwCeda2yN1dVu6/oRRaYm4fiG55pG9vzxZxWa2ZE\n",
       "Zze1Nvt6VPEQx3EEt6RbDDv/GXXBQaR2h5CNorsih0cC81kAXsRqXLfKGxSXC6kFDRZsIZDCYEpd\n",
       "x05zWlobp/SntCNzwZwSA9ONtbkGM7JLvxaRrL2oWOdpQ4od98QnKDQuciqlsJ+ixBIR9IBk5J+A\n",
       "jed8/5sJQ9nBlkBt9B5ODjYsNA2BIORzsi+xiLl28dT+BnegFsGB2RieHuj7LjU08FXnPmCKt/0u\n",
       "k4iXxAqHPosN4fXReB9HGsFH8sxvRBLEgny1emf7c1t0cA7kkdRRlkFFhNpJGo/u42ng52AJnGba\n",
       "i9Xk7O82vNKzWIEMu6w4527e4mECc2hVW4wd/Kie/WPoT3BmPL+R7ndfgsux0itQnrn8izbpqpmY\n",
       "STJuv691KfgPbNJ2fdP58Mmonam+IUb1hHzKyiopa7r5g9vDU+JNhmncMVvFWWMhuYI0raSfHjsa\n",
       "zn4M5Z4Y9f3DpKy30prdqUD8wV3UmU1Kk822cFmVpG0VVSd9PIUVDY9mRQs0/Tj66DjW/YAELQAA\n",
       "IiIBnoRqQR8AAGva4cMldeQAAcTHK47x7dbrPXjuqKugAaKgwaUc63d8RmbeEGJnOA5EKODoKrgQ\n",
       "5/ARldva+13AZAsos3svCoPq6hzp5nSjV5wi7NOfka6VBneLOmILCr1sx1QIBLVGFpBxSm4DPFbt\n",
       "8wMtP5FHFmmQF1L5Q19gXrUGwE+THkqvi82NBAOU9DaQLkwDXxmCgpUK70CHTxvD0p6dBbrU0dx3\n",
       "SCDiEWWBo11q6jTyOT+F7e1zEiEdIcXwSq+3t1SV4LpGmjot1dKyS1ya+3U6/UU10TJQHyxHm7OE\n",
       "BoKVQlLrqWi8A5vh09er9jl56B3MgSS9ABevUJnwr4uS+NLAPwAX1Tf+vgxx7FkIcKWk19sNN5Vs\n",
       "KObgASedYaaU8Y2AjNhUgA5+VOr1y/KoUY7+2cunRMGwNr7fpXGbxn2HIYzUa+iBrPLAMqDBtl0l\n",
       "eGXIEqj+rFHb40dtcU6E93nWRrFHVshtv6wKlFv2kZe+cGEi2rvCZeVXMtKvuOxFa/F/O0+/uVP2\n",
       "hYo0GdkP3EL8B6oteWZGgrtHHRT9p/Hiv3L9766wMmulVRHYXolQWwH1v5r/Zm1pQ1enSJPh3wfn\n",
       "16vO6puJwCEScFJU63DkAvDCnWkc24wagi81M/jyab7/FIZ+jazJhvxcg7Ray/xrfy6T4ShGMNqU\n",
       "eQU0kDjj486/2Qopw7n7xg4ltfL3zVXu2jSvXAJ8QfNPdPjWzPsO+xjsq/19yXCImI071p/msjyw\n",
       "/sqZAcE+BfE/v4Oc9/70ow8Z6mL8q7xrBLWeh8kdd6+PfPRqG4hKXng8vxJvsV0gitCAbAzRlTJb\n",
       "B2keKg8Geq4hYRiGastrBCfTShSv4YcN7+uGMki6scgknRxTszuMfG/630xgotlaMaYnO5/52dU6\n",
       "Eab6IoGSHH8NdVMIcF00GFmoTMs2Y6FUBQWlf4S4FtTSNwwaO1frAikgRbchPcfRklwuYCteWRoz\n",
       "R5qGDlsMZpaWDH0hH2bsbWcJrDKfdWxjBMCiPu6DuRhxxfm2/hzJQjqEwYbX2V5xwo/d7LtR1GLT\n",
       "90yGkitFOxHJkApzrne9c2xVrbJvYYS/DntwAYX9nrPGC3wnop+vAt79g6geTrAFo/zFusODFsZD\n",
       "X6tvVe0IgJ3Hkg3uPv909HjDRBE9T09992T6637CK+45UGAvGsvjeqCC4ZgltBl6dPLzQjIY7Xul\n",
       "ZzHL7knPkxsJgUf19f9fCHr05JbzEwhgZYIfIyz6gniI1eobqWkLhaDawkK0HzcYuUaBDsWEOg2x\n",
       "P96RcyOChJSyBJlUVhKgkCSEsmpxVskJwClSTkjrr5xtRCsLJXX1VmmVswyNrMHLpl5nqPWXs4+E\n",
       "EuDznUH7J39mV+1j1e3eGiKav9XwdFTahAnAyojHSwGBn9/xsmdPWQtL/vqWpRN2vhE9kTSY/qaA\n",
       "lEjNjwIQgUt5vMA3W4r0CKLXBIbgDJSn2lLiZ9fatWpy8DkF+Kb75HduErHq2VeolQ1oxIhMVIEx\n",
       "jBKAVs97W1edxcz3McUyLg81mXWozyjHjihmdXysNEc5l4L7yvCFOcdzyeTbxkYfVQcBJ39GLhjJ\n",
       "mHu9RAl8S2mntasvuJihD9ONFS2f8ZeDiJ0VbUgErOn/+0BctOe5Qa/dqxM6WxCWBbj1IY7PvHRx\n",
       "ZDNhJwOhc8qpHBEnIbP7Eke8fqJIsksxAOB7emDQLRBu2091FBiT4OKoD2oj3zc2+czssv79yCyt\n",
       "OdUNYsAfUgpDd2Y02OJHqQqcMX134V3iL8Ijc6Er7E/sXJvZ2wTMnA2j3sBzhi6JWK5YhxDu+en6\n",
       "Nk8uGoi7989JHJJGNzME8itlbqUU8/dhsnXDYcyPLZS6NZFKZeojgdUsDbfriWQs5WNWzEIbLq1V\n",
       "80I+LudTJOBCZzI7bDn8RxFMryonxjatYEiiuBXb6YTFGfHt6Q6deu/1GDyebE1eFifbkbs8Z/Cl\n",
       "wvWuz9qXL5z0FOj2x29LtKtOuBkaYrbC8nv4sTjSF7TCfb2P+i6gC/HFirvGc3Cllrci6RgHUKXL\n",
       "w92+9PqjFQf96M337V0N73Hx1oDliqOEf4/f+kZCPJwn5hCwOYTBg1eogkx45KlPV4IvQ6GLW33K\n",
       "tA2vrdLLYTjTRIGWTU0+TRChuQact0cJdUh/OS77tnGHMRXxPtXDOSSlf0J3MlBsoWbAo07v2Nmw\n",
       "7H9fTDqnrBI2RPadOCkCu5lPaZuDdXvVbIOSO52uMFFRx6jXxlBUpQ0YZJgKukY8EgnS01ReJFx+\n",
       "t6VYnMVTTn68g4udGYz/FD/AW5hwBvkfYqyvn7A2nJcodAH/CXXuTBMqDksAy7A4ryMZA6RRodW8\n",
       "LdXKBYV+lGQtbKUQFJ1SWRvgYgvXgKf/OUOLMH1nEQD8RRGHMyWcni6Nd5p3RjcTkio0tZ33ONGw\n",
       "hrdW62mhYbI8eSpR9HHUu07vy+qkD3mJcW6dwd6K1VaEy5RZiXDuz+J3CdxnwJvcBatqIoVYaUS7\n",
       "BvDnp0SZNgTTOmxqvHf2MpbBB13RhDHEkU0bL1Lai4qpW3CCFcQ+B8fy1XEq0DJslelHuw8SdYGn\n",
       "NGC9imzBlAdsS5GhKWWd9KP3sZoWvcaJ9ilaWwvBB+SHnq0WDN3aKIMqUS/MF3GI44sA+MJhl1uP\n",
       "XAGpk8GH9DH7sM2GTa5qxD6Sdpll9h8FPAX+jujxCoKdgkcfBljAfND3k/ONFx1LE8NY4RisGFpK\n",
       "AuFfQsn6xOapYlpsRDZIglopd/yoK6ugexJs7w22RIrA6tkIcvCBHKRbwS/Ln8+W60PgJVNkLeLR\n",
       "eK3Kosk/SDWToZkTeYucdnR58zmu4kMlVkc81HNE157Yjnauf7WA4cp6Ub2RJ7kzD4v1IJUnsa6A\n",
       "Bq8tGJ4CQDOjy1nkJRKTQZGSXtmQTvwJtMSq0DqgkMaMfT4bxDTqutxNGyv532kolst1a/tuHHiJ\n",
       "noxTLKXq1su+gxX0B9HYF5r47PnL1I2/gTaVFjj+frS+NIA6olCFFZTIwwFcd7sEI46r2WU6Saa1\n",
       "aWTGNppoIOQkJ9PIuE+xxAcAbkDdWkWnhgNmCm3y2LjLm7Z3Ebu5NMq2Uxd1f+iXgeuwFGFo+F/e\n",
       "7AVclvAJp7SnTZh3gt//ODPhIpbz7h54jkr7v+PLZsj2hUJL8545qJ5BRWgO8Uq+Upf5c+nMouWC\n",
       "HR46flSynTFBuLik+4ticSI3ZaZFRNOjCZ74V4jDp706Trzu4C2rqDfkLR2WiMzVrUwfnB7zDIMI\n",
       "jR3hvt3Px6SvfUqCsJKOckXZRdj1lrSJ5E55LEUXvZLgf3kPNhyYzcONXd+OpDS5th+L/NMwGacM\n",
       "eJrAWgxL7oSEmAQQriK1m30rD7llLYKu4AnKDgeUDNFg4L6TQDe4ZnbV5B786u2dRZ8eYtut/l/9\n",
       "Gic2OC3/Epn3sw+e9K6NYf5pGL7y0WdxqHJMXftyPjycIWUHmWuFCDy3Nu8a3mcyTlXPxvaQhSNm\n",
       "kqsZXn2We1OE0Q9FKWXUtuD2qiCG+cRLX/+cOD6UAr+d5mvwNBd9TuP7m/07pUkeWfPkV958tkFC\n",
       "r25gPGiuFZhsoU7yICWg3ONvkL5jgZ8+UcI/9cifICC01e6ZebHoqnGsFFbPV1FS2FU9+0IodIcK\n",
       "9OiIzkC/kfcr7FH44rRBlM41+n10Ld+XLod2NgeD7Wq8uX6OTOP08IP8IDdB7iLu2XkJpcybeTfO\n",
       "M8i2DD0wOevIdc+uEcfOkYTKdwmDup6LmEmiIL2tAz2BkhkeYh0l6JWPQLdPAl8S0VbJnzqyy5Zh\n",
       "KTdwe+pwhilVJQjp9r0UN89epXaqrTFOFjDwIqzoyLFE0Ewj8KoGU6Q1vDCaIWhmjIJRQAX1NV0L\n",
       "aJb+qK3ol+CESqua+6Tp5lXqRSg+eEbGQyfaYmJBahs4INYaVKGTLwppnl5iz44qdB0UIgfo3/3n\n",
       "U0+51ql1ypswG1F0yVevkDfmf33AX0UP7SlWrpyrfVt51FE5G6YTCKYF2pA7QQu17Or4bxnoI42S\n",
       "RdlTSVpaxIib2xC6dDn5QadNGsJZQomS7RHK3ukddW+YvVng5/cZ9kuiqMcI6u+I7gcLKhYrUQJY\n",
       "vFe6aHoqRcY/9kIcw7KpDniQwoB1BWu+A0hNezJwGtUf5M3g7G/GbPPXkM5CNXQu8B7zvsJR6690\n",
       "+zmDpYPUoB7OFdSPZc8L8LGba9vg9ovR3wYBfnKaVeVYxqu1LljHsVeqm2wP7ZSRz4zb+zSG5Phj\n",
       "2xQwXloSKxk3Xm9xiJ243+vs/6YjJE5gA6Ey7+juPF08GCzRKDpvfwACnEjvUnuLnaVdEngbHwQ9\n",
       "dy5lWUoRne+SZ25A3FYhfFfY3Y1GBTmHZDM9m+8edP6oWuj4NlUpNdr+fgpz/GYTo36tuK9aeB/E\n",
       "4EYomZHzXjT/b8m1asIiNIEQfOXjXiBSz3EEPYX+n5VnehWBV5y6I7kZV3yX9nnPGjB1p4esNRRr\n",
       "OjZyrhWCIEAnLMb4WeSozMQm4kEOYbIbg1pa9fSL6QTUUBY7Anr7hZqHK7lF3TmniuDmtXKI7+TP\n",
       "9XLGSL8DzYPLkDuyVoxD8F8ETIqQASfM4wYw4pI1v/q4QnvsR5dWYvs4XspmBpTF+4TyUowGBI60\n",
       "jeNjqD8krWkg4puia0AGs0RKcxQtTiUjV3O5J/8AAV+B3MZxXQtX59+BuTGfju2h+x1fl3/dYh/7\n",
       "EXX3YSokW9DOIpJ+TskBPDIhIRJ2nPpf6Esdu1AqOV/ZEIzgzqLQkatKhJG2iWm4f/LZ9Dh/1dyo\n",
       "s+5TNLFurJ1aCowndpKYavQq5DMEXl2PowDrXUvqLeG7YfR7A6d67ZBhCwI9Fyzej93v6KnY56t3\n",
       "OJFJ7wCze5gHKhOLWXC+/F+bZn5Ka0c1+wAGWRSGPns6nyPEtHXxlp7Y7YoAU6VX995EEBATPs9n\n",
       "L1SmhvO9W7oqJEo892Gli6+nHBcvK8V4vurz0AeicC+RFFQ297ZGJJUpDYV/AFTA+n8X1j2Ry6U3\n",
       "1gO61mRbi8JU43uoNEbAhSXYBUAFG5s4iwjGfhSRmtNDT7XSM+LWI3oR9yLVv1j+KnfUrtR3BeGF\n",
       "LM3JNaHwc2r/194uCnGgs/rD9wnnMX/HZ1ghwQbsuCl3q4wIp4FsPVMNjqiJ48L8GInoxuxbZcpm\n",
       "7fYD+Ty3FDNMqgsgdOvIXNalm33+MNt7xaIFqpIyO+oAvWu6yP+SwMAccdupk/Z+BOvjEbebWQA1\n",
       "/6TyDTOI0KS3LuRym6CcgLE025lsWNoQlXoLnyOETI4nkhjhTpiB0qCi+mVOqlz1+wPnV5gokHMM\n",
       "yJ6WJVtXz7fZcvFBaAedsWhwDz7TufTw77ltNviZu3F2txbhO1NlQRVwCj6XgH3uvvIg84Umr6Tn\n",
       "HrT4KcfaBL/jeufj9KBLlSvSBlkuzqGLIpQUNW4IfmDxp17TayssftAmdO3bn8Luc3ywr8gFcaUn\n",
       "FBbRqMpsU532KRz75Anu0E9D5ECvQnD0nrgiEo6OU4mNa33uXlMiOd6XeX929Y3Zv/qjw1tBfG62\n",
       "qZeW558wVxM+xUiXAvQ3CtQiGUy5dI8fPzSvPIjQwWRsIrifxyu8AH2YkUC3BOLIVFHJCYuP7Agg\n",
       "ZsvXhXudN8acsFt8Kd32U46PhE40+IGXkRczMnREeWMi0WxYVs0tVlaMqYwsSBqKraegf5ndiJ1m\n",
       "X4hrzYA/dPcQ1XpyVsYU8zPckyTaUb6MLQpxQAoErM7SIZIuwhwIQLvUfuXc1c/90g57NTOxLgVK\n",
       "1gPoJOEXpjoivhdBh4+vfBfJJq0SnAw/nVEXUFu7kNPyruYEIaELOuhvDytubFIuvUQYaEVgPV9B\n",
       "qYif+XOF2OyMS3hAIXilTWpJhhwprxpB4+MSOe2wpzxR7Y68gUYgQvjq3+t6G7EGRKJ6QBPrXAzp\n",
       "LYtOnfE+QEl05Np6Yvd+Wvi8foXRsicMfCjcDnbUVBya3ywB02hSNUBaH0DnaLRCGk3IJTxhbd8l\n",
       "zPbi91DjbjAnRCzkPscNjs1Ctzd2Et5JBvfo9YpRvgOO7Z2SFYUc6vSVYzU/uBNhBxdF5VYgdN2Q\n",
       "fPpq3f7wLtgaXihZR7Y9weAm3iPAT+WtjR5PdSvvTersONYDwjYaaFB66dRyJmznIrwHN2RVfWjk\n",
       "ulW16w+IhCujD9JWGQApn3yYIVxEAct9nk51fPAIY/BnVEtY22uc/jZ2NTJ1FFFIWc4K3lU0qfSx\n",
       "ESnyG/iYYi/8SLVWggapExwjpvvFRqs3xD4XPDQtJbmU4Ug/s9mEVqG1qW295qtR29afJE6BmG10\n",
       "IotBE9Q7mXKXtBo6nS/kvTpVFaobra1o2GxD+O59Pcs5tlSpNPq4lekGGH5odDXwENtW/Q+1Xri1\n",
       "0Vnno7jGEgzROT9VdvcecYFiaB0z6TnW0JsZvwOKEs+0jF2kU64b/0BtoyMmCv+xDh3x2+RPpPCm\n",
       "dngr5Hxzh/bqTCw1BbEnVESjbAucHhj1S+zo9f10cz9+/Dpi7lmwmxn9iSNPY69Z76UwysVsVPud\n",
       "0YQksHKx5KTDUY+LmIhdhwHEDsSGHClbOi6tVVUgr1eTx1EhaEnaJz5Gq5fc6LWFQMMIKy32dXHJ\n",
       "eDIWxRBB1cMx02EpGVBs+Xah1pzxZqGQkL+MmOrkTzY75spi7s4IvVFiHtlwhRWkZ/PoicSA0y0w\n",
       "lUpa+fCqiAdznz723NXROchIqt+AE/qz4bcPYqH+38U2JcTdK1CMbZYyih/nXtzQq6vWdiDMNvvB\n",
       "YQg03VFhmsJSa8hDHDvCNEgwtXNWdh5oG6y7py/szBqIEIjjhmRPKAdDlC8EmZGuQ17pRsm73M9p\n",
       "Tvih4FXs02PCGzm9quK+5YeZsQxACSgiqkpDg+hebkfAIb2NxpGhA3QQ4VHh9bMu7TLBr0xsoeK/\n",
       "76u0Ji9eCGxJ3TjZ+0PbwskxqjVKhJS7NK3l7T11rO/ufNLY3dPMWLXxcAmgx2b9eW3SYsHm5Mui\n",
       "cDZGz2ZFCEoSUsgfvF4jE+rv6xOCQ1B14jWHaKRa5OPbza/p/EGUy7Xsi59DUV1XcPiZh3dxGHKi\n",
       "Wyg0litqPGpih5cKOmYlAltiGkrBzb/rhIehnr6xj8gWSVeg1jMZNG2C444mtf15hvBbMlfgJDee\n",
       "nUArPj8zporDWcI+VJzQDdxcoH1euQJLtxP2+SK1n+NFgqbouiTj6gzyaNbj8bg87/GbEoe918LJ\n",
       "nC7EaNwxsImILiSB/pgQF6OQM8IpE14UDqc4nF31kFV1Nj681AHavyVKa2cyordwiCKNb057s/Eh\n",
       "Uoq9R1gNOPMozH86IFoLNKWwcPwFLuM4bd+/hrxWhqARJFpIA6qvFiB+RgqtXaHfPAR//ExFMCeu\n",
       "CzMLTCRvI+AQNIeM1y9bQudjZDw0wJu7C0eB6fo/kXCWsBjlFNojfjPCpJD1ZD/qKwgpsq5fjNa2\n",
       "NnT7UBRhf+jO5bvplsb0+z6KcIXH0yRlPSkrsR/BuNcs9Z4GOYjBrXIOwHjjKXS02jGzx2wX0Pj4\n",
       "VbSVyU2u4LVeJtDGtK7u0+qUYowTvntfXRgIHfnKbMd7x8fGnymHq8DOdm/LCyMESAzS3zCji4wT\n",
       "9zsbvgMuZSHdh6yq9ZXhLWU0fTTPkIMzaxI8mRoPhy6jf6ozAxO7eDtlEfqlQVH8c5JukyT2Ymjr\n",
       "aWG2eTG8WpUzYmGtRSBIeK2S40Y2jqYQwTD+CcdAEBK9EdwBpcog0KtLD0I8bM345eOD0oLBT53p\n",
       "2BaOevo3IgvIOlNLKsrAe+l4jr27WbVNK284z61lFB/gTL9gN3my9wKEYJVYOYzzlG0hD9FuT6Cv\n",
       "SCv++eUsdRN1/FBNP/TSqEuuPaBw2GGIyMbccKVo1ZF4v0K4xB2f8HjB77q7BzfTcF1jQtcaljWE\n",
       "SKIGBhv4cJ2jVYAptPe/2q3S0LrZP993fO2cxjyIXTEBffq6+YXBb8+RmGcANumCCzp7NP6XjPR0\n",
       "DHEd/7llWjpoiqpmlb6qeq4LeWwRXmvbrzcQbwUt9qHs62AxrX3a4Ncq9s2n69vjUWEDAjoFBmh6\n",
       "6SfNareODM7MeVQ1yIlwsTfX1lfFjKKOhlYnykLp/GH2PH8co9lm6ygeseg/wCuSRkzq1tNNBL/0\n",
       "kZ4UeWt4yvecnqWKEheuf8Y+LTFkR5Ii5qo0pt4fSqgchfwvgOVGCRd3f1dttwwG+Xkj4XC2c12m\n",
       "g8xlfvub4krLMzKeXTm4wOplil3z43Unk4cuAYj3sEScGqKPWdpG4PYZIGMWcS8E/WQSTnpJRw8M\n",
       "5JPeInoq/Fnr5/b6we7pI2Kf6JxaXwjYGYlNH7aXHyFCFwpW0SWpDeW+9zgJpNe0O4dykPRDK8/r\n",
       "gF9+m8iDS32py3PEoVRI3kv48q2xFNQhc9LEW3i1UeVE+p0lpLrQ7zRsEqAqwMZls9sHuNpEwpqB\n",
       "UQHL08Hgvgdy3gxn7WtHp2z1ruksxP+WJEURCsvlC2hSoIS+oYOC8d32+q1OizHDq2ORN61zpVm4\n",
       "ByaeOn8O6bWS0nb1bY7aF5Rydzw73mZeS65ERB3Z/4oR3jFT15VS7B59kOdvJ5X372urex34DjIy\n",
       "Et0jujQolnVGynUaNflPJekQm92/ktGOpzhBWrRL8lQB7DKPyYB8tUGjuiQE680SHKW9dc5KEHDq\n",
       "sR/GuX1ZCMeR8EdlkRXAJrNi62q2NI+oAeXqUsqawAjuV32RY5YIZnGRgG+mIkC5XCiSiaN2vV5j\n",
       "FXBfsHMs5IP5adU8aDIzn+PE66u+NfJgvsWF/gNavSvcLhG0qxpQ6c+DauzrEFKm8IAfke6Zk9cX\n",
       "cjJicHRV3tGoX8r5UlRrt17Gpi1xt9PUz5OnuspeSePnQg7gLb14RQjq/XXqHpPpjJyJIO6IlQ5F\n",
       "b9AntMtFZWYigeAAOLRB7tS/PGcYKdeunmVr2PjUUPbY48aI5grAAjLT/zUBmDi/AI0etigGT0bW\n",
       "vNxe2sx6b6IVWHkwJV6amsS4Q4ly8FAF8g0JT3ejIpb9Ti3BSLm/pW/W4DJtsZvW9uxsR6Mvxj+e\n",
       "tsCx4m7uEJ6VIIiGL3BZAynLXTFt9x+fzbitd9cNGn8joV/ZlL3S3Na5GnvfmDCQWtWoEFWAEFk0\n",
       "dA3CH9y82TbiFNoHExHNTpcjZH04nWLJuJl+l7veHC1v1NzXOGUCFFEf2MOGBdWBVSgyh7alsvZc\n",
       "9JTpxsaQ6ZqJCVAcZlVSlbqS1QO1EqaTkEKfhNZ2mW3DJnt8zFYSI+rpIjSNVyUvpSbHuubH0Pdb\n",
       "dzsixB8hBSid1w8qedfVa2lAuqHmHtGeyWtKqqR0QLa18Rr427f3TO2UT4DvddPPr/tWsugS+JWi\n",
       "bUySoaOar/rzvyeQF+Z/6hqhGzrpnFkHY21fm9slZkxM/N2hdIktP/YAAG003+RINCHjYQOw2UIH\n",
       "bKyPjLvSZ4YOChPtw7ms+oUEOUmYHflzofvTFRkp5zmuLtHBfbsM1qVX4hle2G8GJqtX5OdAn2z7\n",
       "z7vNMxKiVv/9z7/IPvJh/4XkI7CItcdR7heltziatb8aCSDuUNkVzkCpuStgZzpSgZBSFpYH8hzY\n",
       "L0PH/Wk3aNxDU74LghZ6Tm6qcfZnjLygV4hoQu5AapAPLSHZYa/f6DFYFzxYY/VXwijffwtXCwjT\n",
       "rsqZhn6i73oCYdHqYimoq35muJew40bNXJf7C6XVXrzvHfrPp03o2PJsh5FwGzp7RlAsjByMnKkf\n",
       "98FvmNOkJFoNwQ2uiK4kFNG1PqEsqk2CBaFAC2OV5wEeeHcPxSzU+FOpr3V6FfK5WuzhUUgTrDoc\n",
       "qHSn+raTAuzYS11RcWsow17BmfjoPZkTfvWT0qV7qmQdSQ9/y1EEEGjeGAEb7u+P1W65QCYJaOIz\n",
       "prwXcp28XLGZONFl1GFCdXvBs5ZXQkJrJvcWAUjUkRPlUu5Ryltcv511hh38t9o+znbFu5ttVRN7\n",
       "QRcavWbDvl0NdCsKQ1eo9N0APnGFwoPOu3HIXrv4BP3Expv25CG4onwwQyj6ewwfixQBXzuZQg0t\n",
       "88ED+SV2d4jvUM2Y2IilA91xL+9i0gtUGZoynvdaQqgRk8MfpATOzqQ/ykgU070teFz5/lm5ycRg\n",
       "u0uwtKwSFg67mRBs+fV4rk9xPPu/05zYpU76L06WhDLLBLzPfz/rfaE5egjj9lVNHUSUwLUv889b\n",
       "V0zOMiltFSzuW0cm7FHA1H4m2hQ0BGLBnXwTz5DncNk0kd63WcycrQJEK4TNXwV/meAiKEbsPnG1\n",
       "HF0yT0b0qYrZ9Jkwtc6Yc/To5qNN7Y9X9ga7KdfjI2hREeYbEXMGdRZnoFDay3f7Iz2zUS50wgYe\n",
       "eRql2o3SsL+b46V7DXXvqkYdguz+OAtkskSQ1ldLkRwX3X08ouZRw7H0j6njZ/0wGazZeL4nrLXD\n",
       "+/xZ4aZM/h1Bzu36OSDSlveF//izxFC2qJfK5OpCg1mtWYsoTScymG362oRj/hqzMBAuNokkSDGP\n",
       "AwW+XFAlC2tpvQ8I9ZYDTedHf9SzHr6+MLhLJt+F5lcplQ+Gi8dbuRzpERNY9cZFiAJWl/wwdph3\n",
       "cbfPwlvqZSHHIQMsuECkAnUEnjNSVHwFzVJTq7hT9YudEGLQcyhtvpE3KAZKBuUjMMW+lz73wx0R\n",
       "J1ctCd8+XkApnoDT+X9cpknY6YeQNL0e1jd2DOv4kAF7zhNUoaFAmsnWT7WJdMXEVTI3QGXkdRoE\n",
       "cxi1u2FWGEUu7A3rB2vgEOEVPN/uLDgTlLnVVXc+QIhc8TrXkKQngCLcPmUNH6YPRKXsAkkVdNYI\n",
       "N4QZ5IKNUZ2P0Sf/uP4upI4tuoZDXZsc8Icw6LSpEqIKdEIRIvY/lpCMby6M29BmI/Mkl4QpXmp2\n",
       "MzXDID3IfHvRmnF1BboSV4Gy2OnuyjEteCiPR48soAJoUuPl6qvgx3XRCFfluQiG6clS6rbKykKc\n",
       "RI8E0o2uKbSW0fxb6Sb+ZdVgw62j6INFNWwDOVjD5tZj7UiqDi6Ny7FO2rjqpBYPZlG7GbW3YVoT\n",
       "m/hAdevV0B3j8x++fYp5cH+qHad+JR7s6NZGC2TtcCauiQrst3rqLOUr8nszWNN3xDGXg9tdfud4\n",
       "qkWn/s94XEDQkgcwaenLOJ9lc5jT9x+pJRdPYAyFR5KE7IH9OPooAcqg/mWPYEKm00edUtBN5dMC\n",
       "PQ1lLBZoeLsngE5ry3Yf55QdDseCvxOHjOWzbQBY+UwfE0c60K9ake2OXnjy6aZrTGOr5fRnxhkE\n",
       "m9WMM3tuRod2uEj6kMC430iK6Ny0bcsvWwbkX0Xs34mAuR9EBy/Gxl+GrhG4arz7lAAp25HHV8S5\n",
       "q9N7hnZU5CbkRNjA/LCQoryrH+5fwjXzyMEAV1FBWlQfZB1YmVsAcGOHZhIebEdtuOPDxhuFA8Aj\n",
       "zBH8q5ycEuGpJ23Mi95YgSoA3/W050ah3IxqopE39bG6PoOtgcoHYG6WShtXDcoO+BB3+KjecKds\n",
       "RMBIDgWgDx2XGU2iWqggOQL8n31vkMXz3fGuIH95bMSylXea0N3/xLBdzvp8IPPfgggs7VtxNtQX\n",
       "1oSA1o05+huG+gwKhuaHEf3l3qwqnleHR/r8jdalWKDoKYqhdZh4qULlA/tbfxMXwxA2ItmyfHCV\n",
       "V6oKD14C7myil2ABkcuSWACkgQAAHplBmodJqEFomUwU8Ef//rUqgAA6ShFb9QAXbL0XJGcxKuA7\n",
       "mQ2KQ9Ly71Xu1tnS5BmQtfh22uIqZGqlrwJanrDXSFd3An5LQ6kKfDQfAS8LHmf2obhxgWlPMMIP\n",
       "2Xyf4rhlkJFy7NBS0am+Kx4phwr6tq9Sw3Fk5I6tI52jDcTyNxrX4mxX2RLkqVYoUFOjjisAqZOG\n",
       "P0jpsdbOpNK8tX6avmgHUJ+rrWc6KR92i+azYc5L+H4pQGMfduO3xsojo5FtNYX8yWVXTG2hMRP7\n",
       "TWNAA0xK8L2Tocwd1f9iv1WbH1+sdk62tKz4Fg6iCH47GeUcy09u0aXN0l6GTCdYaHaczDdNymXc\n",
       "IswL9GJnppidqhR7UxLnOnc5j2cQRSiIN4Zds6zGvJM6zUcl+SRia7R/ySSeDZ5CA9jbcEkz0T0Y\n",
       "pt6XGgR7X42536q7RJPxUaq1q7k35TggncwQMgz9z9NUtXR0HrIycCM+YP7+m9CBF1zX51JvCZDT\n",
       "nGSC7tDflwGQkvI6pegZZcHgA+pQPH3I2JB4eNsfcZ4FtJchPTMLMNLPl+81XzED6vRh6F1gCptP\n",
       "3Xgg3PFdxXlDudEd8RZK4OEizqGqEk8Hb3HKt+fdear6GSz9nOltRhXxx6GfPZmD05cBB4zOIiHZ\n",
       "9v8PqVCPu8deRlCLjwSX4wFxHjdZFhh0Gy/0ot8AGOG4ErafRkPTJTkKHjwPXV9tOSRcycMH0HAl\n",
       "bMHDjfrN/lFfO+cvGJMMDAbl/6btCg/6YjILsh+Gpf3HSjOqTpVn7oyI16EVz4jTOqrxOgOeBb1I\n",
       "c05ZrWrc6JtFZ+Ki6drL902iGuPFHe44yidsRg6I/D0tuRdqVqoNSsqnvyOlqYPAf3dxUFyXKFc7\n",
       "M5nu1aR2ccZnNL+fpu47TZG8xc7kcDH/aWAuXBZOdurit3zsP1LqoGw5U7dDJz578+yiYSbyBcwH\n",
       "ZX1MWrczloNSMYf2xvn9Jt3w6bhER2rtcGBh/LtKKMIRGbkVv1Ej5P6isgJhFnccIJsZEG7NPwGH\n",
       "I6yV5y8LCCPUk78tHBeK1LVnwNELE0ojw1nA9U95qtnFKiN1AVGjjbvHijQKmzng9oVc5GP+lz+f\n",
       "6aVjp11b8V+3UgQFaT0UnqSKMEX8/G6FHtDq6aLM4CgoNu7UOF2jkygd8aoGmL35trq1fqmoZFT5\n",
       "RZCzAdxYNuxRAC5FgiCp2N9aKvZ5xuMyzcOMwr4bk8SvzY/UQYEzVZJIMTEY0fbqB2tlovl47exj\n",
       "b0GTp/UVk2SqAKOY8U3aCOf5jNdJx3mYWxu36wu4FGAxo8h+lg0D7pzGhAvsOaPYuOcmJK8TVmTz\n",
       "2oZoRMIBGL1gz8mRLDQTSsJIGD1uC1p7DHcC+vKJ5wQOAb0q+q3U87lGNHiG0q5F29dgTdwWg9W6\n",
       "78rWm73TUVGesHg9NQXwQTn9g+Ek4i1sFpN5dK+IanBq68JoiEfJU9UQV7Uv2Jd8w7mI5XRvS0yo\n",
       "zJXmvCJsfoPgesCitI7WZWwWzfO86Kim1T37XrKuRBM+Ft8nmN+XxAAALQ98AjH0cWQAXWXHq+JL\n",
       "fYPogq0VPgHxdVQTWAgeOx7gvFunv9RKI4ov0udBJoMi3LFt8HYr8Ggh8EMuihjt/f72cxcNBg1O\n",
       "6zNmi9aLLdIAP8iDrTFhVakcRGmOa0j7ieZqWxbCRVLAy8Sb1ZV6Q9E779bDoYXN/7g97Fm46JI+\n",
       "Yjn9ghKIC3bzS3rYxFPq9eThOAr3VbODgTzELV3UPwFHaRIRfI6HQ3W6DuZQ8q06aCYuNNVTH/+G\n",
       "omK9u1zQA3iBv63pRuL/oorUrmGmsTyTVki1mepwutEh6PEmWzyL4iMcjgBTuwip8chgkLYWJNbV\n",
       "5HklmJUS+TND8HeBncHq5IWmU7yeiId+R6GPGnRi9wFFzOjy/RfrWWvjXdNVb1XI2h8H6e7OR42x\n",
       "925cWcw8Ab44hwNRbczJe+OwtLfXi5yiV88wOK3YD6O7StNnFsNVcdpSM0yeRvh9Z/GoFsKZnFgT\n",
       "oGyu75mbM4Ttuq9/a4UW8iyQTsw5TlYBNTsyfQm/45XudgP3gjoQZ2MaSd0KMTcHmKDsSePeQIBR\n",
       "Fx1IFlVJuXEbnHny9jYSzJ/tndcs4/HzjQvIQoXcn+IAtBB2SSJjeamqbpS6wOqV0rNhgvOCiE/s\n",
       "R+I+bsYnxPIptUZvQzz9yEiJAzrk52UUJhW4BQlPAmq0Unv7EdANVE0WL8/QauZrznJ6wf5NfKU2\n",
       "L+/1KU8Dq3UOvU+/2QD/owRhthgzm1LEdVW/SneLs6zlGlgqilM/11zBsUDWn2d0n79KX51JkzpV\n",
       "/meN/Hx32NFetq19IYaTqafNFeTxcQTZKYUlTYuYSN2HU0nFQEPB13Vn0vCHZtFw/Am+Bt3VZhT/\n",
       "c6hI5l+EdYCL0Gk1XFo3QE0R6i/jfuEk+3hvY5JdCDSsGo4GC5RDWWnJ4vJ6MhL5+zOJ0Y5A2qMS\n",
       "39fOatu7AZI8paK4BWLsKZUvGVMHl5V3vuTcmcEEuWj7x637dDSTStsOIgJmtPM7sQqCI4Bf4Vq2\n",
       "+zzKss5yFzGMheTNiZEMW9yHfPG8u5gMarNBwOokKielDk7B8PeNIHZGse0UVz7OUz2JD76y3c2Q\n",
       "n2+QHu1+al9IFi6MhIrYwu2WRho5rzLMJrI9dmnxXCn6baFtcayWIGA26jnl1CIQ9ARyzNKFfn/E\n",
       "9tWyE1+HB5AAqfeHpuj3l1vT7Y7FFF3R5cmtWBnKYLEWIUjbGzWXeE+Lgoa6Mqt1RS3mHrWqO6QT\n",
       "HzsVB+qCdQdFRrS4Dt1pRke/zRTauXixN3YhWQn6fT5zIw6SuZbAsgyoVQo7EYqdQJJpMQORI6if\n",
       "5jHKtMGMQg7YCw4Pft63GsyXqauSUgQuWR8c2gEqXBiEufbJzVmjjBdNaQ2JQYD5u5D+eio7J7Jq\n",
       "tyDdGi+OLu3NzzBrLCpW3FdYxHDSrRdCWg4p3I/ePrfcEFHbndVvJY59zLTPOd9kZsQhCFnJVsUX\n",
       "PcYMRfxYIRfXcpGwO2PgpXsCHINDdwUief4E6sc3rCUIjY5URSD0HZ5u6xTBgLRZJc2ikouLH3iQ\n",
       "yb7Ze/qfUDLpdmE0B0i8h9D1XrLXtgGlU6e9YGtVCHCVW+uoouKvh9GYrs4W0qNdtIPTImOvr8CC\n",
       "dEN/27Wc3CwWdnpqJbQB5BknrGT3GmElFjv5xAK4q9llNM7i9oyZhue4x0i3ygNBrC0RCnrhn3Tv\n",
       "2U6td53TrgmyftxbXcPFFb7cy/ne7r59NbDtF1oafm9750IolNau4w3ltJeZUPaNusg0+3A0kBU0\n",
       "YyU3PjSBRJfDy/tHo4KVDnFECLTfVVP2/rkuIZ7nhPF4AScQevljs//3lJZ9PabEQhw6BUJZGDLL\n",
       "uxlHb7pF68Q/Kc2GMZHkyO/LROlF/cGba02Wdlxtwf1dvfPDjC22jG6D+Opk96y61Rh0Op+tVTUf\n",
       "0SvLnuhM/2t0K7JV82vBofjhW8V+nwSL/PiyWoJCnFqB41E6keFsAAaqaWSFhtvNK2v0BREvER2B\n",
       "QSyPXjqE40igMBxkiX4A8H8Kn/Z3w5e0HU1+ZXlOqw25ywZW4Eq5zfJ264ed/XOaLa+++IdXzpLl\n",
       "qsZ4jqxk60S/NKTEPZmeNpXx7knuiQ78UMj5fj6m50zmL2/v9U3GrAqBoOGKDUT+pGsb3bsBSqxr\n",
       "YtLXCzX+5QB547p6qdjShmqUR0xeuHwIFuYGxPtyx12o2pH2Mndj4G0bcqM7gY8SRWE8AULaMbFB\n",
       "xwxtQgpx2aFRpgI7ev1I2tkz6RicMbRrXgemfVAh9shsU4fhTReA/DuMaDATG/ASkF2dJ32a+XYQ\n",
       "yUSFFtUkj7ok8hA69YZwPhlFnroex71fYa67J6O5TV+Z/PCOVRTHhLbRl+q/lsHT71j/idiZ9A/T\n",
       "vNTzvSiCjPKxEnnGZ7WbWJtS0e6IjJYYQjrg7fuf5SpE+R3gQcRQ0j0WjMj2vxWrszTjRqGptoaE\n",
       "LddhVH/4SPW+quEUjb16EDto2R/TESqiIG/NcgQEmqDHnFEFfVPdpz/nzcbZy1mnvDMr2dBvULKm\n",
       "Z25foaCNE1JWJTLpODp+GT4d4gjC6u9b0WMiR+8grFkIB+Gmfzq/643kYooiuisrgPsYm+gj+4rx\n",
       "CcPkVa2De+qh9eCeWyH2StHUxjZPVFvLFTHvwIgnrbfzMkxfRaLuHUZLhrZMJcIZvrpmW3Lk2SN7\n",
       "H7QnjEMRn5Xv4kbsC5QubCevJJTPh4LvtAMSFBwwv3TEXtt0pCbIZEabq+4yVNY5tv175+6k93ng\n",
       "nINvujZYEXit1VG4E+BNO8EIyCNIiJ7ZADr/d11SRBp97f3UpG+dNQoM413GpFRtl4sJoEEQFYxW\n",
       "qcln0qP8AjBWiF9ppW+36ef7pStBl0OR3T3n3fsMbf5OwiQKRdl5GExyWYBTTdTtj5Rlypq7ohVO\n",
       "+0K+K2RfM2ParR8cxCKi8A/Hc/aSCcXRcLPN/e0aeiu5XQl8fpQHK2R0c14Q/ND8puKtG7aaP4FB\n",
       "irqh3Kn1VnRbfrqWpCt0eJUGfCAXB8OfQODjX2+03B8xad1V8fmMvP8aDbqcd1dGPqRsS9UXQfqK\n",
       "wy5dEJSb5fp0J3Ytu3uZ4EcS9/ZvMzcS8lKIu1IwTj6VWayKXQ+XSPJAYNdr31zsA5q5hbNIQhHS\n",
       "uKg+KE//eK+HzP+o4I67Al0BhadwQnDopv9F7XUQEjXfhQl6kZeHE4V1oDSymsbK7hGhwzCXKjqb\n",
       "+/b56RqREWlkmcbTd6Wj8jQOooKhxPBBKve9I0LDUXBaxMYsBhNI6ieUa0YfwvyfaCOr1X8MGKrc\n",
       "ylCpABRilvf+dxKvNBR4SO8EyVHOT34vox/QETp7WCTVVrIye+3dGhL4Qun7LILBYXR6I8TjA5xm\n",
       "wa1+PWgcf+lECluv/YDIw7S+ex82acCRK+1kZK97ySVxieaDhEh3UxtPvn3CFjEw9HhNd/q0kGfv\n",
       "R3Rgboj3u1GAE1KXHEacZSmIzhJGmWDx01ahF1eTsCYbVgX8nJzmIqE92DDxWfgBuTT0GNRH4m+v\n",
       "u1RU8HaDYbKIN5ChcFUGrSXkE8m4mHeA+n1RFU6bsGcEgrvBKWGCR3lJbk60dvj299feKOf4wd3O\n",
       "keh3j4ilMvHGq9R02XEGhEAUXdH19iMdot1IfHEuajhi56ySN4uthAtszoQwIiC9teLQRb+ze2wh\n",
       "vND4z930g98PHGtZoBh1iI2qknEOOPHWBBBMwJ9/Z0gvWsloDoPUWdCEi534vgnZ3z6odHLSMDZB\n",
       "njo1z5cJ9zzkHz/XNMbBXaa4/BYzeuEOtt/2PhJBR/YsX7IxNoWXqmIrdLbJxw55oq8fa01teuUf\n",
       "sZ+tX6YtJmu1BBREESLZSKZkOcdUsvsAl/wQ4C4gobwj0iQ19syL+w2V+Ue9Fvxcb8ioLoDUU6Pn\n",
       "Uf91g+Lb95ywYYLT0Q7I/qaHLECfTqYg6cCoSWWvPrurIFPY4ogReK92X4CcnNL37CPx96+GfR1m\n",
       "FkoiB1ByH9lDnrX539pglnW3JYe4zXfNdWWmUhcmeVNTji4a6fBPAp52s7LUd+0SgWy+Rx4APAM5\n",
       "uPSkOLYDlvHTabafQFbD9Ocv9ZbNnwLXepVgJ2m+sDn1YVE3uCKmj0ZJxSoXLdwr+OH7DI3k9RxB\n",
       "rbvBdcp4zBUO7IjEDQAild9LrQyhxeruaUJvHPBrI/A9hPouzm1kPvMnrvhZPNX2AgVFmRlD5vSy\n",
       "8QEmzFRJ3rx47aLA57G3c9Ex23oioGQwnrI1gDHpV2587MwKKuWT7NgWp0wKN3GxoDQ/2fU/QcvE\n",
       "ZfVwZVFgCOEF74rYvPq/j5EN8krXGfKZDd/C5bvVSLXfwATazXcKp9ohaK97EwsSScvz0ukqZsjO\n",
       "gaNhXvHnFNS/7xuUV+aIzXpaAcx7qILVLPcwPocHYB5KIv//A5ssopc/DOVTtyojAYKU9FQwPqnd\n",
       "rxRBPW17S4wGxFAu40GdgwqMx+Z/zlCroFytpI+oEFT8w1daYFjBmlNZuRIGge0paAdp79q352f2\n",
       "2Q07/ZgrZ3emTojxzGlbPwGUZiOnCq0MBVjgS0kg9YqR5qQOxktuZME8nkMZ6DrOpo5lp5/VpCdl\n",
       "OJI/2/x06rMHnSpsFVNKsOlqzcQcT2SWPjClz2QE4mxqLtkyG36dXqp8tEtfFiq6QD88g24GfrIT\n",
       "XurpkuuLCHvpJfEEbsV68ROJffoZSsudOZRKCa7XHYiARUjAhs151uTJa7VpHNrVJ9OJKmFgpVMt\n",
       "1gT56rovkMQ/WiPRc3HRjqYLgH5Y5JaKmxguWErp++CWaahAAaLDECtN9hD0y16B8UuDK4t4WODn\n",
       "+MJj92cqdmTqjXTKYGxu/FIwa7037KuGscJylcRavP2QCpgeFlmbJHQcfE8B2M8VZgjzNZNe7t2R\n",
       "Jw5sGpifSbp3AWcZy7Pgzw+RHXr6cx50zQZMIp4C57wLaZw3E6r3955AO1HEoRcgP5rn3IQsRy/c\n",
       "X5cugMxPykckT2TuHVYh3Im0/u7eckh6QMyefo6TSGa/zZYgToS+Gq6gz3dDwpVWJ9pIqfkMg23J\n",
       "OE+tZTelzWcZofkN7mx/yC37HkN0GNsKpnSYfk9+ctw101THd1FN6mO7MqAyPbXKN9dzLJKuj5vE\n",
       "sgDpCzTXe0BfvIy94UJqYpXXHDxvb/q8g4r/P/PvkDllsN2Zg5gz4DC7Zo3yHsmtftSMwL/Lg6JT\n",
       "4uQH708jn/wDYYIwv+JHfwQiUw2RRBUKa3bcKUqHjISMRVjCZ4IfrSWPqOdb8llwz9nQ9yhKo6UT\n",
       "YdycsETCjOQt0k4ADmA1cd7IQ5ghFCho9A2/A3ZtVWOCw3W0Ok3mvw9V5YX6WaBXdISzcVsXvjAI\n",
       "1bjv4oioufLxNeremeRk31Qs+UrBrx/D9OtcFZEN2Q51TfPjH0ffnk4sSSKLL1CwImTxYqTFzppw\n",
       "vbNP85n95Wucdx4mLOTUf2qoqNOlLqG1fCkMHOHGpMLdoRYhz7b8gUS5Xxb3t2w8Kkzb+dml0NNe\n",
       "VJwTz7PZW+dgmE+kbx8EMF+lXCUCX4/n0aJWWRRBSoCCQNesl/ObjchZfy8iWZSO/7j1CxgB6vp0\n",
       "uD8qrGZMGi1mL0PQJizU03BBzy8MEaH3GrU+LzYnGhi0yY66VxB07XsiXENDLMcGzLxT2PdTIMVq\n",
       "KDUMNrNm87UPO/6wddmVxIXBVPxQQBGOPdUpfI4xn1v9XBrJvrKkhUPFGAXjLMvADBT5CbrpLMLf\n",
       "NmRaK3IZ+V+3cbFIr8UssgmtHdQgADKq/2D6plDV3lgUS6dVSWDQ5I8CO7tc5Ss98qylq+jHN5PF\n",
       "Y9xHHVprQFQ9GqTCP4DE3LPB1gSxG6Ejk0FTSnwK6y8dUs2YYsGq3ThHQ3Qli+f24aaG4bU33Wo6\n",
       "1SK3uqNBw0gaYg1hMcuch3TmCswlIXtHELpzNEwCLRgSr6D5HcReKrAU9pYtGsIvQIdPFP8Jwg3t\n",
       "blB/wkruy3ptN3EuWclYoFzjsj1FzsVdDuEVGvet6fZQdCSWfem18IHb+xOfn0v2Ngfgzzm0ZeyL\n",
       "inpynLLlhZToN+QuMsNBOklo5nNhBjg1p/jH1qJZk1A5myC0LMxf06G2pbXmkixq3D+d55YkZNJy\n",
       "lRDHLmv3ez2MZUxMubZm60EEWLdX3zu8Wf4KplL29clYcvBaq7JwGS1EVZ7lgkF94TRKHlCx+BPa\n",
       "8SG9zw9pCOUp/W3aOZZd+Zddw6c8j1LqJLyIJi59e5RpElNMEypk8G8DGILtnwr3k5NS9msMThvb\n",
       "RzUY5+y7AbI4d80mRs3kup8U30H3HOHnd7EKaTHjqgLqqlZoSuIy/CAZxcX8IP6ucqIi5Iw79BXf\n",
       "ttpNl09QOAP/Y5pvwbUlc3QkJMEJyK25ZdC33yHAACqwnbs4oY5sHJZFCCtYnn9o6Mo6PTiqHBJK\n",
       "MR4s1R8KM+lN1n6dmezMzuMzxEk6taUPZJMFuAPehsn2QzgWW1xb5DGNcNWxv8O0V1VSYJWloFxW\n",
       "K823VYHcbGxYOCxyM1ZdlCYVXYraNEBqDIA7teCjuWS9QndObewFYpCw73TNrSaUOC3Er2FeRi94\n",
       "8HQb7ZJJxJEaXWQ7LkSROZ1bMq6+4dflVnqvw5Qa/ZuxROGz6yMVREF33tUdql0WOT2qOUyBG7+Y\n",
       "Bx+s0P6u7jANyy4V+sUzGTO9Ru9JJO+Rw34Kct69UEuUN09wbSdYdtOe62oYCYE6/Sx/eAwo1jd0\n",
       "kzsZ4aGZQsi2vDb9306+S947p6CCDSB7IwNtUPqQeF/NGbKz5kmcpzdlEXBW1GM74z2zQw973I/S\n",
       "LdIQB7dvWcUpu02YGB66/c4zAxQTkBpvPFKpw4bP2y5PklP+hGs/91e8NhevRJETc79jHO+pC9WR\n",
       "JiNU85OK0gvoqM23jiQp4yjvmXtDFDo+k00evaWagunI5wWyTgZ5cwi1RVWCelevnFe1sYER2Ujo\n",
       "OnsR4oS+31lXmAoHtKDjfbkdOcGzmIe4fO+K8TKTttQZQFLJ+ZsDsVkB9Dauby7xR+Q0BlktBnpx\n",
       "D1WH8BqYpZB2qprtuwCTGj/4sslY21Yq+fKMv9pcJuirifapqxPQ7c6NDN0llVsc/vEYg6oUp8zK\n",
       "fJIIRt9P64EpRlPH2PFC/TxbZULEO0EuJO22SUhdKBx+NY05QjBlrfjCW3j+KBImhpRhxnDBO4vc\n",
       "f43VdX90AWpZd7e3f5s9D5EsganamorX96YHl3bDGZB6x0YvL8uB5Sb2kglfvnrNZ3ncBkFpAlpF\n",
       "aPEaewzNAlWUHXgWDMslp+R/QYLRr87pNuyUQmlFwPfowcF9MUjLirdASW44dekkBUiynO1fbNmn\n",
       "dUgN7npLeiAt/UIlyhM+mAnvHLeU4fe79aveqdwU/wgafE8Gh+ED5Mps/8fVDkHdHdzzNZMVJAAr\n",
       "MnInZ9u1Wr+nMvLCx6mtWXWuNhdFNB6NDmxUtYjLonpbh6/Ipzhd2N2hzeEE29RmakeXiSGYt6g1\n",
       "P9dBez1zBo6xyRDm66jJbS9STqZ6XRDC/LwaMFo6ZIz8xoRceSfhusMuhs5Em8uQr6haUNz/PoCn\n",
       "nysxYPxPmwG1FIJnoTtVWQxmzFfq989g3RYx//x4eS9fPJB9uIG7s9vevCf3gO4/oaHori45CfVi\n",
       "77JQCJltO0wWcHdQjlFVc020+rRPiLNfs8bXYhDhTapScPSN4DofDN7RFgYOfk4DB2AwKaOBKwHz\n",
       "oKqIdcQz3/gHqkYiFEpTmhqCvdzWtBPzu7F5dD5u6b3loQ2dGCNEA0F/fya2wc1dYuuA7fFaFJ9/\n",
       "BVhxVeQg4CZxX66MEGLlugV6LGWRvcGiRIRCZFXN0b+cFpmB3MvHf/jzkmcfs+y8njlm5rgpvIc0\n",
       "WNf/FINJZyZhZlv04xggZUTdtu1OaPa37pBOrR9tAzxd1rb8mmu4TNZMVqfDjXB6NBqgghHiWBiu\n",
       "E1OaMtUB7tvskZsNamlNvuthskyFLJBeeIbsdi4uGyCBTHwZLp3Z50+3OQ+hMTardxVeMimQeUkp\n",
       "GwD8D/lUx1vEhu0+qhrsEbIuuB57JycWHrO4Y0lW3Hj7iQVVkOqranSGSlIAl+f8/nnsqDhhqGiN\n",
       "bYbfhzUapg305dMmmRTN0f2449oj3vEF/NUQbQ9Agv2xr5Q8fFBLmx5H2mW2UkcQgNAtSYm/RRF2\n",
       "xZsmM70M9BCtWtOt/93cQiLnwOOokpBASyLnSY8RwqSGxQJlknzj5blOBFC6BanbUqzmVHV9MN0E\n",
       "8fTHOr6o7ZTYYwvFRj0W37yPRXCv+H4VkQDSzviD75i386TNaq3v/jH+oiGhFdvrai9Arj8dERtP\n",
       "YdEyOiGBt5OV2zH9X3XNqkG+xXUDm36A9psjzjG4NrFk1+zqujSgKu+RgZro4KBnVU/Bh52UWs78\n",
       "2jwIFQ5YuwQ9whSNUgUjkBjP4+gtJyTh6CgWn1/qJMbCWZ7WxnrrWoyk1XdMOnyO3z4AVD6MFE5+\n",
       "XslMYT67+ESIDe3luMkc+ZT3k6PvwbqFSVwt1vdtdRTASSE62zByKT3f07qyGi7UGs7REuhGXyou\n",
       "NpchYX4f5gXF+13cVmnze+LwwzxKwOaGtR2le3b5Qntz7bhLqntcMU9rs9Pr91YfK7NFdycaTZ+a\n",
       "2Qpw/0K9HD/0HMxTvNf78veG5SyHrAEcUo43F53rnDTc/9yRnorMNDAIASVzXBJ+UdglRYFQEZ2j\n",
       "J4Tj716puUew/reTOgtEcgP7Ggn/0XQcb4KM5Gi9cCksW20XrR2CjqKlNaO1Rx0eVW+uoK6DUEMH\n",
       "ijA084M8Vr8E94HG6Mq+uIGvqi1HrrW73M5q6Iz8vQWPTuFUalBsjCYQXupSTqEdCOOOAq1MGBHv\n",
       "pBvT3Qr1OJwShdRkzMBZGwF4qCvs6f6LxiijXkH5Zg1/tt31OJyMEH5Umw1n5y8AABT0AZ6makEf\n",
       "AAAmmKNfkUAGktkzYWiCG88gpbI6kxdbl3EL6tiSZiKMzAyARDVVtChAaSubbvVPyykIUQkcSgud\n",
       "zVpZK2YrdBt1dYkC+aB1XJ59evzryDIEU4L1Tm/9HvCgX+xzrcVoCTAIIkO+QQgwdA1FTWZdpoXR\n",
       "KPCvDpHcQ6I2YCd7GyVvkzUKTAOqvzyi+GJOkhRCQdJJFWPWt4/xeSDsJ09Y4sYr0bhB4KcuCDNB\n",
       "dy+QsLDj/J9UHO0ZGt87ekpKRbePbiSIA7fItHyGhp++hGAfZDAqQ7DV/pWf50ht02hf+zXGEMkr\n",
       "UzjWeD1bAo4iSTmygKX4a0Q2++YL/sRXeQDojuy/GHq9DNg/ELeOJGhUOpMNDg1wwHMaWCUggquA\n",
       "jbqnfApp9cnjnwwtGMJcYeF4a7aAdduOzdeeOl6zKEQDEsMWTXtjOMks4WhhGz7ksvOVncNng4l7\n",
       "Na72CD0CsHC1WqK2bIN/LwdAus5keZDBjKav9ygG/Vkit8Sg8DFWNAbstLskMpbkbMRCW5VwjSZF\n",
       "uvR37BDk3TZ0IveoH3RZ8EEp9390ELosRL/u5G/qVH+NPXMDQZhEjDvG3newtX+ltYoSVVBmBJHD\n",
       "7clytPCC2KldX2KQnRVoQpMRZEXkZRs4qP7NLLY74Z6WiPlCiWLNb+ZFGreVyY4gRz4Ai9hZE1Pm\n",
       "4+DSwSbUwL4R6dus0leeh2nBAW4ZykL55qcD/BnWuA029AmSNhoTJoM7WyxeAAOogrIu1H2AJCor\n",
       "C8A3ngr/mgBVcvJzDPNwFxYDl47TJaXdZ5jwuwmHOoqWObthw9vMCsfDEMslt/SlYi0ir3VzGMIe\n",
       "PHiH2YxSSAicTP7sY45AkiBx2Sriv5JInIOoWXUtRSetU5RBQj9oG3Iri/lxsyFH4rOk4hQTfo6o\n",
       "HL/Hbco4t7fgdZHwulXFONT4Ix1CdsKS69NONH6t/OO8jP0ZhVyiwNa3XvBRLKzPgqQ8VSX+03wM\n",
       "Mi/4qWuqzVOhNG4dzERqc4pDXZERMA+BpIPAcWs/Gl3SHfm/oM/TsgVWhNS7+AiqsjEMF2F/Sz3P\n",
       "KTfMncigSxa+w3niuIzUFry/JYzg0F6wwrFpiz5+kDXzpu6X0glnd5udPGQgyEK3bOhYEU0rKh8n\n",
       "D5K4UU8Gymi3N8iwmczDTzdS8CwnG9EySDwtxR8EDkrBIlnkCKOA9zn0INxgXfHi5CUovAGFJUqX\n",
       "rmEXmcrpSb7PHmFewHK0gCrrQjReFj64Lnd8AseSZLLB+6I7h2x+9ArP40V37NDiE5bK7apqYvgI\n",
       "zlGATijOiNUZykIdqHw3Z+3f5Qpk1nQ5Ix+qOROUTDhJukvZQv9ql9DkqNWsNqWrbQp+pIAlDjyx\n",
       "jt7sd32NIzfPSeUgPkUgbDBME63QlwXv5Lq4vExC87/bayBg2fKQ2Owafs0l9tO14iAPHXLhAPI4\n",
       "7ZsEI7yptTK8lMkOQilkwo1qu84zNvXJhqvk9CVZdqH7wYmeNZS+JTPFicy+aKvc2dkPr2P4VLfB\n",
       "7/jsiNvd4W0F6qrnqApLsOTLx389B+QTDR7pQ08Vb22/nP/qdUWgxktUOcjav28zdOcaPP5Z6B0O\n",
       "MHZoFOfS7KSYf7WWQWpEAcZfi5D3RWeOVc34ior1DmLdCk6ZEtPR7LcRaxVmf0xlRZWAUoLag28s\n",
       "xiqG+KuuaNkA4MB835UWQJ9Pv4Ey6Fg9Cy2ijWsImCiXPOnn6MXJw4slGbL9L/7Ck5IKVowg6AdQ\n",
       "tjLHu4GQC7AmzjXZjEjDNfYH322/JrZgv7rnse0nz5IGHDVjjyc4eGYDRU3k+awblFVs1ftKitoj\n",
       "UCvuFtbbBIdByrD+qoN6SCVdWAv0XmwdWcWbBuizr5fOUOydBDzTJXWwVsr7+lOc1WdqeOPyTY4Q\n",
       "3mfH16oPx1TaLyDbwWrWnNdUPg39J7hIhXMAlUPvMsDM5r7qNGOLte+lgF4cCN8eiR3aP6yxnSkA\n",
       "brVcxOTTZmjpKRbQhdYXqQ9OaWFgdqJgeAnbmV/e0hVlbO4YV6qk86fF527TZWUjk0byeI7Zy3Mm\n",
       "1Uxkc3FrMRSEJidbowsqe7iXOhQCl90SGQKSjy8yz1QUYLWbTAzkq2OmJW//cCgYy/R9P/uQ9mJk\n",
       "LGhyNV7TJbGWWVQ2Gj1Fg1nH4g+VGo3FZRPqjYsYgP9ZDRbhg+EI0oDIopmmZ5XaTCVKUt6PClSf\n",
       "NTIgzZZ8iEpQDhVuxUPq9MTvPHVsWQZzXhmJgoB2eHLwk+pkv78bWKvvRCoMa9d6nbWOvom8RaOp\n",
       "S9K5AavpakLBvNIJj48Jyo3AVUdpf6BzsEk7/1B5ztgf9MQjewrMGCj+IPgjzSnb4XGm4r9GsLZr\n",
       "1Pq/MmwSCTvH4lTQpCXNxfeOuLteVisBEezcpmsm+wwpjDzmwIfq2UsVCc1ft2CFXnDZWax1bWkA\n",
       "RHvkbCHtl+9GVH2tM9t5OIxJrk01YqdzX8UXUaZJBeNGlq5J8EI2ZcPI2sgZwoycb7aaixA/ab/W\n",
       "/t3GT0jzTzWS5ROEqQKwwrd1r02lcC/2GhlQybfZ++skykXu3AHFQUrxFsdx3Fd5ryYXpq4/thao\n",
       "z8tKTC5sqeccQDQ6L7h5v9eLmqNx6i/BKq6cNTEfZUo+aH5wku9nXXE9rW+CcnBm65WCnu1wGURn\n",
       "CZnSE6D0G45eK2R3rMHNNotLjULlolpxtB8FH5xeVO43lYOoUX2B84+bTP2D1xiWvieLfASYrDii\n",
       "BjVt5Xj7vLkQbFQrNQo5+c890Q+bd9Rx8gCdfDv7WWFTRv9dRbYJpnfUvNYmLQNJq4RDM58VL9x6\n",
       "5Z8pnCLcbzpI+Z5PqdZQ69X1P2YoCqoxd05N/+/blnjkPcjGTajXWTgwAcdAvmqltzVdRWN/ot0Q\n",
       "0pPHVSxWB8JeeiBWrad1rMlTG+XGO/FIiG0HaIobDZh3LtcjHzQOkG2+GyF1yGEHEoQspGTGPlzA\n",
       "DSJPTXnLqmcU3wophbkkIgvWQsTb5YJL5dG1shThYPE6P8gCJmp6aM5JmPDWz1uL9nPAC+8LwU0M\n",
       "ACjvztAiKUvNfyauP5j+q/cBnD+uB8OA1U7t0YRbOPhRJDAiUM2ZVLliguM+9fH5XW2xvQCInMI5\n",
       "OMGfouEByGXtis2KDniaq+1BTrNvh0SargW8hPSHTDA2x3le20uzvXoRY0UV4rIbdC9+2ZcLnFb7\n",
       "ChNqZSzznb8QxaFwuj78ledHjpwuFIEB6DLWwwQxHE+Td6xvo0JVjzS2ZcWohtrz/nFZpFB6hkST\n",
       "m3YbHO3cScop+dhKJ3brRdqcM2/wV5QLpUC481V9S1MlGoXfC/2vB/vvx4UjXpQeyCj1ZjBvSVW6\n",
       "YSTnUUJ+kL9Lz+v/TCLRUQg4FrUW7B+R/fHRleyMohkjh805Pkpazg6OxIe7XKrlFg3bJFf95eX9\n",
       "zz/TU34iHXV17kwYFRa7Dqs2XxCx2vMpJ/KqCWc3B6eoXQ26rk/Ni+rFlNZ4mJddsZ+Ufd9+K/wq\n",
       "0IBwxtPzKC/vbbhOv7zIiH1obUgSQDpnitlaUR0gxXDQLBl+RVh5CAASeBBljgHLMoK2/+42FRel\n",
       "ykIdWcdmrHPBBw8+t83QhdfvIyld5m3VDlfl7jhrkZca/av52vMA0E4UeGxfnfKU78MBYMVQ2t6G\n",
       "At0UW1AQji51JI3TvgCD2GuZnl8Jkit/WtpeNalwUwUc/jGVvN5zGoUNiAbN8l/szECo0ByFlD8D\n",
       "pjkro4g/tkyWY2LCR1t9regB8aUPR+o3eOGFdzFXuQMSEXr1jOYIn5oleYsia0hgcuuq9ajgTC9T\n",
       "2zYhq/3rVnwBGHrTvkF6THoQ9G/2hYBVuJ6wBgGKnxKx7cOGuUbW4Zi/hI0Egs61+//BU3dQpvAZ\n",
       "Ak+fgkpt6EMoztQ6Gndh/MTo7xGUk3Les1d56irE8eDHCD0wvw9bxNnpUImJ+qU6byX1FDeAEh/z\n",
       "YabWbsuXq5f2gzK76lW3/XkRdVBv/DwbknxWJEvq737ojmTA94xs1GcEEYOcY3oPc3ZfxcR9xdiX\n",
       "97f6fdpzH+ILVs009dyHBhsAXKMgyLlQb82EUHJ7gh8uzAo+7yWY/h7yElGBbrJkmg7WulM4wh+A\n",
       "ncOQxR93xE69jomiJ3VSoN+AcDEwHyCP+WsjWS/mJF1CFX5H1rPoPx+c/d0whmAI7xbNKELXWxTG\n",
       "8kJVO6mcrRiBEj+WvHBqvAFF5zv7+eqBBW0mv9Ou1HYMk/ZqmngzSWLhD4T/NxCeA9C0bdPIxBo7\n",
       "wvpYrw5frjGbLBCOMZyk02QTuhpP8R7KZQcWRT/wmyYYbRv7DxzZZxjW97J7aHvKKwkgdBdAzDPR\n",
       "/1ZL2xI4InCWbhXxc6TIygVnna+0NxRV3BC9+EASmGv4Bbq5qtyfynnIvHRPHgczDZzH80rcHPIL\n",
       "b/Bl0toGGJQ5Jel1sP57hbHWlbFG9lnd+Tr+EGhk7zDnSqvb0CtyC1gLqyNTw9iQAYkB2PNBE8cm\n",
       "s8W2N2lWcZO4R9kkjaRpPPG8lc1M7YdWZmeEUg6y2S9Tifth4BQSNchgG64TJGM4u+AYEVtNSEhy\n",
       "GJ08BkNbgmKJzcuGJJTWI8EQcAGjJF5IBzMepDMVwhn3MSYiEEWzyDoKUJv5uDRGGNNX1/hGJPRc\n",
       "lacMFLe3OXe2+2NoA33X2YoDtOCc6XsmNQ3q90Rl36JhWPt8f1wM2UzMIdRQX/8DpbUfrYtCcyYT\n",
       "uXqLQDt9n8TEMrJ0Ecp9N+bd0HMuamoohtBM7uZUn0dvk3lbfm72jUhucLqhle7zjdmYMjsqhhUS\n",
       "zJU9XZasAP2U8y5Oe1kVp11Yz7UVRPtTuNDZGkJEhS3/DzWhJQVVZAUZuUt7oUBNWsaBDLXxrFIi\n",
       "bmM76FA4rWqNAFmkXbHqPtaX20Gi4JWIE3BVyHhnsIRrGAvmW0HCwJ00jvi6sNcq7Ykrey4BNIi3\n",
       "o3Spnai/vvv0uCMY+CJ+boJAmz3GzMPq8dnrrTJ8HIAGAQco3QRwXcYSztJQMSO6okaSQa2wOAFr\n",
       "MOX4aIJQuwIuUHfXEDetAeIR9VKwlePSEgPcQOXIFQt+OpJk4P8JBaz/kBKamgLgMIcakb3BILKf\n",
       "qbzc5lLiniPvCoYI39ITyNtCItM+npz8L4iw6xqPjtdcJ1Adj+1b89yky0v4a23OEkLsj2ZOSJcV\n",
       "PF0etNLO34AycBbXd3BiHfbdAPnxxRp1k1A/XJHIkAY2IoAV3wcdVLnOLfLl3o2fhYAoreMGvicm\n",
       "/f8l5rvrlOXBqiD2/qf3EejShx3q+L012wkih1OCGCPNs2Bg4JAEVFbW2ge++MC+J9N2OxGk4SQG\n",
       "6yYPY7WDqn6n/k9eVdoof++NvpNsI1tjytnhO0OZ7lwNnU3EEqNl586U8JJMlUcRCfMcKVrx95tu\n",
       "gvKcT6xIE3yfjADnklwB3kgtKTnlA4P+qgmHbZopdmCTlC6SSLnHp6Pq4X1PyLfm+TLAmeaeKtO5\n",
       "xQ9WMZ2Q8LvZmbWwxSURYXv7ZZ8JjxABCexyE1Mt3Kk8u+90igrqXxQrSSQa8RhtnTU89vaMp1My\n",
       "hmFBWIyvUA/QCnmOQOMYus/r5QePj1zeIBSy91zLoh6nVvkr+9g9vLpL4VKqmnjoAWP9pW+aahiy\n",
       "+if3qQDfrM3ujD8U6PRlYH8p9SibkTOMPZoLBe/vk54yoVxJmuU1qbXiOpVsHF6gv2ZKUsz/L9gJ\n",
       "a8R7fzLMSdeYeXKCOSBpwKepaPN5R8MCFhcHZiBj+aL9wwcdgyTu3LEDdBdnpe49ZXNMVeUi40zs\n",
       "WOJj0YVe+UzUoqvVcF5YEotKSR0I4FrtL4noskfgCzKAkptCBTL7BqrN0SOPR59Wg8E/qU4enqAx\n",
       "oVly+Vx/on/kmtGygUTmjHBojCK+itoJQreDVTfmGHlbhPPaPdLBECDgA4FL43cQiRYTN+oFnzDg\n",
       "9g1vzlmku/ZXZBZilZkvwE44sUrnbAni63/ELRt1M+J1d0+97btL5iF/c6xcb9O0UExXlATmUgWP\n",
       "SxA+jHTL9u4zdd2BX0yNqh6dlfAEAk7vahT1JrlfgCCmh1LGxLv9Kkf4YFyNlYg9OnDilT9nPmN2\n",
       "zXrnVt6RqSnyxmu3TsWromZNcllzrhb0l3+kn6MYJKfU1FkEB7vJmSw4QsOG4yWmxWL656IL93wN\n",
       "6cfcuBxH0PCfRoH68c799ba/ZZurbV10nSp1JL7yjislFh/x2H38ZrGd3ZbdrfpAf+NbkjTe/bLk\n",
       "hs97fg3XMh4/uk3Ox+DQQ2+onqxA1AKpv5gNY4Snf843gBu0ANVjnDOuzVSvPbhkiM7aGG/1qJJS\n",
       "QwlEmZQSjas7RJkm4sDtDHHWekOnD/c1afTQiacNicqZi5P4WvWW2aT9chIY8RanScfMW8TdJagG\n",
       "xMMuEIgSVeqq8HAC3yJPOaFcxALGOILKv3S3VQwCMUxBIJseRuk/eL2i4tv4Ws70GzA1BS+W/uZh\n",
       "diqMnDKhaKU2Qsiz6kG0mKCVa0tAz9Ac5xWnMjdyhFa6Ba3ov4ihsY6VMVVtTuDB9w6/QFa2fQra\n",
       "bvna9ClCGjoJGcxE2cYqjrOKlorHfhBpV1D/lcAudrhCmnW1R7Sxt4DBXGe2PeC91/7l0Ilf3nMS\n",
       "rfVv6UkxcxUgYmAIaMaEvlhgr1hSPnzNwhSzuCtf6w5T9H3zrS+9jIWRz6zaLB14QErR8HsCUEgq\n",
       "cGRkgZQ3Wy6GbgE86BdZU+2ru4NPMYwevdM4cHNI4UviMcwF1fiVEmSI8MeWPVR9H1QP/4vD6o5a\n",
       "BaYI/UT8Im1q1kMIWhg7UNMBnMeu6iLhF14cFgxG61Fc9wsso9JyRZRhs+NqGg1okbbFjHTws3yZ\n",
       "dAzgtpKeGK+Ho8BEcXu6aGsH+e5bIrha3iTFaPC9MxGt/rAPitcFThdOKPJLau875hM0a5I6dFvv\n",
       "JDD/F4uEghGtcDn923d/3wQzql37VanrRDA1nongeX9Smf3p9NeWGWTM/3yPhF6gkz3k/4CdGIUo\n",
       "VV2znLZ4JTr9lBnpfhAPUS4SBErmhsBltheDW9w4PQehr/ZDO73LMNBoKLo6J3gYnrT4VJuPTKHP\n",
       "yTnzdhxyCVsvJN3zml08V5LAfOmI840/ZJ8JN9i7rnsnsOOPIDpTt032zTkaYABB9C8AAFZAAPSB\n",
       "AAADim1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAlgAAEAAAEAAAAAAAAAAAAAAAABAAAA\n",
       "AAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAIAAAK0dHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAAAlgAAAAAAAAAAAAAAAAAAAA\n",
       "AAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAKAAAAB4AAAAAAAJGVkdHMAAAAc\n",
       "ZWxzdAAAAAAAAAABAAAJYAAAGAAAAQAAAAACLG1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAKAAA\n",
       "AGAAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAAAddt\n",
       "aW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAA\n",
       "AAEAAAGXc3RibAAAALdzdHNkAAAAAAAAAAEAAACnYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAA\n",
       "AAKAAeAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAA\n",
       "ADVhdmNDAWQAFv/hABhnZAAWrNlAoD2hAAADAAMAAAMAFA8WLZYBAAZo6+PLIsD9+PgAAAAAHHV1\n",
       "aWRraEDyXyRPxbo5pRvPAyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAAIAAAMAAAAABRzdHNzAAAA\n",
       "AAAAAAEAAAABAAAASGN0dHMAAAAAAAAABwAAAAEAABgAAAAAAQAAJAAAAAABAAAMAAAAAAEAADAA\n",
       "AAAAAgAADAAAAAABAAAkAAAAAAEAAAwAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAAAIAAAAAQAAADRz\n",
       "dHN6AAAAAAAAAAAAAAAIAABW5gAAK/sAACOzAAA22wAAJjYAACImAAAenQAAFPgAAAAUc3RjbwAA\n",
       "AAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAA\n",
       "AAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNjAuMTYuMTAw\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0],\n",
    "                               interval=300)\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lsc4WadilvD1"
   },
   "source": [
    "And now wrap it in the Pytorch Datasets class and print one example as sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ZXPeOA34lvD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] num_image_token: 256\n",
      "[Dataset] dynamic_image_size: False\n",
      "[Dataset] use_thumbnail: False\n",
      "[Dataset] min_dynamic_patch: 1, max_dynamic_patch: 6\n",
      "Formatting inputs...Skip in lazy mode\n",
      "[Dataset] num_image_token: 256\n",
      "[Dataset] dynamic_image_size: False\n",
      "[Dataset] use_thumbnail: False\n",
      "[Dataset] min_dynamic_patch: 1, max_dynamic_patch: 6\n",
      "Formatting inputs...Skip in lazy mode\n",
      "Train dataset: 150 examples\n",
      "Eval dataset: 38 examples\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = getattr(processor, \"tokenizer\", None)\n",
    "# if tokenizer is None:\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, use_fast=False, padding_side = \"right\", model_max_length=MAX_LENGTH)\n",
    "\n",
    "train_dataset = VideoQADataset(\n",
    "    template_name='internvl2_5',\n",
    "    raw_data=dataset[\"train\"],\n",
    "    video_data_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name=TASK_NAME,\n",
    "    num_image_token=256,\n",
    "    image_size=448,\n",
    "    is_train=True,\n",
    "    min_num_frame=8,\n",
    "    max_num_frame=8\n",
    ")\n",
    "\n",
    "eval_dataset = VideoQADataset(\n",
    "    template_name='internvl2_5',\n",
    "    raw_data=dataset[\"test\"],\n",
    "    video_data_dir=f\"{data_dir}/video/videos_unzipped/{video_dir}\",\n",
    "    tokenizer=tokenizer,\n",
    "    ds_name=TASK_NAME,\n",
    "    num_image_token=256,\n",
    "    image_size=448,\n",
    "    is_train=False,\n",
    "    min_num_frame=8,\n",
    "    max_num_frame=8\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Eval dataset: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in sample: dict_keys(['input_ids', 'labels', 'attention_mask', 'pixel_values', 'image_flags', 'answer_choice'])\n",
      "Input IDs shape: torch.Size([4096])\n",
      "Pixel values shape: torch.Size([8, 3, 448, 448])\n",
      "Labels shape: torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "# Display a sample from the training dataset\n",
    "sample_data = train_dataset[0]\n",
    "\n",
    "print(\"Keys in sample:\", sample_data.keys())\n",
    "print(\"Input IDs shape:\", sample_data['input_ids'].shape)\n",
    "print(\"Pixel values shape:\", sample_data['pixel_values'].shape)\n",
    "print(\"Labels shape:\", sample_data['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Th3HTV6A_89"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd4mrynzlvD2"
   },
   "source": [
    "## Load model\n",
    "Next, load your InternVL model from the hub. This is a model with about 1 billion trainable parameters (as it combines a **Qwen2 1B language model** with a relatively low-parameter vision **InternViT encoder**). Do note that we load a model here which already has undergone supervised fine-tuning (SFT) instructions dataset. We can benefit from the fine-tuning that the model already has undergone.\n",
    "\n",
    "## Full fine-tuning, LoRa and Q-LoRa\n",
    "\n",
    "**Select the fine-tuning method.**\n",
    "\n",
    " For reference, fine-tuning a model using the AdamW optimizer (which is often used to optimize neural networks) with mixed precision, you need about 18 times the amount of parameters in GB of GPU RAM. So in this case, we would need 18x1 billion bytes = 18 GB of GPU RAM if we want to update all the parameters of the model. Not so huge right? But using PEFT approach it could be less.\n",
    "\n",
    "Some clever people came up with the LoRa method (LoRa is short for low-rank adapation). It allows to just freeze the existing weights and only train a couple of adapter layers on top of the base model. Hugging Face offers the separate [PEFT library](https://huggingface.co/docs/peft/main/en/index) for easy use of LoRa, along with other Parameter-Efficient Fine-Tuning methods.\n",
    "\n",
    "Moreover, one can not only freeze the existing base model but also quantize it (which means, shrinking down its size). A neural network's parameters are typically saved in either float32 (which means, 32 bits or 4 bytes are used to store each parameter value) or float16 (which means, 16 bits or half a byte - also called half precision). However, with some clever algorithms one can shrink each parameter to just 8 or 4 bits (half a byte!), without significant effect on final performance. Read all about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes.\n",
    "\n",
    "This means that we're going to shrink the size of the base 1B model considerably using 4-bit quantization, and then only train a couple of adapter layers on top using LoRa (in float16). This idea of combining LoRa with quantization is called Q-LoRa and is the most memory friendly version.\n",
    "\n",
    "There exist many forms of quantization, here we leverage the [BitsAndBytes integration](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "DQ0nTqbVlvD2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with QLoRA (4-bit quantization)\n",
      "Model type: <class 'transformers_modules.OpenGVLab.InternVL3_5_hyphen_1B.2f71cf52542334823e48a46ffba0e2bc9add3446.modeling_internvl_chat.InternVLChatModel'>\n",
      "Model device: cuda:0\n",
      "Total parameters: 687,080,448\n"
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# QLoRA: model uses 4-bit quantization, which helps in reducing memory usage while maintaining performance.\n",
    "# Standard LoRA:  model is loaded with standard LoRA adaptations.\n",
    "# Full Fine-Tuning: no memory optimization are done. In that case Flash Attention is used to speed up training, if hardware supports it.\n",
    "\n",
    "# We use QLoRA as it's the most memory-efficient approach\n",
    "# This allows fine-tuning on consumer GPUs while maintaining good performance\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "if USE_QLORA:\n",
    "    # Configure 4-bit quantization using bitsandbytes\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,  # Nested quantization for additional memory savings\n",
    "        bnb_4bit_quant_type=\"nf4\",  # NormalFloat 4-bit quantization\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # Compute in bfloat16 for better performance\n",
    "    )\n",
    "    \n",
    "    # Load model with 4-bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=False,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model loaded with QLoRA (4-bit quantization)\")\n",
    "    \n",
    "elif USE_LORA:\n",
    "    # Load model in bfloat16 without quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        use_flash_attn=False\n",
    "    )\n",
    "    print(\"Model loaded for standard LoRA\")\n",
    "    \n",
    "else:\n",
    "    # Full fine-tuning - load model without any optimizations\n",
    "    model = AutoModel.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"  # Use Flash Attention if supported\n",
    "    )\n",
    "    print(\"Model loaded for full fine-tuning\")\n",
    "\n",
    "if hasattr(model.config, 'img_context_token_id'):\n",
    "    model.img_context_token_id = model.config.img_context_token_id\n",
    "else:\n",
    "     model.img_context_token_id = tokenizer.convert_tokens_to_ids('<IMG_CONTEXT>')\n",
    "    \n",
    "# Print model info\n",
    "print(f\"Model type: {type(model)}\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYDW50LslvD2",
    "outputId": "400cf245-42be-42d6-83f1-23211f77cb3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target modules for LoRA: ['o_proj', 'up_proj', 'k_proj', 'q_proj', 'gate_proj', 'v_proj', 'down_proj']\n",
      "torch.float32\n",
      "trainable params: 10,092,544 || all params: 1,070,990,336 || trainable%: 0.9424\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    # Only for LoRA ot QLoRA\n",
    "\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    "\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            lin_layer_name = name.split('.')[-1]\n",
    "            if lin_layer_name in target_modules:\n",
    "                lora_module_names.add(lin_layer_name)\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "original_dtype = model.dtype\n",
    "# If you selected LoRA ot QLora make a choise of parameters to replace\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Find target modules for LoRA\n",
    "target_modules = find_all_linear_names(model)\n",
    "print(f\"Target modules for LoRA: {target_modules}\")\n",
    "\n",
    "# Then create LoraConfig and run prepare_model_for_kbit_training(...)\n",
    "# and finally: model = get_peft_model(model, ...)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,  # LoRA alpha\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.dtype)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448 14\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "vision_cfg = model.config.vision_config\n",
    "print(vision_cfg.image_size, vision_cfg.patch_size)\n",
    "tokens = (vision_cfg.image_size // vision_cfg.patch_size) ** 2\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuv04h-DlvD2"
   },
   "source": [
    "## Define PyTorch Lightning Module for Video-LLaVA\n",
    "To streamline the training and evaluation of the Video-InternVL model, you can use [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html), which abstracts away much of the boilerplate code and provides a structured framework for model training. In this section, you need to define the InternVLModelPLModule, a custom PyTorch Lightning module that encapsulates the model, training loop, validation loop, and optimizer configuration.\n",
    "\n",
    "### InternVLModelPLModule Class\n",
    "\n",
    "The InternVLModelPLModule class inherits from LightningModule and includes methods for training, validation, and optimizer configuration. This setup ensures a clean and efficient training process.\n",
    "\n",
    "Basically, PyTorch Lightning will take care of all device placements (.to(device)) for us, as well as the backward pass, putting the model in training mode, etc.\n",
    "\n",
    "Notice the difference between a training step and an evaluation step:\n",
    "\n",
    "- a training step only consists of a forward pass, in which we compute the cross-entropy loss between the model's next token predictions and the ground truth (in parallel for all tokens, this technique is known as \"teacher forcing\"). The backward pass is handled by PyTorch Lightning.\n",
    "- an evaluation step consists of making the model autoregressively complete the prompt using the generate() method. After that, you compute an evaluation metric between the predicted sequences and the ground truth ones. This allows you to see how the model is improving over the course of training. The metric we use here is accuracy of answering the question.\n",
    "\n",
    "Besides that, you define the optimizer to use (AdamW is a good default choice) and the data loaders, which use the collate functions defined above to batch together items of the PyTorch datasets. Do note that AdamW is a pretty heavy optimizer in terms of memory requirements, but as we're training with QLoRa we only need to store optimizer states for the adapter layers. For full fine-tuning, one could take a look at more memory friendly optimizers such as 8-bit Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InternVLModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, tokenizer, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.model.train()\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = config.get('batch_size', 1)\n",
    "        self.learning_rate = config.get('learning_rate', 1e-4)\n",
    "        self.warmup_steps = config.get('warmup_steps', 50)\n",
    "        self.max_epochs = config.get('max_epochs', 10)\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters(ignore=['model', 'tokenizer'])\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.model.train()\n",
    "        # Extract inputs\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        image_flags = batch['image_flags']\n",
    "        \n",
    "        # Forward pass\n",
    "        # Direct forward pass without PEFT wrapper issues\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = self.model.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pixel_values=pixel_values,\n",
    "                labels=labels,\n",
    "                image_flags=image_flags,\n",
    "                return_dict=True\n",
    "            )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Log training loss\n",
    "        self.log('train_loss', loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        self.model.eval()\n",
    "        # For validation, we'll compute both loss and generation accuracy\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        pixel_values = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        image_flags = batch['image_flags']\n",
    "        answer_choices = batch.get('answer_choices', [])\n",
    "\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss = None\n",
    "        if labels is not None:\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                    outputs = self.model.base_model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        pixel_values=pixel_values,\n",
    "                        labels=labels,\n",
    "                        image_flags=image_flags,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                val_loss = outputs.loss\n",
    "                self.log('val_loss', val_loss, prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "        # Generate responses for accuracy calculation, add generation prompt\n",
    "        response_ids = []\n",
    "        response_masks = []\n",
    "        for input_id in input_ids:\n",
    "            decoded_text = self.tokenizer.decode(input_id, skip_special_tokens=False).strip()\n",
    "            response = decoded_text.split('<|im_start|>assistant')[0]\n",
    "            response += '\\nOnly give the best option.'\n",
    "            tokenized = tokenizer(\n",
    "                response + '<|im_start|>assistant',\n",
    "                return_tensors='pt',\n",
    "                padding='max_length',\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=False,\n",
    "            )\n",
    "            response_ids.append(tokenized.input_ids)\n",
    "            response_masks.append(tokenized.attention_mask)\n",
    "\n",
    "        response_ids = torch.cat(response_ids).to(self.model.device)\n",
    "        response_masks = torch.cat(response_masks).to(self.model.device)\n",
    "        prompt_lengths = response_masks.sum(dim=1)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                generated_ids = self.model.base_model.generate(\n",
    "                    input_ids=response_ids,\n",
    "                    attention_mask=response_masks,\n",
    "                    pixel_values=pixel_values,\n",
    "                    max_new_tokens=10,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                )\n",
    "                assistant_texts = []\n",
    "                for i in range(len(generated_ids)):\n",
    "                    start = int(prompt_lengths[i].item())\n",
    "                    assistant_tokens = generated_ids[i, start:]\n",
    "                    assistant_text = self.tokenizer.decode(assistant_tokens, skip_special_tokens=True).strip()\n",
    "                    assistant_text = f\"Best option:({assistant_text})\"\n",
    "                    assistant_texts.append(assistant_text)\n",
    "\n",
    "        print(f\"\\n=== Epoch {self.current_epoch}, Batch {batch_idx} ===\")\n",
    "        for i in range(len(generated_ids)):  # First 2 samples\n",
    "            input_text = self.tokenizer.decode(response_ids[i], skip_special_tokens=True).strip()\n",
    "            generated_text = assistant_texts[i]\n",
    "            expected_answer = answer_choices[i] if i < len(answer_choices) else ''\n",
    "            print(f\"Sample {i}:\")\n",
    "            print(f\"  Expected: '{expected_answer}'\")\n",
    "            print(f\"  Input: '{input_text}'\")\n",
    "            print(f\"  Generated: {assistant_texts[i]!r}\")\n",
    "            assistant_upper = assistant_texts[i].upper().strip()\n",
    "            expected_upper = (expected_answer or \"\").upper()\n",
    "            match_value = bool(expected_upper and (assistant_upper.startswith(expected_upper) or f'({expected_upper}' in assistant_upper))\n",
    "            print(f\"  Match: {match_value}\")\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = self._calculate_accuracy(assistant_texts, answer_choices)\n",
    "        self.log('val_accuracy', accuracy, prog_bar=True, logger=True, sync_dist=True)\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_accuracy': accuracy}\n",
    "    \n",
    "    def _calculate_accuracy(self, assistant_texts, answer_choices):\n",
    "        \"\"\"Calculate accuracy by comparing generated text with answer choices\"\"\"\n",
    "        if not answer_choices:\n",
    "            print('############# There is no answers #########')\n",
    "            return 0.0\n",
    "        \n",
    "        correct = 0\n",
    "        total = len(assistant_texts)\n",
    "        \n",
    "        for text, expected in zip(assistant_texts, answer_choices):\n",
    "            expected_answer = (expected or \"\").upper()\n",
    "            text_upper = text.upper().strip()\n",
    "            if expected_answer and (text_upper.startswith(expected_answer) or f'({expected_answer}' in text_upper):\n",
    "                correct += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer and learning rate scheduler\"\"\"\n",
    "        # Separate parameters for different weight decay\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.config.get('weight_decay', 0.01),\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() \n",
    "                          if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        # Calculate total training steps\n",
    "        train_loader = self.train_dataloader()\n",
    "        steps_per_epoch = len(train_loader)\n",
    "        total_steps = steps_per_epoch * self.max_epochs\n",
    "        \n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            train_dataset, \n",
    "            collate_fn=train_collate_fn,\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=True, \n",
    "            num_workers=4\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            eval_dataset, \n",
    "            collate_fn=train_collate_fn,\n",
    "            batch_size=self.batch_size, \n",
    "            shuffle=False, \n",
    "            num_workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 6,\n",
    "          #\"val_check_interval\": 1.0, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 1,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "}\n",
    "\n",
    "model_module = InternVLModelPLModule(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model  # Your loaded model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-EncDAllvD2"
   },
   "source": [
    "## Define callbacks\n",
    "Optionally, Lightning allows to define so-called [callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html), which are arbitrary pieces of code that can be executed during training.\n",
    "\n",
    "You'd better use the EarlyStopping callback of Lightning, which will automatically stop training once the evaluation metric (edit distance in our case) doesn't improve after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "p2KRo-rclvD2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"InternVL3.5_finetuning_qlora\")\n",
    "\n",
    "# api = HfApi()\n",
    "\n",
    "# class PushToHubCallback(Callback):\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "#         pl_module.model.push_to_hub(REPO_ID,\n",
    "#                                     commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "#     def on_train_end(self, trainer, pl_module):\n",
    "#         print(f\"Pushing model to the hub after training\")\n",
    "#         pl_module.processor.push_to_hub(REPO_ID,\n",
    "#                                     commit_message=f\"Training done\")\n",
    "#         pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    # commit_message=f\"Training done\")\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints',           # Directory to save checkpoints\n",
    "    filename='internvl-epoch-{epoch:02d}-val_acc_{val_accuracy:.2f}',  # File name pattern\n",
    "    save_top_k=-1,                     # Save all checkpoints (-1 means all)\n",
    "    every_n_epochs=1,                  # Save every epoch\n",
    "    save_last=True,                    # Also save a 'last.ckpt' file\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_accuracy\",\n",
    "                                    patience=3, verbose=False, mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-ylKSrMlvD2"
   },
   "source": [
    "## Train!\n",
    " Trainer class supports many more flags. See the [docs](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "csv_logger = CSVLogger(save_dir=\"logs\", name=\"internvl_qlora_video\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    check_val_every_n_epoch=config[\"check_val_every_n_epoch\"],\n",
    "    gradient_clip_val=config[\"gradient_clip_val\"],\n",
    "    accumulate_grad_batches=config[\"accumulate_grad_batches\"],\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger=csv_logger,\n",
    "    precision=\"16-mixed\",\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 697 M  | train\n",
      "-------------------------------------------------------\n",
      "10.1 M    Trainable params\n",
      "687 M     Non-trainable params\n",
      "697 M     Total params\n",
      "2,788.692 Total estimated model params size (MB)\n",
      "2736      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 0, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'yet?'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ':\n",
      "\n",
      "\n",
      "\n",
      ";:01: 1:'\n",
      "  Match: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309232280ac14ea2a75bef304b8042cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 0, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'else: 1: 1: 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 2 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person took the dish?\n",
      "A. Threw the shoe.;\n",
      "B. Put down the food.;\n",
      "C. Threw the pillow.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'very good'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 3 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person drank from the cup/glass/bottle?\n",
      "A. Ate the sandwich.;\n",
      "B. Took the dish.;\n",
      "C. Washed the cup/glass/bottle.;\n",
      "D. Took the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 4 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the clothes?\n",
      "A. Opened the window.;\n",
      "B. Tidied up the closet/cabinet.;\n",
      "C. Took the food.;\n",
      "D. Tidied up the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.* ***..'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 5 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the clothes?\n",
      "A. Threw the book.;\n",
      "B. Put down the bag.;\n",
      "C. Opened the window.;\n",
      "D. Put down the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '****: 4** 4.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 6 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the book?\n",
      "A. Sat on the bed.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the picture.;\n",
      "D. Threw the book.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1 ]'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 7 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Closed the refrigerator.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Took the towel.;\n",
      "D. Took the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'enough.\n",
      ";.\n",
      ";;;;;;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 8 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person washed the clothes?\n",
      "A. Put down the blanket.;\n",
      "B. Took the clothes.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the pillow.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'example: for example: 1: 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 9 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the refrigerator?\n",
      "A. Closed the window.;\n",
      "B. Put down the cup/glass/bottle.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'deeply\n",
      "  ; 1 1 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 10 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the window?\n",
      "A. Put down the food.;\n",
      "B. Washed the window.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Opened the door.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. for.. for.. for.. for.. for'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 11 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person watched at the picture?\n",
      "A. Opened the book.;\n",
      "B. Lied on the floor.;\n",
      "C. Washed the clothes.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ',**;  for, 1 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 12 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the floor?\n",
      "A. Took the bag.;\n",
      "B. Took the phone/camera.;\n",
      "C. Took the blanket.;\n",
      "D. Took the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '..'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 13 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat at the table?\n",
      "A. Threw the bag.;\n",
      "B. Closed the closet/cabinet.;\n",
      "C. Ate the sandwich.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ';;;;;;;;;;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 14 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the food?\n",
      "A. Took the food.;\n",
      "B. Opened the bag.;\n",
      "C. Took the sandwich.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '**'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 15 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the door?\n",
      "A. Put down the box.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the towel.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ':\n",
      "\n",
      "\n",
      "\n",
      ":'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 16 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'itarian'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 17 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Threw the broom.;\n",
      "B. Took the book.;\n",
      "C. Put down the phone/camera.;\n",
      "D. Put down the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '*** ***.\n",
      ".\n",
      ".\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 18 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the blanket?\n",
      "A. Took the bag.;\n",
      "B. Took the towel.;\n",
      "C. Tidied up the clothes.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anywayy*; * for * for * for'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 19 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the dish?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Sat on the floor.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anyway.;  Put down the cup/glass'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 20 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the box?\n",
      "A. Tidied up the blanket.;\n",
      "B. Put down the dish.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ''\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 21 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the food?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Closed the book.;\n",
      "C. Put down the cup/glass/bottle.;\n",
      "D. Put down the phone/camera.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ''\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 22 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the box?\n",
      "A. Put down the food.;\n",
      "B. Threw the towel.;\n",
      "C. Took the shoe.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 23 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person drank from the cup/glass/bottle?\n",
      "A. Lied on the floor.;\n",
      "B. Took the broom.;\n",
      "C. Put down the dish.;\n",
      "D. Opened the bag.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '*** for.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 24 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the laptop?\n",
      "A. Took the bag.;\n",
      "B. Took the sandwich.;\n",
      "C. Took the food.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anywayly: 1: 1:'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 25 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the bed?\n",
      "A. Took the phone/camera.;\n",
      "B. Opened the window.;\n",
      "C. Took the pillow.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '........'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 26 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the dish?\n",
      "A. Put down the food.;\n",
      "B. Put down the box.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Put down the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 11111111'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 27 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the bag.;\n",
      "B. Closed the book.;\n",
      "C. Took the shoe.;\n",
      "D. Washed the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 28 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Opened the laptop.;\n",
      "B. Threw the broom.;\n",
      "C. Opened the door.;\n",
      "D. Closed the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ''\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 29 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the sandwich?\n",
      "A. Put down the dish.;\n",
      "B. Put down the food.;\n",
      "C. Put down the towel.;\n",
      "D. Closed the refrigerator.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'again; like; like; like; like;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 30 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the book?\n",
      "A. Put down the towel.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the bag.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 31 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the blanket?\n",
      "A. Took the book.;\n",
      "B. Put down the dish.;\n",
      "C. Opened the closet/cabinet.;\n",
      "D. Put down the laptop.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ';\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 32 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the door?\n",
      "A. Threw the box.;\n",
      "B. Put down the phone/camera.;\n",
      "C. Took the pillow.;\n",
      "D. Tidied up the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ':\n",
      "\n",
      "\n",
      "\n",
      ";;: 3 = 1 ='\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 33 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Took the broom.;\n",
      "B. Took the blanket.;\n",
      "C. Opened the door.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 34 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the shoe?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Threw the broom.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Washed the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '*  ;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 35 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the cup/glass/bottle?\n",
      "A. Threw the blanket.;\n",
      "B. Threw the box.;\n",
      "C. Put down the shoe.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anything as  ; anything as a  ; anything'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 36 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Threw the towel.;\n",
      "C. Threw the book.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 1 ； ； ；'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 37 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the book.;\n",
      "B. Threw the shoe.;\n",
      "C. Took the book.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%;\n",
      "%.\n",
      ".\n",
      ".\n",
      ".'\n",
      "  Match: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '$$ 8  Put down the paper/note'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'gone 2: Put down the towel..'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 2 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person took the dish?\n",
      "A. Threw the shoe.;\n",
      "B. Put down the food.;\n",
      "C. Threw the pillow.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 3 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person drank from the cup/glass/bottle?\n",
      "A. Ate the sandwich.;\n",
      "B. Took the dish.;\n",
      "C. Washed the cup/glass/bottle.;\n",
      "D. Took the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***  Ate the sandwich.\n",
      "\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 4 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the clothes?\n",
      "A. Opened the window.;\n",
      "B. Tidied up the closet/cabinet.;\n",
      "C. Took the food.;\n",
      "D. Tidied up the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*  Opened the window..  A'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 5 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the clothes?\n",
      "A. Threw the book.;\n",
      "B. Put down the bag.;\n",
      "C. Opened the window.;\n",
      "D. Put down the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '****: 4  k 4  k'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 6 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the book?\n",
      "A. Sat on the bed.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the picture.;\n",
      "D. Threw the book.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a.\n",
      "。\n",
      "。\n",
      "。\n",
      "。。\n",
      "。。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 7 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Closed the refrigerator.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Took the towel.;\n",
      "D. Took the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'like. = 7\n",
      "  Closed the door'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 8 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person washed the clothes?\n",
      "A. Put down the blanket.;\n",
      "B. Took the clothes.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the pillow.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'from:'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 9 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the refrigerator?\n",
      "A. Closed the window.;\n",
      "B. Put down the cup/glass/bottle.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'bigger'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 10 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the window?\n",
      "A. Put down the food.;\n",
      "B. Washed the window.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Opened the door.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'yet, 1  Put down the food.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 11 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person watched at the picture?\n",
      "A. Opened the book.;\n",
      "B. Lied on the floor.;\n",
      "C. Washed the clothes.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ')**, 1 1 1 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 12 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the floor?\n",
      "A. Took the bag.;\n",
      "B. Took the phone/camera.;\n",
      "C. Took the blanket.;\n",
      "D. Took the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '**'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 13 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat at the table?\n",
      "A. Threw the bag.;\n",
      "B. Closed the closet/cabinet.;\n",
      "C. Ate the sandwich.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'another for.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 14 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the food?\n",
      "A. Took the food.;\n",
      "B. Opened the bag.;\n",
      "C. Took the sandwich.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ''\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 15 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the door?\n",
      "A. Put down the box.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the towel.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ''\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 16 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'well'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 17 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Threw the broom.;\n",
      "B. Took the book.;\n",
      "C. Put down the phone/camera.;\n",
      "D. Put down the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '******************************'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 18 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the blanket?\n",
      "A. Took the bag.;\n",
      "B. Took the towel.;\n",
      "C. Tidied up the clothes.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'bigger. 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 19 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the dish?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Sat on the floor.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anyway.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 20 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the box?\n",
      "A. Tidied up the blanket.;\n",
      "B. Put down the dish.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '..'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 21 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the food?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Closed the book.;\n",
      "C. Put down the cup/glass/bottle.;\n",
      "D. Put down the phone/camera.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 22 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the box?\n",
      "A. Put down the food.;\n",
      "B. Threw the towel.;\n",
      "C. Took the shoe.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 23 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person drank from the cup/glass/bottle?\n",
      "A. Lied on the floor.;\n",
      "B. Took the broom.;\n",
      "C. Put down the dish.;\n",
      "D. Opened the bag.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*  Lied on the..\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 24 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the laptop?\n",
      "A. Took the bag.;\n",
      "B. Took the sandwich.;\n",
      "C. Took the food.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=\"%_\")'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 25 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the bed?\n",
      "A. Took the phone/camera.;\n",
      "B. Opened the window.;\n",
      "C. Took the pillow.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 20000000'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 26 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the dish?\n",
      "A. Put down the food.;\n",
      "B. Put down the box.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Put down the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'for 1:  Put down the food'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 27 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the bag.;\n",
      "B. Closed the book.;\n",
      "C. Took the shoe.;\n",
      "D. Washed the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*;\n",
      ".*.\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 28 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Opened the laptop.;\n",
      "B. Threw the broom.;\n",
      "C. Opened the door.;\n",
      "D. Closed the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anything else'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 29 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the sandwich?\n",
      "A. Put down the dish.;\n",
      "B. Put down the food.;\n",
      "C. Put down the towel.;\n",
      "D. Closed the refrigerator.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ']*.*'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 30 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the book?\n",
      "A. Put down the towel.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the bag.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "?'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 31 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the blanket?\n",
      "A. Took the book.;\n",
      "B. Put down the dish.;\n",
      "C. Opened the closet/cabinet.;\n",
      "D. Put down the laptop.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ';\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "。\n",
      ";'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 32 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the door?\n",
      "A. Threw the box.;\n",
      "B. Put down the phone/camera.;\n",
      "C. Took the pillow.;\n",
      "D. Tidied up the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '( 1 0) 0 0'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 33 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Took the broom.;\n",
      "B. Took the blanket.;\n",
      "C. Opened the door.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 34 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the shoe?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Threw the broom.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Washed the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.; ***: ***: a\n",
      "[1]'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 35 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the cup/glass/bottle?\n",
      "A. Threw the blanket.;\n",
      "B. Threw the box.;\n",
      "C. Put down the shoe.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anything  ; anything  ; anything 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 36 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Threw the towel.;\n",
      "C. Threw the book.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. Put down the cup/glass/bottle'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 37 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the book.;\n",
      "B. Threw the shoe.;\n",
      "C. Took the book.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%;\n",
      " * *.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "。\n",
      "。'\n",
      "  Match: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 2, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '$$  A Put down the paper/notebook'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 2  Put down the phone/c'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 2 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person took the dish?\n",
      "A. Threw the shoe.;\n",
      "B. Put down the food.;\n",
      "C. Threw the pillow.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 3 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person drank from the cup/glass/bottle?\n",
      "A. Ate the sandwich.;\n",
      "B. Took the dish.;\n",
      "C. Washed the cup/glass/bottle.;\n",
      "D. Took the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '** for 3  A  Ate the'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 4 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the clothes?\n",
      "A. Opened the window.;\n",
      "B. Tidied up the closet/cabinet.;\n",
      "C. Took the food.;\n",
      "D. Tidied up the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.* ***  A. Opened the window.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 5 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the clothes?\n",
      "A. Threw the book.;\n",
      "B. Put down the bag.;\n",
      "C. Opened the window.;\n",
      "D. Put down the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anyway........\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 6 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the book?\n",
      "A. Sat on the bed.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the picture.;\n",
      "D. Threw the book.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 7 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Closed the refrigerator.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Took the towel.;\n",
      "D. Took the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '* * * * * * * * * y'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 8 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person washed the clothes?\n",
      "A. Put down the blanket.;\n",
      "B. Took the clothes.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the pillow.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '己 2: 1: 1:'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 9 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the refrigerator?\n",
      "A. Closed the window.;\n",
      "B. Put down the cup/glass/bottle.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'everything 2 2 2 2'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 10 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the window?\n",
      "A. Put down the food.;\n",
      "B. Washed the window.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Opened the door.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'everything\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",; �'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 11 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person watched at the picture?\n",
      "A. Opened the book.;\n",
      "B. Lied on the floor.;\n",
      "C. Washed the clothes.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***, 5  5  5'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 12 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the floor?\n",
      "A. Took the bag.;\n",
      "B. Took the phone/camera.;\n",
      "C. Took the blanket.;\n",
      "D. Took the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '**.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 13 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat at the table?\n",
      "A. Threw the bag.;\n",
      "B. Closed the closet/cabinet.;\n",
      "C. Ate the sandwich.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '; for =:::::::'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 14 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the food?\n",
      "A. Took the food.;\n",
      "B. Opened the bag.;\n",
      "C. Took the sandwich.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '******************************'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 15 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the door?\n",
      "A. Put down the box.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the towel.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '1 1 1 1 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 16 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'yet\n",
      "\n",
      " for'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 17 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Threw the broom.;\n",
      "B. Took the book.;\n",
      "C. Put down the phone/camera.;\n",
      "D. Put down the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***.****.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 18 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the blanket?\n",
      "A. Took the bag.;\n",
      "B. Took the towel.;\n",
      "C. Tidied up the clothes.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'differently.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 19 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the dish?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Sat on the floor.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anyway.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 20 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the box?\n",
      "A. Tidied up the blanket.;\n",
      "B. Put down the dish.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***  Tidied up the blanket.;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 21 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the food?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Closed the book.;\n",
      "C. Put down the cup/glass/bottle.;\n",
      "D. Put down the phone/camera.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      " ***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 22 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the box?\n",
      "A. Put down the food.;\n",
      "B. Threw the towel.;\n",
      "C. Took the shoe.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 23 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person drank from the cup/glass/bottle?\n",
      "A. Lied on the floor.;\n",
      "B. Took the broom.;\n",
      "C. Put down the dish.;\n",
      "D. Opened the bag.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*  C Put down the dish.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 24 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the laptop?\n",
      "A. Took the bag.;\n",
      "B. Took the sandwich.;\n",
      "C. Took the food.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "。\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 25 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the bed?\n",
      "A. Took the phone/camera.;\n",
      "B. Opened the window.;\n",
      "C. Took the pillow.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 20000000'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 26 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the dish?\n",
      "A. Put down the food.;\n",
      "B. Put down the box.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Put down the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*  for  for 1: A Put'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 27 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the bag.;\n",
      "B. Closed the book.;\n",
      "C. Took the shoe.;\n",
      "D. Washed the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*  : 人工工工工工工'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 28 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Opened the laptop.;\n",
      "B. Threw the broom.;\n",
      "C. Opened the door.;\n",
      "D. Closed the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '己 ***\n",
      "\n",
      "\n",
      "2 2 2'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 29 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the sandwich?\n",
      "A. Put down the dish.;\n",
      "B. Put down the food.;\n",
      "C. Put down the towel.;\n",
      "D. Closed the refrigerator.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '))* for when did she put down the dish.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 30 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the book?\n",
      "A. Put down the towel.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the bag.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 31 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the blanket?\n",
      "A. Took the book.;\n",
      "B. Put down the dish.;\n",
      "C. Opened the closet/cabinet.;\n",
      "D. Put down the laptop.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ';\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "。\n",
      ".\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 32 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the door?\n",
      "A. Threw the box.;\n",
      "B. Put down the phone/camera.;\n",
      "C. Took the pillow.;\n",
      "D. Tidied up the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '(  ): 1) 1)'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 33 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Took the broom.;\n",
      "B. Took the blanket.;\n",
      "C. Opened the door.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[1]'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 34 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the shoe?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Threw the broom.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Washed the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '))*  A Opened closet/cabinet.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 35 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the cup/glass/bottle?\n",
      "A. Threw the blanket.;\n",
      "B. Threw the box.;\n",
      "C. Put down the shoe.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anything'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 36 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Threw the towel.;\n",
      "C. Threw the book.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '2  Put down the cup/glass/b'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 2, Batch 37 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the book.;\n",
      "B. Threw the shoe.;\n",
      "C. Took the book.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '[];\n",
      ";\n",
      ");\n",
      ";\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".'\n",
      "  Match: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 3, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***.\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '20;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 2 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person took the dish?\n",
      "A. Threw the shoe.;\n",
      "B. Put down the food.;\n",
      "C. Threw the pillow.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 3 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person drank from the cup/glass/bottle?\n",
      "A. Ate the sandwich.;\n",
      "B. Took the dish.;\n",
      "C. Washed the cup/glass/bottle.;\n",
      "D. Took the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '** for.\n",
      "  Ate the sandwich.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 4 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the clothes?\n",
      "A. Opened the window.;\n",
      "B. Tidied up the closet/cabinet.;\n",
      "C. Took the food.;\n",
      "D. Tidied up the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.* ***.\n",
      ".  A. Opened the'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 5 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the clothes?\n",
      "A. Threw the book.;\n",
      "B. Put down the bag.;\n",
      "C. Opened the window.;\n",
      "D. Put down the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 2: 2: 2:'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 6 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the book?\n",
      "A. Sat on the bed.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the picture.;\n",
      "D. Threw the book.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 7 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Closed the refrigerator.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Took the towel.;\n",
      "D. Took the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '* * * * * = . .\n",
      ".\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 8 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person washed the clothes?\n",
      "A. Put down the blanket.;\n",
      "B. Took the clothes.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the pillow.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'if:** for example: 1: 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 9 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the refrigerator?\n",
      "A. Closed the window.;\n",
      "B. Put down the cup/glass/bottle.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'everything 2 2 2 2'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 10 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the window?\n",
      "A. Put down the food.;\n",
      "B. Washed the window.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Opened the door.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'really really really really really really really really really really'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 11 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person watched at the picture?\n",
      "A. Opened the book.;\n",
      "B. Lied on the floor.;\n",
      "C. Washed the clothes.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '*** = 2, 2, 2'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 12 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the floor?\n",
      "A. Took the bag.;\n",
      "B. Took the phone/camera.;\n",
      "C. Took the blanket.;\n",
      "D. Took the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '**.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 13 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat at the table?\n",
      "A. Threw the bag.;\n",
      "B. Closed the closet/cabinet.;\n",
      "C. Ate the sandwich.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " for.;'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 14 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the food?\n",
      "A. Took the food.;\n",
      "B. Opened the bag.;\n",
      "C. Took the sandwich.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***ate  A Took the sandwich.\n",
      "\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 15 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the door?\n",
      "A. Put down the box.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the towel.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'yet?'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 16 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'well\n",
      "  Put down the pillow.  A'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 17 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Threw the broom.;\n",
      "B. Took the book.;\n",
      "C. Put down the phone/camera.;\n",
      "D. Put down the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 18 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the blanket?\n",
      "A. Took the bag.;\n",
      "B. Took the towel.;\n",
      "C. Tidied up the clothes.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '* * * * * * * .'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 19 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the dish?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Sat on the floor.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'anyway.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 20 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the box?\n",
      "A. Tidied up the blanket.;\n",
      "B. Put down the dish.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 21 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the food?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Closed the book.;\n",
      "C. Put down the cup/glass/bottle.;\n",
      "D. Put down the phone/camera.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: ''\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 22 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the box?\n",
      "A. Put down the food.;\n",
      "B. Threw the towel.;\n",
      "C. Took the shoe.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '****  A Put down the food.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 23 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person drank from the cup/glass/bottle?\n",
      "A. Lied on the floor.;\n",
      "B. Took the broom.;\n",
      "C. Put down the dish.;\n",
      "D. Opened the bag.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.* ***'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 24 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the laptop?\n",
      "A. Took the bag.;\n",
      "B. Took the sandwich.;\n",
      "C. Took the food.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "..'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 25 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the bed?\n",
      "A. Took the phone/camera.;\n",
      "B. Opened the window.;\n",
      "C. Took the pillow.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.. 20000000'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 26 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the dish?\n",
      "A. Put down the food.;\n",
      "B. Put down the box.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Put down the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'well;  for example: 1: Put'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 27 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the bag.;\n",
      "B. Closed the book.;\n",
      "C. Took the shoe.;\n",
      "D. Washed the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '.*'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 28 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Opened the laptop.;\n",
      "B. Threw the broom.;\n",
      "C. Opened the door.;\n",
      "D. Closed the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '己'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 29 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the sandwich?\n",
      "A. Put down the dish.;\n",
      "B. Put down the food.;\n",
      "C. Put down the towel.;\n",
      "D. Closed the refrigerator.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '))* for to for.\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 30 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the book?\n",
      "A. Put down the towel.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the bag.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '。'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 31 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the blanket?\n",
      "A. Took the book.;\n",
      "B. Put down the dish.;\n",
      "C. Opened the closet/cabinet.;\n",
      "D. Put down the laptop.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***;\n",
      " 。\n",
      ";\n",
      "。\n",
      ".\n",
      ".\n",
      ".\n",
      ".'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 32 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the door?\n",
      "A. Threw the box.;\n",
      "B. Put down the phone/camera.;\n",
      "C. Took the pillow.;\n",
      "D. Tidied up the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '(  for 1 1 1'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 33 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Took the broom.;\n",
      "B. Took the blanket.;\n",
      "C. Opened the door.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Frame1: 0)'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 34 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the shoe?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Threw the broom.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Washed the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '***.'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 35 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the cup/glass/bottle?\n",
      "A. Threw the blanket.;\n",
      "B. Threw the box.;\n",
      "C. Put down the shoe.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: 'for'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 36 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Threw the towel.;\n",
      "C. Threw the book.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '1  ;  Put down the cup/g'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 3, Batch 37 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the book.;\n",
      "B. Threw the shoe.;\n",
      "C. Took the book.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "assistant'\n",
      "  Generated: '%;\n",
      " ***;\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      ";\n",
      "。\n",
      ";'\n",
      "  Match: False\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:751: Checkpoint directory /home/jovyan/work/genai_work/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | PeftModelForCausalLM | 697 M  | train\n",
      "-------------------------------------------------------\n",
      "10.1 M    Trainable params\n",
      "687 M     Non-trainable params\n",
      "697 M     Total params\n",
      "2,788.692 Total estimated model params size (MB)\n",
      "2736      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing training up to epoch 2 (current epoch: 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 0, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d137a68e616478b8b12d4c4f36fad28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 0, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 2 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person took the dish?\n",
      "A. Threw the shoe.;\n",
      "B. Put down the food.;\n",
      "C. Threw the pillow.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 3 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person drank from the cup/glass/bottle?\n",
      "A. Ate the sandwich.;\n",
      "B. Took the dish.;\n",
      "C. Washed the cup/glass/bottle.;\n",
      "D. Took the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 4 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the clothes?\n",
      "A. Opened the window.;\n",
      "B. Tidied up the closet/cabinet.;\n",
      "C. Took the food.;\n",
      "D. Tidied up the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 5 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the clothes?\n",
      "A. Threw the book.;\n",
      "B. Put down the bag.;\n",
      "C. Opened the window.;\n",
      "D. Put down the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 6 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the book?\n",
      "A. Sat on the bed.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the picture.;\n",
      "D. Threw the book.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 7 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Closed the refrigerator.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Took the towel.;\n",
      "D. Took the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 8 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person washed the clothes?\n",
      "A. Put down the blanket.;\n",
      "B. Took the clothes.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the pillow.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 9 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the refrigerator?\n",
      "A. Closed the window.;\n",
      "B. Put down the cup/glass/bottle.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 10 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the window?\n",
      "A. Put down the food.;\n",
      "B. Washed the window.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Opened the door.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 11 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person watched at the picture?\n",
      "A. Opened the book.;\n",
      "B. Lied on the floor.;\n",
      "C. Washed the clothes.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 12 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the floor?\n",
      "A. Took the bag.;\n",
      "B. Took the phone/camera.;\n",
      "C. Took the blanket.;\n",
      "D. Took the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 13 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat at the table?\n",
      "A. Threw the bag.;\n",
      "B. Closed the closet/cabinet.;\n",
      "C. Ate the sandwich.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 14 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the food?\n",
      "A. Took the food.;\n",
      "B. Opened the bag.;\n",
      "C. Took the sandwich.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 15 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the door?\n",
      "A. Put down the box.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the towel.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 16 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 17 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Threw the broom.;\n",
      "B. Took the book.;\n",
      "C. Put down the phone/camera.;\n",
      "D. Put down the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 18 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the blanket?\n",
      "A. Took the bag.;\n",
      "B. Took the towel.;\n",
      "C. Tidied up the clothes.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 19 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the dish?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Sat on the floor.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 20 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the box?\n",
      "A. Tidied up the blanket.;\n",
      "B. Put down the dish.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 21 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the food?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Closed the book.;\n",
      "C. Put down the cup/glass/bottle.;\n",
      "D. Put down the phone/camera.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 22 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the box?\n",
      "A. Put down the food.;\n",
      "B. Threw the towel.;\n",
      "C. Took the shoe.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 23 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person drank from the cup/glass/bottle?\n",
      "A. Lied on the floor.;\n",
      "B. Took the broom.;\n",
      "C. Put down the dish.;\n",
      "D. Opened the bag.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 24 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the laptop?\n",
      "A. Took the bag.;\n",
      "B. Took the sandwich.;\n",
      "C. Took the food.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 25 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the bed?\n",
      "A. Took the phone/camera.;\n",
      "B. Opened the window.;\n",
      "C. Took the pillow.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 26 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the dish?\n",
      "A. Put down the food.;\n",
      "B. Put down the box.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Put down the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 27 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the bag.;\n",
      "B. Closed the book.;\n",
      "C. Took the shoe.;\n",
      "D. Washed the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 28 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Opened the laptop.;\n",
      "B. Threw the broom.;\n",
      "C. Opened the door.;\n",
      "D. Closed the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 29 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the sandwich?\n",
      "A. Put down the dish.;\n",
      "B. Put down the food.;\n",
      "C. Put down the towel.;\n",
      "D. Closed the refrigerator.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 30 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the book?\n",
      "A. Put down the towel.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the bag.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 31 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the blanket?\n",
      "A. Took the book.;\n",
      "B. Put down the dish.;\n",
      "C. Opened the closet/cabinet.;\n",
      "D. Put down the laptop.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 32 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the door?\n",
      "A. Threw the box.;\n",
      "B. Put down the phone/camera.;\n",
      "C. Took the pillow.;\n",
      "D. Tidied up the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 33 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Took the broom.;\n",
      "B. Took the blanket.;\n",
      "C. Opened the door.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 34 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the shoe?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Threw the broom.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Washed the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 0, Batch 35 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the cup/glass/bottle?\n",
      "A. Threw the blanket.;\n",
      "B. Threw the box.;\n",
      "C. Put down the shoe.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 36 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Threw the towel.;\n",
      "C. Threw the book.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 0, Batch 37 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the book.;\n",
      "B. Threw the shoe.;\n",
      "C. Took the book.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1, Batch 0 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the paper/notebook.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Sat on the floor.;\n",
      "D. Took the shoe.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 1 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person washed the clothes?\n",
      "A. Put down the phone/camera.;\n",
      "B. Put down the clothes.;\n",
      "C. Put down the towel.;\n",
      "D. Washed the window.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 2 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person took the dish?\n",
      "A. Threw the shoe.;\n",
      "B. Put down the food.;\n",
      "C. Threw the pillow.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 3 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person drank from the cup/glass/bottle?\n",
      "A. Ate the sandwich.;\n",
      "B. Took the dish.;\n",
      "C. Washed the cup/glass/bottle.;\n",
      "D. Took the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 4 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the clothes?\n",
      "A. Opened the window.;\n",
      "B. Tidied up the closet/cabinet.;\n",
      "C. Took the food.;\n",
      "D. Tidied up the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 5 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the clothes?\n",
      "A. Threw the book.;\n",
      "B. Put down the bag.;\n",
      "C. Opened the window.;\n",
      "D. Put down the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 6 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the book?\n",
      "A. Sat on the bed.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the picture.;\n",
      "D. Threw the book.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 7 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Closed the refrigerator.;\n",
      "B. Tidied up the blanket.;\n",
      "C. Took the towel.;\n",
      "D. Took the cup/glass/bottle.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 8 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person washed the clothes?\n",
      "A. Put down the blanket.;\n",
      "B. Took the clothes.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the pillow.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 9 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the refrigerator?\n",
      "A. Closed the window.;\n",
      "B. Put down the cup/glass/bottle.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 10 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the window?\n",
      "A. Put down the food.;\n",
      "B. Washed the window.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Opened the door.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 11 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person watched at the picture?\n",
      "A. Opened the book.;\n",
      "B. Lied on the floor.;\n",
      "C. Washed the clothes.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 12 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the floor?\n",
      "A. Took the bag.;\n",
      "B. Took the phone/camera.;\n",
      "C. Took the blanket.;\n",
      "D. Took the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 13 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat at the table?\n",
      "A. Threw the bag.;\n",
      "B. Closed the closet/cabinet.;\n",
      "C. Ate the sandwich.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 14 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person held the food?\n",
      "A. Took the food.;\n",
      "B. Opened the bag.;\n",
      "C. Took the sandwich.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 15 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the door?\n",
      "A. Put down the box.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the towel.;\n",
      "D. Sat at the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 16 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 17 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person closed the door?\n",
      "A. Threw the broom.;\n",
      "B. Took the book.;\n",
      "C. Put down the phone/camera.;\n",
      "D. Put down the picture.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 18 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person put down the blanket?\n",
      "A. Took the bag.;\n",
      "B. Took the towel.;\n",
      "C. Tidied up the clothes.;\n",
      "D. Opened the box.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 19 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the dish?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Sat on the floor.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 20 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the box?\n",
      "A. Tidied up the blanket.;\n",
      "B. Put down the dish.;\n",
      "C. Took the cup/glass/bottle.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 21 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the food?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Closed the book.;\n",
      "C. Put down the cup/glass/bottle.;\n",
      "D. Put down the phone/camera.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 22 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the box?\n",
      "A. Put down the food.;\n",
      "B. Threw the towel.;\n",
      "C. Took the shoe.;\n",
      "D. Opened the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 23 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person drank from the cup/glass/bottle?\n",
      "A. Lied on the floor.;\n",
      "B. Took the broom.;\n",
      "C. Put down the dish.;\n",
      "D. Opened the bag.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 24 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person closed the laptop?\n",
      "A. Took the bag.;\n",
      "B. Took the sandwich.;\n",
      "C. Took the food.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 25 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person sat on the bed?\n",
      "A. Took the phone/camera.;\n",
      "B. Opened the window.;\n",
      "C. Took the pillow.;\n",
      "D. Put down the broom.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 26 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the dish?\n",
      "A. Put down the food.;\n",
      "B. Put down the box.;\n",
      "C. Took the paper/notebook.;\n",
      "D. Put down the blanket.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 27 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the bag.;\n",
      "B. Closed the book.;\n",
      "C. Took the shoe.;\n",
      "D. Washed the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 28 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Opened the laptop.;\n",
      "B. Threw the broom.;\n",
      "C. Opened the door.;\n",
      "D. Closed the closet/cabinet.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 29 ===\n",
      "Sample 0:\n",
      "  Expected: 'A'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the sandwich?\n",
      "A. Put down the dish.;\n",
      "B. Put down the food.;\n",
      "C. Put down the towel.;\n",
      "D. Closed the refrigerator.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 30 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the book?\n",
      "A. Put down the towel.;\n",
      "B. Took the paper/notebook.;\n",
      "C. Took the bag.;\n",
      "D. Tidied up the table.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 31 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the blanket?\n",
      "A. Took the book.;\n",
      "B. Put down the dish.;\n",
      "C. Opened the closet/cabinet.;\n",
      "D. Put down the laptop.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 32 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person opened the door?\n",
      "A. Threw the box.;\n",
      "B. Put down the phone/camera.;\n",
      "C. Took the pillow.;\n",
      "D. Tidied up the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 33 ===\n",
      "Sample 0:\n",
      "  Expected: 'D'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person put down the clothes?\n",
      "A. Took the broom.;\n",
      "B. Took the blanket.;\n",
      "C. Opened the door.;\n",
      "D. Threw the towel.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 34 ===\n",
      "Sample 0:\n",
      "  Expected: 'C'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened after the person held the shoe?\n",
      "A. Opened the closet/cabinet.;\n",
      "B. Threw the broom.;\n",
      "C. Sat on the sofa/couch.;\n",
      "D. Washed the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: False\n",
      "\n",
      "=== Epoch 1, Batch 35 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person took the cup/glass/bottle?\n",
      "A. Threw the blanket.;\n",
      "B. Threw the box.;\n",
      "C. Put down the shoe.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 36 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the cup/glass/bottle.;\n",
      "B. Threw the towel.;\n",
      "C. Threw the book.;\n",
      "D. Took the food.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n",
      "\n",
      "=== Epoch 1, Batch 37 ===\n",
      "Sample 0:\n",
      "  Expected: 'B'\n",
      "  Input: 'system\n",
      "你是书生·万象，英文名是InternVL，是由上海人工智能实验室、清华大学及多家合作单位联合开发的多模态大语言模型。\n",
      "user\n",
      "What happened before the person opened the door?\n",
      "A. Put down the book.;\n",
      "B. Threw the shoe.;\n",
      "C. Took the book.;\n",
      "D. Took the clothes.;\n",
      "Frame1: \n",
      "Frame2: \n",
      "Frame3: \n",
      "Frame4: \n",
      "Frame5: \n",
      "Frame6: \n",
      "Frame7: \n",
      "Frame8: \n",
      "\n",
      "Only give the best option.assistant'\n",
      "  Generated: 'Best option:()'\n",
      "  Match: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "# Optional: run a continuation fine-tuning\n",
    "extra_epochs = 1\n",
    "target_max_epochs = model_module.current_epoch + 1 + extra_epochs\n",
    "print(f'Continuing training up to epoch {target_max_epochs} (current epoch: {model_module.current_epoch})')\n",
    "continue_trainer = L.Trainer(\n",
    "    max_epochs=target_max_epochs,\n",
    "    check_val_every_n_epoch=config['check_val_every_n_epoch'],\n",
    "    gradient_clip_val=config['gradient_clip_val'],\n",
    "    accumulate_grad_batches=config['accumulate_grad_batches'],\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    logger=csv_logger,\n",
    "    precision='16-mixed',\n",
    "    accelerator='auto',\n",
    "    devices='auto',\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "continue_trainer.fit(\n",
    "    model_module,\n",
    "    ckpt_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "<|im_start|>system\n",
      "Carefully watch the video and pay attention to the cause and sequence of events, the detail and movement of objects, and the action and pose of persons. Based on your observations, select the best option that accurately addresses the question.<|im_end|>\n",
      "<|im_start|>user\n",
      "What happened after the person lied on the bed?\n",
      "A. Washed the dish.;\n",
      "B. Opened the laptop.;\n",
      "C. Put down the pillow.;\n",
      "D. Took the towel.;\n",
      "<video>\n",
      "Only give the best option.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Input IDs shape: torch.Size([1, 2206])\n",
      "Pixel values shape: torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input for inference\n",
    "sample = dataset['test'][0]\n",
    "clip = read_video_pyav(\n",
    "    f\"{data_dir}/video/videos_unzipped/{video_dir}/{sample['video']}\",\n",
    "    sample.get('start', 0),\n",
    "    sample.get('end', 1e10),\n",
    "    n_frames=8,\n",
    ")\n",
    "\n",
    "video_frames = [\n",
    "    frame if isinstance(frame, Image.Image) else Image.fromarray(frame)\n",
    "    for frame in clip\n",
    "]\n",
    "\n",
    "system_prompt = (\n",
    "    \"Carefully watch the video and pay attention to the cause and sequence of events, \"\n",
    "    \"the detail and movement of objects, and the action and pose of persons. Based on your observations, \"\n",
    "    \"select the best option that accurately addresses the question.\"\n",
    ")\n",
    "conversation = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': system_prompt\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': sample['conversations'][0]['value'] + '\\nOnly give the best option.'\n",
    "    }\n",
    "]\n",
    "prompt_text = processor.apply_chat_template(\n",
    "    conversation, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt:\\n{prompt_text}\\n\")\n",
    "\n",
    "# Process the input\n",
    "inputs = processor(\n",
    "    text=[prompt_text],\n",
    "    videos=[video_frames],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Pixel values shape: {inputs.get('pixel_values_videos', torch.tensor([])).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_module = InternVLModelPLModule(config=config, tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678eb1687bdf496e8b99f091e189938c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model accuracy on the validation split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn=train_collate_fn,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model_module.eval()\n",
    "model_module.model.to(device)\n",
    "model_module.model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch in tqdm(eval_dataloader):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    pixel_values = batch['pixel_values'].to(device, dtype=getattr(model_module.model, 'dtype', torch.float32))\n",
    "    answer_choices = batch['answer_choices']\n",
    "\n",
    "    response_ids = []\n",
    "    response_masks = []\n",
    "    for input_id in input_ids:\n",
    "        decoded_text = tokenizer.decode(input_id, skip_special_tokens=False).strip()\n",
    "        response_prefix = decoded_text.split('<|im_start|>assistant')[0]\n",
    "        tokenized = tokenizer(\n",
    "            response_prefix + '<|im_start|>assistant',\n",
    "            return_tensors='pt',\n",
    "            padding='max_length',\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=False,\n",
    "        )\n",
    "        response_ids.append(tokenized.input_ids)\n",
    "        response_masks.append(tokenized.attention_mask)\n",
    "\n",
    "    response_ids = torch.cat(response_ids, dim=0).to(device)\n",
    "    response_masks = torch.cat(response_masks, dim=0).to(device)\n",
    "    prompt_lengths = response_masks.sum(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                generated_ids = model_module.model.base_model.generate(\n",
    "                    input_ids=response_ids,\n",
    "                    attention_mask=response_masks,\n",
    "                    pixel_values=pixel_values,\n",
    "                    max_new_tokens=10,\n",
    "                    do_sample=False,\n",
    "                    temperature=1.0,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "        else:\n",
    "            generated_ids = model_module.model.base_model.generate(\n",
    "                input_ids=response_ids,\n",
    "                attention_mask=response_masks,\n",
    "                pixel_values=pixel_values,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                temperature=1.0,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "    assistant_texts = []\n",
    "    for i in range(len(generated_ids)):\n",
    "        start = int(prompt_lengths[i].item())\n",
    "        assistant_tokens = generated_ids[i, start:]\n",
    "        assistant_text = tokenizer.decode(assistant_tokens, skip_special_tokens=True).strip()\n",
    "        assistant_texts.append(assistant_text)\n",
    "\n",
    "    for predicted_text, answer in zip(assistant_texts, answer_choices):\n",
    "        predicted_upper = predicted_text.upper().strip()\n",
    "        expected_upper = (answer or \"\").upper()\n",
    "        if expected_upper and predicted_upper.startswith(expected_upper):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "test_accuracy = correct / total if total else 0.0\n",
    "print(f\"Validation accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION RESULTS\n",
      "================================================================================\n",
      "           Task          Model   Fine-tuning Method  Train Dataset Size  Test Dataset Size   Metric Test Accuracy  Training Epochs  Learning Rate  Batch Size  LoRA Rank  LoRA Alpha\n",
      "Action Sequence InternVL3.5-1B QLoRA (4-bit + LoRA)                 150                 38 Accuracy        0.0000                6         0.0001           1         16          32\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a summary table with results\n",
    "results = {\n",
    "    \"Task\": [\"Action Sequence\"],\n",
    "    \"Model\": [\"InternVL3.5-1B\"],\n",
    "    \"Fine-tuning Method\": [\"QLoRA (4-bit + LoRA)\"],\n",
    "    \"Train Dataset Size\": [len(train_dataset)],\n",
    "    \"Test Dataset Size\": [len(eval_dataset)],\n",
    "    \"Metric\": [\"Accuracy\"],\n",
    "    \"Test Accuracy\": [f\"{test_accuracy:.4f}\"],\n",
    "    \"Training Epochs\": [config[\"max_epochs\"]],\n",
    "    \"Learning Rate\": [config[\"lr\"]],\n",
    "    \"Batch Size\": [config[\"batch_size\"]],\n",
    "    \"LoRA Rank\": [16],\n",
    "    \"LoRA Alpha\": [32],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# After training, you can update the test accuracy by running evaluation on the full test set\n",
    "# Example: test_accuracy = trainer.validate(model_module)\n",
    "# Then update: results[\"Test Accuracy\"] = [f\"{test_accuracy:.4f}\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The poor quality may have been caused by choosing QLoRA at 4 bits, as we only retrain thin LoRA ranks.\n",
    "The prompt format with special tokens, etc., may not be entirely correct."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c80b263628d4c37b129278c8cb80fc2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e40a2be14574c9cab05c6a2066506c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9e922c6bf14487599e94963b8afd747",
      "max": 188,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7dd46cf21684684b12361c82c5e01ca",
      "value": 188
     }
    },
    "24b4a2b91c7e484bacb5a543a60dff44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4c53a11f8b4d48bab7bb07005e09dab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_606c508016d34f1bb86d21b2c4e3cfc3",
      "placeholder": "​",
      "style": "IPY_MODEL_8c9c50dc1265410e82248451e1fdd156",
      "value": "Map: 100%"
     }
    },
    "606c508016d34f1bb86d21b2c4e3cfc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6997d1b4ec2e40268bbfe56ff5ad1ddc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c13ed62e28e44468633fdfa010a07cb",
      "placeholder": "​",
      "style": "IPY_MODEL_24b4a2b91c7e484bacb5a543a60dff44",
      "value": " 188/188 [00:00&lt;00:00, 1990.77 examples/s]"
     }
    },
    "7c13ed62e28e44468633fdfa010a07cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c9c50dc1265410e82248451e1fdd156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c44886c523194321a45ac116a99d7e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4c53a11f8b4d48bab7bb07005e09dab2",
       "IPY_MODEL_1e40a2be14574c9cab05c6a2066506c9",
       "IPY_MODEL_6997d1b4ec2e40268bbfe56ff5ad1ddc"
      ],
      "layout": "IPY_MODEL_0c80b263628d4c37b129278c8cb80fc2"
     }
    },
    "e7dd46cf21684684b12361c82c5e01ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9e922c6bf14487599e94963b8afd747": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
